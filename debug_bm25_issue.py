"""
è°ƒè¯•BM25æ£€ç´¢é—®é¢˜ - åˆ†æåˆ†è¯å’ŒåŒ¹é…é€»è¾‘
================================

ç›®æ ‡ï¼šåˆ†æä¸ºä»€ä¹ˆåŒ…å«"warbond recommendation"çš„æ¡ç›®æ²¡æœ‰è¢«æ­£ç¡®åŒ¹é…
"""

import jieba
import re
from typing import List, Set

def debug_tokenization():
    """è°ƒè¯•åˆ†è¯é—®é¢˜"""
    print("=== BM25åˆ†è¯è°ƒè¯• ===\n")
    
    # æµ‹è¯•æŸ¥è¯¢
    query = "best warbond recommendations guide"
    
    # æµ‹è¯•ç›®æ ‡æ–‡æœ¬ï¼ˆä»metadata.jsonä¸­çš„å®é™…æ¡ç›®ï¼‰
    target_texts = [
        "Warbond Recommendation: Democratic Detonation",
        "Warbond Recommendation: Truth Enforcers", 
        "Warbond Recommendation: Polar Patriots",
        "Warbond Recommendation: Steeled Veterans",
        "Weapon Recommendations from Warbonds",  # è¿™ä¸ªç¡®å®è¢«åŒ¹é…äº†
    ]
    
    print(f"ğŸ” æŸ¥è¯¢: {query}")
    print(f"ğŸ“ æ ‡å‡†åŒ–æŸ¥è¯¢: {query.lower()}")
    
    # æ¨¡æ‹Ÿç°æœ‰çš„é¢„å¤„ç†é€»è¾‘
    def current_preprocess_text(text: str) -> List[str]:
        """å½“å‰çš„é¢„å¤„ç†é€»è¾‘ï¼ˆç®€åŒ–ç‰ˆï¼‰"""
        if not text:
            return []
            
        # è½¬æ¢ä¸ºå°å†™
        text = text.lower()
        
        # ç§»é™¤ç‰¹æ®Šå­—ç¬¦ï¼Œä½†ä¿ç•™ä¸­æ–‡ã€è‹±æ–‡ã€æ•°å­—å’Œç©ºæ ¼
        text = re.sub(r'[^\u4e00-\u9fa5a-zA-Z0-9\s]', ' ', text)
        
        # ä¸­æ–‡åˆ†è¯ï¼ˆjiebaå¯¹è‹±æ–‡å¤„ç†å¯èƒ½æœ‰é—®é¢˜ï¼‰
        tokens = list(jieba.cut(text))
        
        # åŸºæœ¬åœç”¨è¯
        stop_words = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by'}
        
        # è¿‡æ»¤
        filtered_tokens = []
        for token in tokens:
            token = token.strip()
            if (token and 
                token not in stop_words and 
                (len(token) > 1 or token.isdigit())):
                filtered_tokens.append(token)
        
        return filtered_tokens
    
    # åˆ†ææŸ¥è¯¢åˆ†è¯
    query_tokens = current_preprocess_text(query)
    print(f"ğŸ”¤ æŸ¥è¯¢åˆ†è¯: {query_tokens}")
    
    print(f"\nğŸ“‹ ç›®æ ‡æ–‡æœ¬åˆ†è¯ç»“æœ:")
    for i, text in enumerate(target_texts, 1):
        tokens = current_preprocess_text(text)
        print(f"{i}. '{text}'")
        print(f"   åˆ†è¯: {tokens}")
        
        # æ£€æŸ¥åŒ¹é…
        matched = []
        for query_token in query_tokens:
            if query_token in tokens:
                matched.append(query_token)
        print(f"   åŒ¹é…çš„æŸ¥è¯¢è¯: {matched}")
        print(f"   åŒ¹é…æ•°é‡: {len(matched)}/{len(query_tokens)}")
        print()

def debug_english_tokenization():
    """ä¸“é—¨è°ƒè¯•è‹±æ–‡åˆ†è¯é—®é¢˜"""
    print("=== è‹±æ–‡åˆ†è¯é—®é¢˜è°ƒè¯• ===\n")
    
    test_phrases = [
        "warbond recommendation",
        "warbond recommendations", 
        "best warbond recommendations guide",
        "Warbond Recommendation: Democratic Detonation"
    ]
    
    print("ğŸ” jiebaåˆ†è¯ç»“æœ:")
    for phrase in test_phrases:
        tokens = list(jieba.cut(phrase.lower()))
        print(f"'{phrase}' -> {tokens}")
    
    print("\nğŸ” ç®€å•ç©ºæ ¼åˆ†è¯ç»“æœ:")
    for phrase in test_phrases:
        tokens = phrase.lower().split()
        print(f"'{phrase}' -> {tokens}")

def suggest_fix():
    """å»ºè®®ä¿®å¤æ–¹æ¡ˆ"""
    print("=== ä¿®å¤å»ºè®® ===\n")
    
    print("ğŸ”§ é—®é¢˜åˆ†æ:")
    print("1. jiebaåˆ†è¯ä¸»è¦é’ˆå¯¹ä¸­æ–‡ï¼Œå¯¹è‹±æ–‡åˆ†è¯ä¸å¤Ÿå‡†ç¡®")
    print("2. 'recommendations'ï¼ˆå¤æ•°ï¼‰å’Œ'recommendation'ï¼ˆå•æ•°ï¼‰å¯èƒ½æ— æ³•æ­£ç¡®åŒ¹é…")
    print("3. è‹±æ–‡è¯æ±‡å¯èƒ½è¢«é”™è¯¯åˆ†å‰²")
    
    print("\nğŸ’¡ ä¿®å¤æ–¹æ¡ˆ:")
    print("1. å¯¹è‹±æ–‡ä½¿ç”¨ç©ºæ ¼åˆ†è¯ï¼Œå¯¹ä¸­æ–‡ä½¿ç”¨jiebaåˆ†è¯")
    print("2. æ·»åŠ è¯å¹²æå–ï¼ˆstemmingï¼‰ä»¥åŒ¹é…å•å¤æ•°å½¢å¼")
    print("3. æ·»åŠ åŒä¹‰è¯æ‰©å±•")
    print("4. æ”¹è¿›æƒé‡è®¡ç®—é€»è¾‘")

def improved_preprocess_text(text: str) -> List[str]:
    """æ”¹è¿›çš„é¢„å¤„ç†é€»è¾‘"""
    if not text:
        return []
        
    # è½¬æ¢ä¸ºå°å†™
    text = text.lower()
    
    # åˆ†ç¦»ä¸­æ–‡å’Œè‹±æ–‡å¤„ç†
    # ç®€å•çš„è‹±æ–‡è¯å¹²å¤„ç†
    def simple_stem(word):
        """ç®€å•çš„è¯å¹²æå–"""
        if word.endswith('s') and len(word) > 3:
            # å»é™¤å¤æ•°s
            return word[:-1]
        if word.endswith('ing') and len(word) > 5:
            return word[:-3]
        if word.endswith('ed') and len(word) > 4:
            return word[:-2]
        return word
    
    # ç§»é™¤æ ‡ç‚¹ç¬¦å·
    text = re.sub(r'[^\u4e00-\u9fa5a-zA-Z0-9\s]', ' ', text)
    
    # æ£€æµ‹æ˜¯å¦åŒ…å«ä¸­æ–‡
    has_chinese = bool(re.search(r'[\u4e00-\u9fa5]', text))
    
    if has_chinese:
        # åŒ…å«ä¸­æ–‡ï¼Œä½¿ç”¨jiebaåˆ†è¯
        tokens = list(jieba.cut(text))
    else:
        # çº¯è‹±æ–‡ï¼Œä½¿ç”¨ç©ºæ ¼åˆ†è¯
        tokens = text.split()
    
    # åœç”¨è¯
    stop_words = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by'}
    
    # è¿‡æ»¤å’Œè¯å¹²å¤„ç†
    processed_tokens = []
    for token in tokens:
        token = token.strip()
        if (token and 
            token not in stop_words and 
            (len(token) > 1 or token.isdigit())):
            # åº”ç”¨è¯å¹²æå–
            stemmed = simple_stem(token)
            processed_tokens.append(stemmed)
            # å¦‚æœè¯å¹²ä¸åŒï¼Œä¹ŸåŠ å…¥åŸè¯
            if stemmed != token:
                processed_tokens.append(token)
    
    return processed_tokens

def test_improved_tokenization():
    """æµ‹è¯•æ”¹è¿›çš„åˆ†è¯é€»è¾‘"""
    print("=== æ”¹è¿›åˆ†è¯æµ‹è¯• ===\n")
    
    query = "best warbond recommendations guide"
    target_texts = [
        "Warbond Recommendation: Democratic Detonation",
        "Warbond Recommendation: Truth Enforcers", 
        "Weapon Recommendations from Warbonds",
    ]
    
    print(f"ğŸ” æŸ¥è¯¢: {query}")
    
    # æµ‹è¯•æ”¹è¿›çš„åˆ†è¯
    query_tokens = improved_preprocess_text(query)
    print(f"ğŸ”¤ æ”¹è¿›åˆ†è¯: {query_tokens}")
    
    print(f"\nğŸ“‹ ç›®æ ‡æ–‡æœ¬åˆ†è¯ç»“æœï¼ˆæ”¹è¿›ç‰ˆï¼‰:")
    for i, text in enumerate(target_texts, 1):
        tokens = improved_preprocess_text(text)
        print(f"{i}. '{text}'")
        print(f"   åˆ†è¯: {tokens}")
        
        # æ£€æŸ¥åŒ¹é…
        matched = []
        for query_token in query_tokens:
            if query_token in tokens:
                matched.append(query_token)
        print(f"   åŒ¹é…çš„æŸ¥è¯¢è¯: {matched}")
        print(f"   åŒ¹é…æ•°é‡: {len(matched)}/{len(query_tokens)}")
        print()

if __name__ == "__main__":
    debug_tokenization()
    print("\n" + "="*50 + "\n")
    debug_english_tokenization()
    print("\n" + "="*50 + "\n")
    suggest_fix()
    print("\n" + "="*50 + "\n")
    test_improved_tokenization() 