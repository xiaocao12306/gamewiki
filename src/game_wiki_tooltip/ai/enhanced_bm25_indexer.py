"""
ç®€åŒ–BM25ç´¢å¼•å™¨ - ä¸“æ³¨äºé«˜æ•ˆæ£€ç´¢
=================================

åŠŸèƒ½ï¼š
1. æ™ºèƒ½æ–‡æœ¬é¢„å¤„ç†
2. å¤šè¯­è¨€æ”¯æŒï¼ˆä¸­è‹±æ–‡ï¼‰
3. ç®€åŒ–çš„BM25æ£€ç´¢
4. ç”±LLMè´Ÿè´£æŸ¥è¯¢ä¼˜åŒ–
"""

import jieba
import json
import pickle
import re
import logging
from typing import List, Dict, Any, Optional, Set, Tuple
from pathlib import Path

# å¯¼å…¥ç¿»è¯‘å‡½æ•°
from src.game_wiki_tooltip.i18n import t

# å°è¯•å¯¼å…¥bm25sï¼Œæ›´ç°ä»£ã€æ›´å¿«çš„BM25å®ç°
try:
    import bm25s
    BM25_AVAILABLE = True
    BM25_IMPORT_ERROR = None
except ImportError as e:
    BM25_AVAILABLE = False
    bm25s = None
    BM25_IMPORT_ERROR = str(e)

logger = logging.getLogger(__name__)

class BM25UnavailableError(Exception):
    """BM25åŠŸèƒ½ä¸å¯ç”¨é”™è¯¯"""
    pass

class EnhancedBM25Indexer:
    """ç®€åŒ–BM25ç´¢å¼•å™¨ï¼Œä¸“æ³¨äºé«˜æ•ˆæ£€ç´¢ï¼ŒæŸ¥è¯¢ä¼˜åŒ–ç”±LLMè´Ÿè´£"""
    
    def __init__(self, game_name: str = "helldiver2", stop_words: Optional[List[str]] = None):
        """
        åˆå§‹åŒ–ç®€åŒ–BM25ç´¢å¼•å™¨
        
        Args:
            game_name: æ¸¸æˆåç§° (ç”¨äºæ•Œäººåç§°æ ‡å‡†åŒ–)
            stop_words: åœç”¨è¯åˆ—è¡¨
            
        Raises:
            BM25UnavailableError: å½“bm25såŒ…ä¸å¯ç”¨æ—¶
        """
        self.game_name = game_name
        self.bm25 = None
        self.documents = []
        
        if not BM25_AVAILABLE:
            error_msg = t("bm25_package_unavailable", error=BM25_IMPORT_ERROR)
            error_msg += "\nè¯·å°è¯•ä»¥ä¸‹è§£å†³æ–¹æ¡ˆï¼š"
            error_msg += "\n1. å®‰è£…bm25s: pip install bm25s"
            error_msg += "\n2. å¦‚æœä»æœ‰é—®é¢˜ï¼Œå°è¯•é‡æ–°å®‰è£…: pip uninstall bm25s && pip install bm25s"
            error_msg += "\n3. ç¡®ä¿numpyå’Œscipyå·²æ­£ç¡®å®‰è£…: pip install numpy scipy"
            logger.error(error_msg)
            raise BM25UnavailableError(error_msg)
            
        self.stop_words = self._load_stop_words(stop_words)
        logger.info(f"BM25ç´¢å¼•å™¨åˆå§‹åŒ–æˆåŠŸ - æ¸¸æˆ: {game_name}")

    def _load_stop_words(self, stop_words: Optional[List[str]] = None) -> Set[str]:
        """åŠ è½½åœç”¨è¯ï¼Œä½†ä¿ç•™é‡è¦çš„æˆ˜æœ¯æœ¯è¯­"""
        default_stop_words = {
            # ä¸­æ–‡åœç”¨è¯
            'çš„', 'äº†', 'åœ¨', 'æ˜¯', 'æˆ‘', 'æœ‰', 'å’Œ', 'å°±', 'ä¸', 'äºº', 'éƒ½', 'ä¸€', 'ä¸€ä¸ª', 'ä¸Š', 'ä¹Ÿ', 'å¾ˆ', 'åˆ°', 'è¯´', 'è¦', 'å»', 'ä½ ', 'ä¼š', 'ç€', 'æ²¡æœ‰', 'çœ‹', 'å¥½', 'è‡ªå·±', 'è¿™',
            # è‹±æ–‡åœç”¨è¯  
            'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by', 'from', 'as', 'is', 'was', 'are', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'should', 'could', 'can', 'may', 'might', 'must', 'shall',
            # é€šç”¨æ¸¸æˆè¯æ±‡ï¼ˆä½†ä¸åŒ…æ‹¬æˆ˜æœ¯æœ¯è¯­ï¼‰
            'game', 'player', 'mission', 'level'
        }
        
        if stop_words:
            default_stop_words.update(stop_words)
            
        return default_stop_words
        
    def _normalize_enemy_name(self, text: str) -> str:
        """æ ‡å‡†åŒ–æ•Œäººåç§° - åŸºäºå½“å‰æ¸¸æˆé…ç½®"""
        text = text.lower()
        
        # åŸºäºæ¸¸æˆç‰¹å®šçš„æ•Œäººå…³é”®è¯è¿›è¡Œæ ‡å‡†åŒ–
        # è¿™é‡Œæˆ‘ä»¬ä½¿ç”¨ä¸€ä¸ªé€šç”¨çš„æ–¹æ³•ï¼Œä¸å†ç¡¬ç¼–ç ç‰¹å®šæ¸¸æˆçš„æ˜ å°„
        # å¯ä»¥æ ¹æ®éœ€è¦åœ¨æ¸¸æˆé…ç½®ä¸­æ·»åŠ åˆ«åæ˜ å°„
        
        # é’ˆå¯¹Helldivers 2çš„ç‰¹æ®Šå¤„ç† (ä¿ç•™å‘åå…¼å®¹æ€§)
        if self.game_name == "helldiver2":
            enemy_mappings = {
                'bt': 'bile titan',
                'biletitan': 'bile titan',
                'bile_titan': 'bile titan',
                'èƒ†æ±æ³°å¦': 'bile titan',
                'å·¨äººæœºç”²': 'hulk',
                'hulk devastator': 'hulk',
                'å†²é”‹è€…': 'charger',
                'ç©¿åˆºè€…': 'impaler',
                'æ½œè¡Œè€…': 'stalker',
                'æ—ç¾¤æŒ‡æŒ¥å®˜': 'brood commander',
                'å·¥å‚è¡Œè€…': 'factory strider',
                'æ¯ç­è€…': 'devastator',
                'ç‹‚æˆ˜å£«': 'berserker',
                'æ­¦è£…ç›´å‡æœº': 'gunship',
                'å¦å…‹': 'tank',
                'è¿è¾“èˆ°': 'dropship',
            }
            
            for original, normalized in enemy_mappings.items():
                text = text.replace(original, normalized)
            
        return text
        
    def preprocess_text(self, text: str) -> List[str]:
        """
        ç®€åŒ–çš„æ–‡æœ¬é¢„å¤„ç†ï¼Œä¸“æ³¨äºé«˜æ•ˆåˆ†è¯
        ç§»é™¤å¤æ‚çš„æƒé‡é€»è¾‘ï¼Œç”±LLMè´Ÿè´£æŸ¥è¯¢ä¼˜åŒ–
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            
        Returns:
            å¤„ç†åçš„tokenåˆ—è¡¨
        """
        if not text:
            return []
            
        # è½¬æ¢ä¸ºå°å†™å¹¶æ ‡å‡†åŒ–æ•Œäººåç§°
        text = self._normalize_enemy_name(text.lower())
        
        # ç§»é™¤ç‰¹æ®Šå­—ç¬¦ï¼Œä½†ä¿ç•™ä¸­æ–‡ã€è‹±æ–‡ã€æ•°å­—å’Œç©ºæ ¼
        text = re.sub(r'[^\u4e00-\u9fa5a-zA-Z0-9\s]', ' ', text)
        
        # æ£€æµ‹æ˜¯å¦åŒ…å«ä¸­æ–‡
        has_chinese = bool(re.search(r'[\u4e00-\u9fa5]', text))
        
        # åˆ†è¯å¤„ç†
        if has_chinese:
            # åŒ…å«ä¸­æ–‡ï¼Œä½¿ç”¨jiebaåˆ†è¯
            tokens = list(jieba.cut(text))
        else:
            # çº¯è‹±æ–‡ï¼Œä½¿ç”¨ç©ºæ ¼åˆ†è¯ï¼ˆæ›´å‡†ç¡®ï¼‰
            tokens = text.split()
        
        # ç®€å•çš„è‹±æ–‡è¯å¹²æå–
        def simple_stem(word):
            """ç®€å•çš„è¯å¹²æå–ï¼Œå¤„ç†å¸¸è§çš„è‹±æ–‡å˜å½¢"""
            if len(word) <= 2:
                return word
                
            # å¤„ç†å¤æ•°å½¢å¼
            if word.endswith('s') and len(word) > 3:
                # ç‰¹æ®Šå¤æ•°å½¢å¼
                if word.endswith('ies') and len(word) > 4:
                    return word[:-3] + 'y'  # strategies -> strategy
                elif word.endswith('es') and len(word) > 4:
                    return word[:-2]  # boxes -> box
                else:
                    return word[:-1]  # recommendations -> recommendation
                    
            # å¤„ç†å…¶ä»–å¸¸è§åç¼€
            if word.endswith('ing') and len(word) > 5:
                return word[:-3]  # running -> run
            if word.endswith('ed') and len(word) > 4:
                return word[:-2]  # played -> play
            if word.endswith('ly') and len(word) > 4:
                return word[:-2]  # quickly -> quick
                
            return word
        
        # å¤„ç†token - ç®€åŒ–ç‰ˆæœ¬
        processed_tokens = []
        for token in tokens:
            token = token.strip()
            
            # è¿‡æ»¤æ¡ä»¶ï¼šéç©ºã€ä¸æ˜¯åœç”¨è¯ã€é•¿åº¦>1æˆ–è€…æ˜¯æ•°å­—
            if (token and 
                token not in self.stop_words and 
                (len(token) > 1 or token.isdigit())):
                
                # å¯¹è‹±æ–‡å•è¯åº”ç”¨è¯å¹²æå–
                if not re.search(r'[\u4e00-\u9fa5]', token):  # éä¸­æ–‡
                    stemmed = simple_stem(token)
                    processed_tokens.append(stemmed)
                    
                    # å¦‚æœè¯å¹²ä¸åŸè¯ä¸åŒï¼Œä¹Ÿæ·»åŠ åŸè¯
                    if stemmed != token:
                        processed_tokens.append(token)
                else:
                    # ä¸­æ–‡è¯æ±‡ç›´æ¥å¤„ç†
                    processed_tokens.append(token)
        
        return processed_tokens
    
    def build_enhanced_text(self, chunk: Dict[str, Any]) -> str:
        """
        æ„å»ºæœç´¢æ–‡æœ¬ï¼Œä¸“æ³¨äºå†…å®¹æå–
        ç§»é™¤æƒé‡é€»è¾‘ï¼Œç”±LLMè´Ÿè´£æŸ¥è¯¢ä¼˜åŒ–
        
        Args:
            chunk: çŸ¥è¯†å—
            
        Returns:
            æœç´¢æ–‡æœ¬
        """
        text_parts = []
        
        # 1. Topicï¼ˆé‡è¦å†…å®¹ï¼‰
        topic = chunk.get("topic", "")
        if topic:
            text_parts.append(topic)
            
        # 2. å…³é”®è¯
        keywords = chunk.get("keywords", [])
        if keywords:
            text_parts.extend(keywords)
            
        # 3. Summary
        summary = chunk.get("summary", "")
        if summary:
            text_parts.append(summary)
            
        # 4. ç»“æ„åŒ–æ•°æ®å¤„ç†
        self._extract_structured_content(chunk, text_parts)
        
        return " ".join(text_parts)
    
    def _extract_structured_content(self, chunk: Dict[str, Any], text_parts: List[str]) -> None:
        """æå–ç»“æ„åŒ–å†…å®¹ï¼Œä¸“æ³¨äºå†…å®¹è€Œéæƒé‡"""
        
        # æ•Œäººå¼±ç‚¹ä¿¡æ¯
        if "structured_data" in chunk:
            structured = chunk["structured_data"]
            
            # æ•Œäººåç§°
            if "enemy_name" in structured:
                text_parts.append(structured["enemy_name"])
                
            # å¼±ç‚¹ä¿¡æ¯
            if "weak_points" in structured:
                for weak_point in structured["weak_points"]:
                    if "name" in weak_point:
                        text_parts.append(weak_point["name"])
                    if "notes" in weak_point:
                        text_parts.append(weak_point["notes"])
                        
            # æ¨èæ­¦å™¨
            if "recommended_weapons" in structured:
                for weapon in structured["recommended_weapons"]:
                    text_parts.append(weapon)
                    
        # Buildä¿¡æ¯
        if "build" in chunk:
            build = chunk["build"]
            
            # Buildåç§°
            if "name" in build:
                text_parts.append(build["name"])
                
            # æˆ˜æœ¯ç„¦ç‚¹
            if "focus" in build:
                text_parts.append(build["focus"])
                
            # ç­–ç•¥ä¿¡æ¯
            if "stratagems" in build:
                for stratagem in build["stratagems"]:
                    if "name" in stratagem:
                        text_parts.append(stratagem["name"])
                    if "rationale" in stratagem:
                        text_parts.append(stratagem["rationale"])
    
    def build_index(self, chunks: List[Dict[str, Any]]) -> None:
        """
        æ„å»ºå¢å¼ºBM25ç´¢å¼•
        
        Args:
            chunks: çŸ¥è¯†å—åˆ—è¡¨
            
        Raises:
            BM25UnavailableError: å½“BM25åŠŸèƒ½ä¸å¯ç”¨æ—¶
        """
        if not BM25_AVAILABLE:
            raise BM25UnavailableError(t("bm25_build_failed"))
            
        logger.info(f"å¼€å§‹æ„å»ºå¢å¼ºBM25ç´¢å¼•ï¼Œå…± {len(chunks)} ä¸ªçŸ¥è¯†å—")
        
        self.documents = chunks
        
        # æ„å»ºå¢å¼ºæœç´¢æ–‡æœ¬
        search_texts = []
        for i, chunk in enumerate(chunks):
            try:
                # æ„å»ºå¢å¼ºæ–‡æœ¬
                enhanced_text = self.build_enhanced_text(chunk)
                
                # é¢„å¤„ç†å’Œæƒé‡åŒ–
                tokenized = self.preprocess_text(enhanced_text)
                search_texts.append(tokenized)
                
                # è°ƒè¯•ä¿¡æ¯
                if i < 3:  # åªæ‰“å°å‰3ä¸ªç”¨äºè°ƒè¯•
                    logger.info(f"æ ·æœ¬ {i}: {chunk.get('topic', 'Unknown')}")
                    logger.info(f"å¢å¼ºæ–‡æœ¬: {enhanced_text[:200]}...")
                    logger.info(f"Tokenæ ·æœ¬: {tokenized[:10]}")
                    logger.info(f"Tokenæ€»æ•°: {len(tokenized)}")
                
            except Exception as e:
                logger.error(f"å¤„ç†ç¬¬ {i} ä¸ªçŸ¥è¯†å—æ—¶å‡ºé”™: {e}")
                search_texts.append([])
        
        # åˆ›å»ºBM25ç´¢å¼•
        try:
            self.bm25 = bm25s.BM25()
            self.bm25.index(search_texts)
            # ä¿å­˜åŸå§‹æ–‡æ¡£ä»¥ä¾¿åç»­ä½¿ç”¨
            self.corpus_tokens = search_texts
            logger.info("å¢å¼ºBM25ç´¢å¼•æ„å»ºå®Œæˆ")
        except Exception as e:
            error_msg = t("bm25_build_error", error=str(e))
            logger.error(error_msg)
            raise BM25UnavailableError(error_msg)

    def search(self, query: str, top_k: int = 10) -> List[Dict[str, Any]]:
        """
        å¢å¼ºBM25æœç´¢
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            top_k: è¿”å›ç»“æœæ•°é‡
            
        Returns:
            æœç´¢ç»“æœåˆ—è¡¨
            
        Raises:
            BM25UnavailableError: å½“BM25åŠŸèƒ½ä¸å¯ç”¨æ—¶
        """
        if not BM25_AVAILABLE:
            raise BM25UnavailableError(t("bm25_search_failed"))
            
        if not self.bm25:
            raise BM25UnavailableError(t("bm25_search_not_initialized"))
            
        # é¢„å¤„ç†æŸ¥è¯¢ - ä½¿ç”¨ä¸ç´¢å¼•æ„å»ºç›¸åŒçš„é€»è¾‘
        normalized_query = self._normalize_enemy_name(query.lower())
        tokenized_query = self.preprocess_text(normalized_query)
        
        if not tokenized_query:
            logger.warning("æŸ¥è¯¢é¢„å¤„ç†åä¸ºç©º")
            return []
        
        print(f"ğŸ” [BM25-DEBUG] ç®€åŒ–BM25æœç´¢ - åŸå§‹æŸ¥è¯¢: {query}")
        print(f"   ğŸ“ [BM25-DEBUG] æ ‡å‡†åŒ–æŸ¥è¯¢: {normalized_query}")
        print(f"   ğŸ”¤ [BM25-DEBUG] åˆ†è¯ç»“æœ: {tokenized_query}")
        logger.info(f"ç®€åŒ–BM25æœç´¢ - åŸå§‹æŸ¥è¯¢: {query}")
        logger.info(f"æ ‡å‡†åŒ–æŸ¥è¯¢: {normalized_query}")
        logger.info(f"åˆ†è¯ç»“æœ: {tokenized_query}")
        
        try:
            # ä½¿ç”¨bm25sçš„retrieveæ–¹æ³•
            results_ids, scores = self.bm25.retrieve(tokenized_query, k=top_k)
            # results_ids shape: (1, top_k), scores shape: (1, top_k)
            top_indices = results_ids[0]  # è·å–ç¬¬ä¸€ä¸ªæŸ¥è¯¢çš„ç»“æœ
            top_scores = scores[0]  # è·å–ç¬¬ä¸€ä¸ªæŸ¥è¯¢çš„åˆ†æ•°
            
            print(f"   ğŸ“Š [BM25-DEBUG] Top {len(top_scores)} ç»“æœåˆ†æ•°: {top_scores}")
            print(f"   ğŸ“‹ [BM25-DEBUG] Top {top_k} ç´¢å¼•: {top_indices}")
            print(f"   ğŸ“‹ [BM25-DEBUG] å¯¹åº”åˆ†æ•°: {top_scores}")
            
            results = []
            for i, idx in enumerate(top_indices):
                score = top_scores[i]  # ä½¿ç”¨å·²æ’åºçš„åˆ†æ•°
                if score > 0:
                    chunk = self.documents[idx]
                    match_info = {
                        "topic": chunk.get("topic", ""),
                        "enemy": self._extract_enemy_from_chunk(chunk),
                        "relevance_reason": self._explain_relevance(tokenized_query, chunk, original_query=query)
                    }
                    result = {
                        "chunk": chunk,
                        "score": float(score),
                        "rank": i + 1,
                        "match_info": match_info
                    }
                    results.append(result)
                    
                    # è¯¦ç»†çš„åŒ¹é…è°ƒè¯•ä¿¡æ¯
                    print(f"   ğŸ“‹ [BM25-DEBUG] ç»“æœ {i+1}:")
                    print(f"      - ç´¢å¼•: {idx}")
                    print(f"      - åˆ†æ•°: {score:.4f}")
                    print(f"      - ä¸»é¢˜: {chunk.get('topic', 'Unknown')}")
                    print(f"      - æ•Œäºº: {match_info['enemy']}")
                    print(f"      - åŒ¹é…ç†ç”±: {match_info['relevance_reason']}")
                    print(f"      - æ‘˜è¦: {chunk.get('summary', '')[:100]}...")
                    
                    # æ˜¾ç¤ºå…³é”®è¯åŒ¹é…ä¿¡æ¯
                    chunk_text = self.build_enhanced_text(chunk).lower()
                    matched_keywords = []
                    for token in set(tokenized_query):
                        if token in chunk_text:
                            matched_keywords.append(token)
                    if matched_keywords:
                        print(f"      - åŒ¹é…å…³é”®è¯: {', '.join(matched_keywords[:10])}")
            
            print(f"âœ… [BM25-DEBUG] å¢å¼ºBM25æœç´¢å®Œæˆï¼Œæ‰¾åˆ° {len(results)} ä¸ªç»“æœ")
            logger.info(f"å¢å¼ºBM25æœç´¢å®Œæˆï¼Œæ‰¾åˆ° {len(results)} ä¸ªç»“æœ")
            return results
            
        except Exception as e:
            error_msg = t("bm25_search_execution_failed", error=str(e))
            logger.error(error_msg)
            raise BM25UnavailableError(error_msg)
    
    def _extract_enemy_from_chunk(self, chunk: Dict[str, Any]) -> str:
        """ä»chunkä¸­æå–æ•Œäºº/ç›®æ ‡åç§°"""
        # æ£€æŸ¥ç»“æ„åŒ–æ•°æ®
        if "structured_data" in chunk and "enemy_name" in chunk["structured_data"]:
            return chunk["structured_data"]["enemy_name"]
            
        # ç®€å•æå–ï¼šä»topicä¸­æŸ¥æ‰¾å¯èƒ½çš„æ•Œäººåç§°
        topic = chunk.get("topic", "")
        
        # åŸºæœ¬çš„æ•Œäºº/ç›®æ ‡è¯†åˆ«å…³é”®è¯
        target_indicators = ["enemy", "boss", "æ•Œäºº", "é¦–é¢†", "æ€ªç‰©", "å¯¹æ‰‹"]
        if any(indicator in topic.lower() for indicator in target_indicators):
            # æå–topicä¸­çš„ä¸»è¦è¯æ±‡ä½œä¸ºç›®æ ‡åç§°
            words = topic.split()
            if len(words) >= 2:
                # å–å‰ä¸¤ä¸ªè¯ä½œä¸ºç›®æ ‡åç§°
                return " ".join(words[:2])
        
        # å¦‚æœæ²¡æœ‰æ˜ç¡®çš„æ•Œäººæ ‡è¯†ï¼Œè¿”å›é€šç”¨æ ‡è¯†
        return "Target"
    
    def _explain_relevance(self, query_tokens: List[str], chunk: Dict[str, Any], original_query: str = None) -> str:
        """è§£é‡ŠåŒ¹é…ç›¸å…³æ€§ï¼Œä¸“æ³¨äºè¯æ±‡åŒ¹é…è€Œéæƒé‡"""
        chunk_text = self.build_enhanced_text(chunk).lower()
        
        matched_terms = []
        original_terms = []
        
        # å¦‚æœæœ‰åŸå§‹æŸ¥è¯¢ï¼Œåˆ†æåŸå§‹æŸ¥è¯¢è¯çš„åŒ¹é…æƒ…å†µ
        if original_query:
            original_tokens = original_query.lower().split()
            for token in original_tokens:
                # æ£€æŸ¥åŸå§‹è¯å’Œè¯å¹²å½¢å¼çš„åŒ¹é…
                if token in chunk_text:
                    original_terms.append(token)
                else:
                    # æ£€æŸ¥è¯å¹²åŒ¹é…
                    # ç®€å•è¯å¹²æå–é€»è¾‘ï¼ˆä¸preprocess_textä¸­çš„ä¸€è‡´ï¼‰
                    if token.endswith('s') and len(token) > 3:
                        stemmed = token[:-1]
                        if stemmed in chunk_text:
                            original_terms.append(f"{token}->{stemmed}")
        
        # åˆ†æå¤„ç†åçš„tokenåŒ¹é…
        for token in set(query_tokens):  # å»é‡
            if token in chunk_text:
                matched_terms.append(token)
        
        # æ„å»ºåŒ¹é…è¯´æ˜
        if original_terms and matched_terms:
            return f"åŒ¹é…: {', '.join(original_terms[:3])} | å¤„ç†å: {', '.join(matched_terms[:3])}"
        elif matched_terms:
            return f"åŒ¹é…: {', '.join(matched_terms[:5])}"
        elif original_terms:
            return f"åŒ¹é…: {', '.join(original_terms[:5])}"
        else:
            return "æ— æ˜æ˜¾åŒ¹é…"
    
    def save_index(self, path: str) -> None:
        """
        ä¿å­˜ç®€åŒ–BM25ç´¢å¼•
        
        Raises:
            BM25UnavailableError: å½“BM25åŠŸèƒ½ä¸å¯ç”¨æ—¶
        """
        if not BM25_AVAILABLE:
            raise BM25UnavailableError(t("bm25_save_not_available"))
            
        try:
            # ä½¿ç”¨bm25sçš„ä¿å­˜æ–¹æ³•
            path_obj = Path(path)
            bm25_dir = path_obj.parent / f"{path_obj.stem}_bm25s"
            
            # ä¿å­˜BM25ç´¢å¼•
            self.bm25.save(str(bm25_dir))
            
            # ä¿å­˜é™„åŠ æ•°æ®ï¼ˆæ–‡æ¡£å’Œåœç”¨è¯ï¼‰
            additional_data = {
                'documents': self.documents,
                'stop_words': list(self.stop_words),
                'corpus_tokens': getattr(self, 'corpus_tokens', [])
            }
            
            with open(path, 'wb') as f:
                pickle.dump(additional_data, f)
            
            logger.info(f"ç®€åŒ–BM25ç´¢å¼•å·²ä¿å­˜åˆ°: {path} (BM25æ•°æ®: {bm25_dir})")
            
        except Exception as e:
            error_msg = t("bm25_save_failed", error=str(e))
            logger.error(error_msg)
            raise BM25UnavailableError(error_msg)
    
    def load_index(self, path: str) -> None:
        """
        åŠ è½½ç®€åŒ–BM25ç´¢å¼•
        
        Raises:
            BM25UnavailableError: å½“BM25åŠŸèƒ½ä¸å¯ç”¨æ—¶
        """
        if not BM25_AVAILABLE:
            error_msg = t("bm25_package_unavailable", error=BM25_IMPORT_ERROR)
            logger.error(error_msg)
            raise BM25UnavailableError(error_msg)
            
        try:
            # åŠ è½½é™„åŠ æ•°æ®
            with open(path, 'rb') as f:
                data = pickle.load(f)
                
            self.documents = data['documents']
            self.stop_words = set(data.get('stop_words', []))
            self.corpus_tokens = data.get('corpus_tokens', [])
            
            # åŠ è½½BM25ç´¢å¼•
            path_obj = Path(path)
            bm25_dir = path_obj.parent / f"{path_obj.stem}_bm25s"
            
            if bm25_dir.exists():
                self.bm25 = bm25s.BM25.load(str(bm25_dir))
            else:
                # å¦‚æœbm25sç›®å½•ä¸å­˜åœ¨ï¼Œå°è¯•é‡å»ºç´¢å¼•
                logger.warning(f"BM25ç´¢å¼•ç›®å½•ä¸å­˜åœ¨: {bm25_dir}ï¼Œå°è¯•é‡å»ºç´¢å¼•")
                if self.corpus_tokens:
                    self.bm25 = bm25s.BM25()
                    self.bm25.index(self.corpus_tokens)
                else:
                    raise FileNotFoundError(t("bm25_index_missing", path=str(bm25_dir)))
            
            logger.info(f"ç®€åŒ–BM25ç´¢å¼•å·²åŠ è½½: {path}")
            
        except Exception as e:
            error_msg = t("bm25_load_failed", error=str(e))
            logger.error(error_msg)
            raise BM25UnavailableError(error_msg)
    
    def get_stats(self) -> Dict[str, Any]:
        """
        è·å–å¢å¼ºç´¢å¼•ç»Ÿè®¡ä¿¡æ¯
        
        Raises:
            BM25UnavailableError: å½“BM25åŠŸèƒ½ä¸å¯ç”¨æ—¶
        """
        if not BM25_AVAILABLE:
            raise BM25UnavailableError(t("bm25_stats_failed"))
            
        if not self.bm25:
            return {"status": "æœªåˆå§‹åŒ–", "error": "BM25ç´¢å¼•æœªæ„å»º"}
        
        # åˆ†ææ•Œäººåˆ†å¸ƒ
        enemy_distribution = {}
        for chunk in self.documents:
            enemy = self._extract_enemy_from_chunk(chunk)
            enemy_distribution[enemy] = enemy_distribution.get(enemy, 0) + 1
        
        # è®¡ç®—å¹³å‡æ–‡æ¡£é•¿åº¦ï¼ˆä¿®å¤corpus_sizeè®¿é—®é”™è¯¯ï¼‰
        try:
            # bm25sçš„corpusæ˜¯æ–‡æ¡£tokenåˆ—è¡¨çš„åˆ—è¡¨
            if hasattr(self.bm25, 'corpus') and self.bm25.corpus:
                avg_doc_length = sum(len(doc) for doc in self.bm25.corpus) / len(self.bm25.corpus)
            elif hasattr(self.bm25, 'corpus_size') and isinstance(self.bm25.corpus_size, int):
                # å¦‚æœcorpus_sizeæ˜¯æ•´æ•°ï¼Œè¡¨ç¤ºæ–‡æ¡£æ•°é‡
                avg_doc_length = float(self.bm25.corpus_size)
            else:
                avg_doc_length = 0.0
        except Exception as e:
            logger.warning(f"è®¡ç®—å¹³å‡æ–‡æ¡£é•¿åº¦å¤±è´¥: {e}")
            avg_doc_length = 0.0
        
        return {
            "status": "å·²åˆå§‹åŒ–",
            "document_count": len(self.documents),
            "stop_words_count": len(self.stop_words),
            "enemy_distribution": enemy_distribution,
            "average_document_length": avg_doc_length,
            "top_enemies": list(sorted(enemy_distribution.items(), key=lambda x: x[1], reverse=True)[:5])
        }


def test_enhanced_bm25():
    """æµ‹è¯•å¢å¼ºBM25ç´¢å¼•å™¨ - å¤šæ¸¸æˆæ”¯æŒ"""
    
    # Helldivers 2 æµ‹è¯•æ•°æ®
    helldivers_chunks = [
        {
            "chunk_id": "bile_titan_test",
            "topic": "Terminid: Bile Titan Weaknesses",
            "summary": "This guide details how to kill a Bile Titan. Its head is a critical weak point (1500 HP, Class 4 Armor) that can be one-shot by anti-tank launchers for an instant kill.",
            "keywords": ["Bile Titan", "Terminid", "boss weakness", "anti-tank", "headshot"],
            "structured_data": {
                "enemy_name": "Bile Titan",
                "faction": "Terminid",
                "weak_points": [
                    {
                        "name": "Head/Face",
                        "health": 1500,
                        "notes": "Instant kill if destroyed. Ideal target for anti-tank launchers."
                    }
                ],
                "recommended_weapons": ["EAT", "Recoilless Rifle", "Quasar Cannon"]
            }
        }
    ]
    
    # DST æµ‹è¯•æ•°æ®
    dst_chunks = [
        {
            "chunk_id": "dst_winter_test",
            "topic": "Winter Survival: Managing Temperature and Deerclops",
            "summary": "Surviving winter requires thermal stone management and preparing for Deerclops boss fight around day 30.",
            "keywords": ["winter", "temperature", "deerclops", "boss", "thermal stone"],
            "data": {
                "season": "Winter",
                "boss_name": "Deerclops",
                "key_items": ["Thermal Stone", "Winter Hat", "Fire Pit"]
            }
        }
    ]
    
    # Elden Ring æµ‹è¯•æ•°æ®
    eldenring_chunks = [
        {
            "chunk_id": "malenia_test",
            "topic": "Malenia Boss Strategy: Waterfowl Dance Counter",
            "summary": "Malenia's waterfowl dance can be dodged by running away during the first flurry, then dodging through the second and third attacks.",
            "keywords": ["Malenia", "waterfowl dance", "boss strategy", "dodge", "timing"],
            "structured_data": {
                "boss_name": "Malenia",
                "difficulty": "Very Hard",
                "key_attacks": ["Waterfowl Dance", "Scarlet Rot"]
            }
        }
    ]
    
    # æµ‹è¯•ä¸åŒæ¸¸æˆçš„ç´¢å¼•å™¨
    test_cases = [
        ("helldiver2", helldivers_chunks, ["how to kill bile titan", "anti-tank weapons"]),
        ("dst", dst_chunks, ["winter survival", "deerclops strategy"]),
        ("eldenring", eldenring_chunks, ["malenia boss fight", "waterfowl dance counter"])
    ]
    
    print("=== å¤šæ¸¸æˆå¢å¼ºBM25ç´¢å¼•å™¨æµ‹è¯• ===\n")
    
    for game_name, chunks, queries in test_cases:
        print(f"ğŸ® æµ‹è¯•æ¸¸æˆ: {game_name.upper()}")
        print(f"ğŸ“š çŸ¥è¯†å—æ•°é‡: {len(chunks)}")
        
        # åˆ›å»ºæ¸¸æˆç‰¹å®šçš„ç´¢å¼•å™¨
        indexer = EnhancedBM25Indexer(game_name=game_name)
        
        # æ„å»ºç´¢å¼•
        indexer.build_index(chunks)
        
        # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
        stats = indexer.get_stats()
        print(f"ğŸ“Š ç´¢å¼•ç»Ÿè®¡: æ–‡æ¡£æ•°={stats['document_count']}, åœç”¨è¯æ•°={stats['stop_words_count']}")
        
        # æµ‹è¯•æŸ¥è¯¢
        for query in queries:
            print(f"\nğŸ” æŸ¥è¯¢: {query}")
            results = indexer.search(query, top_k=2)
            for i, result in enumerate(results, 1):
                print(f"   {i}. åˆ†æ•°={result['score']:.3f} | {result['chunk']['topic']}")
                print(f"      ç›¸å…³æ€§: {result['match_info']['relevance_reason']}")
        
        print("\n" + "="*50 + "\n")


if __name__ == "__main__":
    test_enhanced_bm25() 