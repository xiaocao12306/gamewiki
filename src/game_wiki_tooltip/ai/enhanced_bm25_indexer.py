"""
å¢å¼ºBM25ç´¢å¼•å™¨ - é’ˆå¯¹æ¸¸æˆæˆ˜æœ¯ä¿¡æ¯ä¼˜åŒ–
=====================================

åŠŸèƒ½ï¼š
1. æ•Œäººç‰¹å®šå…³é”®è¯æƒé‡æå‡
2. æˆ˜æœ¯æœ¯è¯­æƒé‡å¢å¼º  
3. æ™ºèƒ½æ–‡æœ¬é¢„å¤„ç†
4. å¤šè¯­è¨€æ”¯æŒï¼ˆä¸­è‹±æ–‡ï¼‰
"""

import jieba
import json
import pickle
import re
import logging
from rank_bm25 import BM25Okapi
from typing import List, Dict, Any, Optional, Set, Tuple
from pathlib import Path
from .game_keyword_configs import GameKeywordConfig

logger = logging.getLogger(__name__)

class EnhancedBM25Indexer:
    """å¢å¼ºBM25ç´¢å¼•å™¨ï¼Œæ”¯æŒå¤šæ¸¸æˆä¼˜åŒ–çš„æˆ˜æœ¯ä¿¡æ¯æ£€ç´¢"""
    
    def __init__(self, game_name: str = "helldiver2", stop_words: Optional[List[str]] = None):
        """
        åˆå§‹åŒ–å¢å¼ºBM25ç´¢å¼•å™¨
        
        Args:
            game_name: æ¸¸æˆåç§° (helldiver2, dst, eldenring, civilization6)
            stop_words: åœç”¨è¯åˆ—è¡¨
        """
        self.game_name = game_name
        self.bm25 = None
        self.documents = []
        self.stop_words = self._load_stop_words(stop_words)
        
        # æ ¹æ®æ¸¸æˆåŠ è½½ç‰¹å®šçš„å…³é”®è¯æƒé‡é…ç½®
        self._load_game_config()
    
    def _load_game_config(self) -> None:
        """åŠ è½½æ¸¸æˆç‰¹å®šçš„å…³é”®è¯é…ç½®"""
        try:
            config = GameKeywordConfig.get_config(self.game_name)
            
            # åˆå¹¶æ‰€æœ‰å…³é”®è¯æƒé‡
            self.keyword_weights = {}
            self.keyword_weights.update(config['common_keywords'])
            self.keyword_weights.update(config['enemy_keywords'])
            self.keyword_weights.update(config['tactical_keywords'])
            self.keyword_weights.update(config['item_keywords'])
            self.keyword_weights.update(config['special_keywords'])
            
            # ä¿å­˜å„ç±»åˆ«å…³é”®è¯ç”¨äºç‰¹æ®Šå¤„ç†
            self.enemy_keywords = config['enemy_keywords']
            self.tactical_keywords = config['tactical_keywords']
            self.item_keywords = config['item_keywords']
            self.special_keywords = config['special_keywords']
            
            logger.info(f"å·²åŠ è½½ {self.game_name} æ¸¸æˆé…ç½®ï¼Œå…± {len(self.keyword_weights)} ä¸ªå…³é”®è¯")
            
        except Exception as e:
            logger.error(f"åŠ è½½æ¸¸æˆé…ç½®å¤±è´¥: {e}")
            # é™çº§åˆ°ç©ºé…ç½®
            self.keyword_weights = {}
            self.enemy_keywords = {}
            self.tactical_keywords = {}
            self.item_keywords = {}
            self.special_keywords = {}
        
    def _load_stop_words(self, stop_words: Optional[List[str]] = None) -> Set[str]:
        """åŠ è½½åœç”¨è¯ï¼Œä½†ä¿ç•™é‡è¦çš„æˆ˜æœ¯æœ¯è¯­"""
        default_stop_words = {
            # ä¸­æ–‡åœç”¨è¯
            'çš„', 'äº†', 'åœ¨', 'æ˜¯', 'æˆ‘', 'æœ‰', 'å’Œ', 'å°±', 'ä¸', 'äºº', 'éƒ½', 'ä¸€', 'ä¸€ä¸ª', 'ä¸Š', 'ä¹Ÿ', 'å¾ˆ', 'åˆ°', 'è¯´', 'è¦', 'å»', 'ä½ ', 'ä¼š', 'ç€', 'æ²¡æœ‰', 'çœ‹', 'å¥½', 'è‡ªå·±', 'è¿™',
            # è‹±æ–‡åœç”¨è¯  
            'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by', 'from', 'as', 'is', 'was', 'are', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'should', 'could', 'can', 'may', 'might', 'must', 'shall',
            # é€šç”¨æ¸¸æˆè¯æ±‡ï¼ˆä½†ä¸åŒ…æ‹¬æˆ˜æœ¯æœ¯è¯­ï¼‰
            'game', 'player', 'mission', 'level'
        }
        
        if stop_words:
            default_stop_words.update(stop_words)
            
        return default_stop_words
        
    def _normalize_enemy_name(self, text: str) -> str:
        """æ ‡å‡†åŒ–æ•Œäººåç§° - åŸºäºå½“å‰æ¸¸æˆé…ç½®"""
        text = text.lower()
        
        # åŸºäºæ¸¸æˆç‰¹å®šçš„æ•Œäººå…³é”®è¯è¿›è¡Œæ ‡å‡†åŒ–
        # è¿™é‡Œæˆ‘ä»¬ä½¿ç”¨ä¸€ä¸ªé€šç”¨çš„æ–¹æ³•ï¼Œä¸å†ç¡¬ç¼–ç ç‰¹å®šæ¸¸æˆçš„æ˜ å°„
        # å¯ä»¥æ ¹æ®éœ€è¦åœ¨æ¸¸æˆé…ç½®ä¸­æ·»åŠ åˆ«åæ˜ å°„
        
        # é’ˆå¯¹Helldivers 2çš„ç‰¹æ®Šå¤„ç† (ä¿ç•™å‘åå…¼å®¹æ€§)
        if self.game_name == "helldiver2":
            enemy_mappings = {
                'bt': 'bile titan',
                'biletitan': 'bile titan',
                'bile_titan': 'bile titan',
                'èƒ†æ±æ³°å¦': 'bile titan',
                'å·¨äººæœºç”²': 'hulk',
                'hulk devastator': 'hulk',
                'å†²é”‹è€…': 'charger',
                'ç©¿åˆºè€…': 'impaler',
                'æ½œè¡Œè€…': 'stalker',
                'æ—ç¾¤æŒ‡æŒ¥å®˜': 'brood commander',
                'å·¥å‚è¡Œè€…': 'factory strider',
                'æ¯ç­è€…': 'devastator',
                'ç‹‚æˆ˜å£«': 'berserker',
                'æ­¦è£…ç›´å‡æœº': 'gunship',
                'å¦å…‹': 'tank',
                'è¿è¾“èˆ°': 'dropship',
            }
            
            for original, normalized in enemy_mappings.items():
                text = text.replace(original, normalized)
            
        return text
        
    def preprocess_text(self, text: str) -> List[str]:
        """
        å¢å¼ºæ–‡æœ¬é¢„å¤„ç†ï¼Œé‡ç‚¹å¤„ç†æˆ˜æœ¯ä¿¡æ¯
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            
        Returns:
            å¤„ç†åçš„tokenåˆ—è¡¨ï¼ŒåŒ…å«æƒé‡ä¿¡æ¯
        """
        if not text:
            return []
            
        # è½¬æ¢ä¸ºå°å†™å¹¶æ ‡å‡†åŒ–æ•Œäººåç§°
        text = self._normalize_enemy_name(text.lower())
        
        # ç§»é™¤ç‰¹æ®Šå­—ç¬¦ï¼Œä½†ä¿ç•™ä¸­æ–‡ã€è‹±æ–‡ã€æ•°å­—å’Œç©ºæ ¼
        text = re.sub(r'[^\u4e00-\u9fa5a-zA-Z0-9\s]', ' ', text)
        
        # ä¸­æ–‡åˆ†è¯
        tokens = list(jieba.cut(text))
        
        # å¤„ç†tokenå¹¶åº”ç”¨æƒé‡
        weighted_tokens = []
        for token in tokens:
            token = token.strip()
            
            # è¿‡æ»¤æ¡ä»¶ï¼šéç©ºã€ä¸æ˜¯åœç”¨è¯ã€é•¿åº¦>1æˆ–è€…æ˜¯æ•°å­—
            if (token and 
                token not in self.stop_words and 
                (len(token) > 1 or token.isdigit())):
                
                # æ£€æŸ¥æ˜¯å¦æ˜¯é«˜æƒé‡å…³é”®è¯
                weight = self.keyword_weights.get(token, 1.0)
                
                # æ ¹æ®æƒé‡é‡å¤token
                repeat_count = int(weight)
                weighted_tokens.extend([token] * repeat_count)
        
        return weighted_tokens
    
    def build_enhanced_text(self, chunk: Dict[str, Any]) -> str:
        """
        æ„å»ºå¢å¼ºçš„æœç´¢æ–‡æœ¬ï¼Œä¼˜åŒ–æ•Œäººå’Œæˆ˜æœ¯ä¿¡æ¯
        
        Args:
            chunk: çŸ¥è¯†å—
            
        Returns:
            å¢å¼ºçš„æœç´¢æ–‡æœ¬
        """
        text_parts = []
        
        # 1. Topic (æœ€é«˜æƒé‡ - é‡å¤5æ¬¡)
        topic = chunk.get("topic", "")
        if topic:
            text_parts.extend([topic] * 5)
            
        # 2. å…³é”®è¯ (é«˜æƒé‡ - é‡å¤3æ¬¡)
        keywords = chunk.get("keywords", [])
        if keywords:
            text_parts.extend(keywords * 3)
            
        # 3. Summary (æ­£å¸¸æƒé‡)
        summary = chunk.get("summary", "")
        if summary:
            text_parts.append(summary)
            
        # 4. ç»“æ„åŒ–æ•°æ®å¤„ç†
        self._extract_structured_content(chunk, text_parts)
        
        return " ".join(text_parts)
    
    def _extract_structured_content(self, chunk: Dict[str, Any], text_parts: List[str]) -> None:
        """æå–ç»“æ„åŒ–å†…å®¹å¹¶æ·»åŠ åˆ°æ–‡æœ¬éƒ¨åˆ†"""
        
        # æ•Œäººå¼±ç‚¹ä¿¡æ¯
        if "structured_data" in chunk:
            structured = chunk["structured_data"]
            
            # æ•Œäººåç§° (é‡å¤4æ¬¡)
            if "enemy_name" in structured:
                text_parts.extend([structured["enemy_name"]] * 4)
                
            # å¼±ç‚¹ä¿¡æ¯ (é‡å¤3æ¬¡)
            if "weak_points" in structured:
                for weak_point in structured["weak_points"]:
                    if "name" in weak_point:
                        text_parts.extend([weak_point["name"]] * 3)
                    if "notes" in weak_point:
                        text_parts.append(weak_point["notes"])
                        
            # æ¨èæ­¦å™¨ (é‡å¤2æ¬¡)
            if "recommended_weapons" in structured:
                for weapon in structured["recommended_weapons"]:
                    text_parts.extend([weapon] * 2)
                    
        # Buildä¿¡æ¯
        if "build" in chunk:
            build = chunk["build"]
            
            # Buildåç§° (é‡å¤3æ¬¡)
            if "name" in build:
                text_parts.extend([build["name"]] * 3)
                
            # æˆ˜æœ¯ç„¦ç‚¹ (é‡å¤2æ¬¡)
            if "focus" in build:
                text_parts.extend([build["focus"]] * 2)
                
            # ç­–ç•¥ä¿¡æ¯
            if "stratagems" in build:
                for stratagem in build["stratagems"]:
                    if "name" in stratagem:
                        text_parts.extend([stratagem["name"]] * 2)
                    if "rationale" in stratagem:
                        text_parts.append(stratagem["rationale"])
    
    def build_index(self, chunks: List[Dict[str, Any]]) -> None:
        """
        æ„å»ºå¢å¼ºBM25ç´¢å¼•
        
        Args:
            chunks: çŸ¥è¯†å—åˆ—è¡¨
        """
        logger.info(f"å¼€å§‹æ„å»ºå¢å¼ºBM25ç´¢å¼•ï¼Œå…± {len(chunks)} ä¸ªçŸ¥è¯†å—")
        
        self.documents = chunks
        
        # æ„å»ºå¢å¼ºæœç´¢æ–‡æœ¬
        search_texts = []
        for i, chunk in enumerate(chunks):
            try:
                # æ„å»ºå¢å¼ºæ–‡æœ¬
                enhanced_text = self.build_enhanced_text(chunk)
                
                # é¢„å¤„ç†å’Œæƒé‡åŒ–
                tokenized = self.preprocess_text(enhanced_text)
                search_texts.append(tokenized)
                
                # è°ƒè¯•ä¿¡æ¯
                if i < 3:  # åªæ‰“å°å‰3ä¸ªç”¨äºè°ƒè¯•
                    logger.info(f"æ ·æœ¬ {i}: {chunk.get('topic', 'Unknown')}")
                    logger.info(f"å¢å¼ºæ–‡æœ¬: {enhanced_text[:200]}...")
                    logger.info(f"Tokenæ ·æœ¬: {tokenized[:10]}")
                    logger.info(f"Tokenæ€»æ•°: {len(tokenized)}")
                
            except Exception as e:
                logger.error(f"å¤„ç†ç¬¬ {i} ä¸ªçŸ¥è¯†å—æ—¶å‡ºé”™: {e}")
                search_texts.append([])
        
        # åˆ›å»ºBM25ç´¢å¼•
        try:
            self.bm25 = BM25Okapi(search_texts)
            logger.info("å¢å¼ºBM25ç´¢å¼•æ„å»ºå®Œæˆ")
        except Exception as e:
            logger.error(f"å¢å¼ºBM25ç´¢å¼•æ„å»ºå¤±è´¥: {e}")
            raise
    
    def search(self, query: str, top_k: int = 10) -> List[Dict[str, Any]]:
        """
        å¢å¼ºBM25æœç´¢
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            top_k: è¿”å›ç»“æœæ•°é‡
            
        Returns:
            æœç´¢ç»“æœåˆ—è¡¨
        """
        if not self.bm25:
            logger.warning("å¢å¼ºBM25ç´¢å¼•æœªåˆå§‹åŒ–")
            return []
            
        # é¢„å¤„ç†æŸ¥è¯¢
        normalized_query = self._normalize_enemy_name(query.lower())
        tokenized_query = self.preprocess_text(normalized_query)
        
        if not tokenized_query:
            logger.warning("æŸ¥è¯¢é¢„å¤„ç†åä¸ºç©º")
            return []
        
        print(f"ğŸ” [BM25-DEBUG] å¢å¼ºBM25æœç´¢ - åŸå§‹æŸ¥è¯¢: {query}")
        print(f"   ğŸ“ [BM25-DEBUG] æ ‡å‡†åŒ–æŸ¥è¯¢: {normalized_query}")
        print(f"   ğŸ”¤ [BM25-DEBUG] æƒé‡åŒ–åˆ†è¯: {tokenized_query}")
        logger.info(f"å¢å¼ºBM25æœç´¢ - åŸå§‹æŸ¥è¯¢: {query}")
        logger.info(f"æ ‡å‡†åŒ–æŸ¥è¯¢: {normalized_query}")
        logger.info(f"æƒé‡åŒ–åˆ†è¯: {tokenized_query}")
        
        # è·å–åˆ†æ•°
        scores = self.bm25.get_scores(tokenized_query)
        print(f"   ğŸ“Š [BM25-DEBUG] æ‰€æœ‰æ–‡æ¡£åˆ†æ•°èŒƒå›´: {scores.min():.4f} - {scores.max():.4f}")
        
        # è·å–top_kç»“æœ
        top_indices = scores.argsort()[-top_k:][::-1]
        print(f"   ğŸ“‹ [BM25-DEBUG] Top {top_k} ç´¢å¼•: {top_indices}")
        print(f"   ğŸ“‹ [BM25-DEBUG] å¯¹åº”åˆ†æ•°: {scores[top_indices]}")
        
        results = []
        for i, idx in enumerate(top_indices):
            score = scores[idx]
            if score > 0:
                chunk = self.documents[idx]
                match_info = {
                    "topic": chunk.get("topic", ""),
                    "enemy": self._extract_enemy_from_chunk(chunk),
                    "relevance_reason": self._explain_relevance(tokenized_query, chunk)
                }
                result = {
                    "chunk": chunk,
                    "score": float(score),
                    "rank": i + 1,
                    "match_info": match_info
                }
                results.append(result)
                
                # è¯¦ç»†çš„åŒ¹é…è°ƒè¯•ä¿¡æ¯
                print(f"   ğŸ“‹ [BM25-DEBUG] ç»“æœ {i+1}:")
                print(f"      - ç´¢å¼•: {idx}")
                print(f"      - åˆ†æ•°: {score:.4f}")
                print(f"      - ä¸»é¢˜: {chunk.get('topic', 'Unknown')}")
                print(f"      - æ•Œäºº: {match_info['enemy']}")
                print(f"      - åŒ¹é…ç†ç”±: {match_info['relevance_reason']}")
                print(f"      - æ‘˜è¦: {chunk.get('summary', '')[:100]}...")
                
                # æ˜¾ç¤ºå…³é”®è¯æƒé‡åŒ¹é…ä¿¡æ¯
                chunk_text = self.build_enhanced_text(chunk).lower()
                matched_keywords = []
                for token in set(tokenized_query):
                    if token in chunk_text:
                        weight = self.keyword_weights.get(token, 1.0)
                        if weight > 1.0:
                            matched_keywords.append(f"{token}({weight}x)")
                        else:
                            matched_keywords.append(token)
                if matched_keywords:
                    print(f"      - åŒ¹é…å…³é”®è¯: {', '.join(matched_keywords[:10])}")
        
        print(f"âœ… [BM25-DEBUG] å¢å¼ºBM25æœç´¢å®Œæˆï¼Œæ‰¾åˆ° {len(results)} ä¸ªç»“æœ")
        logger.info(f"å¢å¼ºBM25æœç´¢å®Œæˆï¼Œæ‰¾åˆ° {len(results)} ä¸ªç»“æœ")
        return results
    
    def _extract_enemy_from_chunk(self, chunk: Dict[str, Any]) -> str:
        """ä»chunkä¸­æå–æ•Œäºº/ç›®æ ‡åç§° - åŸºäºæ¸¸æˆç±»å‹"""
        # æ£€æŸ¥ç»“æ„åŒ–æ•°æ®
        if "structured_data" in chunk and "enemy_name" in chunk["structured_data"]:
            return chunk["structured_data"]["enemy_name"]
            
        # æ£€æŸ¥topicä¸­çš„æ•Œäººå…³é”®è¯
        topic = chunk.get("topic", "").lower()
        for enemy in self.enemy_keywords:
            if enemy in topic:
                return enemy.title()
        
        # å¯¹äºä¸åŒæ¸¸æˆç±»å‹ï¼ŒæŸ¥æ‰¾ä¸åŒçš„ç›®æ ‡ç±»å‹
        if self.game_name == "dst":
            # DST: æŸ¥æ‰¾Bossã€ç”Ÿç‰©æˆ–è§’è‰²å
            for special in self.special_keywords:
                if special in topic:
                    return special.title()
        elif self.game_name == "eldenring":
            # Elden Ring: æŸ¥æ‰¾Bossåç§°
            for enemy in self.enemy_keywords:
                if enemy in topic:
                    return enemy.title()
        elif self.game_name == "civilization6":
            # Civilization 6: æŸ¥æ‰¾æ–‡æ˜åç§°
            for civ in self.special_keywords:
                if civ in topic:
                    return civ.title()
                
        return "Unknown"
    
    def _explain_relevance(self, query_tokens: List[str], chunk: Dict[str, Any]) -> str:
        """è§£é‡ŠåŒ¹é…ç›¸å…³æ€§"""
        chunk_text = self.build_enhanced_text(chunk).lower()
        
        matched_terms = []
        for token in set(query_tokens):  # å»é‡
            if token in chunk_text:
                if token in self.keyword_weights:
                    matched_terms.append(f"{token}(æƒé‡:{self.keyword_weights[token]})")
                else:
                    matched_terms.append(token)
        
        return f"åŒ¹é…: {', '.join(matched_terms[:5])}"  # é™åˆ¶æ˜¾ç¤ºæ•°é‡
    
    def save_index(self, path: str) -> None:
        """ä¿å­˜å¢å¼ºBM25ç´¢å¼•"""
        try:
            data = {
                'bm25': self.bm25,
                'documents': self.documents,
                'stop_words': list(self.stop_words),
                'keyword_weights': self.keyword_weights
            }
            
            with open(path, 'wb') as f:
                pickle.dump(data, f)
            
            logger.info(f"å¢å¼ºBM25ç´¢å¼•å·²ä¿å­˜åˆ°: {path}")
            
        except Exception as e:
            logger.error(f"ä¿å­˜å¢å¼ºBM25ç´¢å¼•å¤±è´¥: {e}")
            raise
    
    def load_index(self, path: str) -> None:
        """åŠ è½½å¢å¼ºBM25ç´¢å¼•"""
        try:
            with open(path, 'rb') as f:
                data = pickle.load(f)
            
            self.bm25 = data['bm25']
            self.documents = data['documents']
            self.stop_words = set(data.get('stop_words', []))
            self.keyword_weights = data.get('keyword_weights', {})
            
            logger.info(f"å¢å¼ºBM25ç´¢å¼•å·²åŠ è½½: {path}")
            
        except Exception as e:
            logger.error(f"åŠ è½½å¢å¼ºBM25ç´¢å¼•å¤±è´¥: {e}")
            raise
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–å¢å¼ºç´¢å¼•ç»Ÿè®¡ä¿¡æ¯"""
        if not self.bm25:
            return {"status": "æœªåˆå§‹åŒ–"}
        
        # åˆ†ææ•Œäººåˆ†å¸ƒ
        enemy_distribution = {}
        for chunk in self.documents:
            enemy = self._extract_enemy_from_chunk(chunk)
            enemy_distribution[enemy] = enemy_distribution.get(enemy, 0) + 1
        
        # è®¡ç®—å¹³å‡æ–‡æ¡£é•¿åº¦ï¼ˆä¿®å¤corpus_sizeè®¿é—®é”™è¯¯ï¼‰
        try:
            # BM25Okapiçš„corpusæ˜¯æ–‡æ¡£tokenåˆ—è¡¨çš„åˆ—è¡¨
            if hasattr(self.bm25, 'corpus') and self.bm25.corpus:
                avg_doc_length = sum(len(doc) for doc in self.bm25.corpus) / len(self.bm25.corpus)
            elif hasattr(self.bm25, 'corpus_size') and isinstance(self.bm25.corpus_size, int):
                # å¦‚æœcorpus_sizeæ˜¯æ•´æ•°ï¼Œè¡¨ç¤ºæ–‡æ¡£æ•°é‡
                avg_doc_length = float(self.bm25.corpus_size)
            else:
                avg_doc_length = 0.0
        except Exception as e:
            logger.warning(f"è®¡ç®—å¹³å‡æ–‡æ¡£é•¿åº¦å¤±è´¥: {e}")
            avg_doc_length = 0.0
        
        return {
            "status": "å·²åˆå§‹åŒ–",
            "document_count": len(self.documents),
            "stop_words_count": len(self.stop_words),
            "keyword_weights_count": len(self.keyword_weights),
            "enemy_distribution": enemy_distribution,
            "average_document_length": avg_doc_length,
            "top_enemies": list(sorted(enemy_distribution.items(), key=lambda x: x[1], reverse=True)[:5])
        }


def test_enhanced_bm25():
    """æµ‹è¯•å¢å¼ºBM25ç´¢å¼•å™¨ - å¤šæ¸¸æˆæ”¯æŒ"""
    
    # Helldivers 2 æµ‹è¯•æ•°æ®
    helldivers_chunks = [
        {
            "chunk_id": "bile_titan_test",
            "topic": "Terminid: Bile Titan Weaknesses",
            "summary": "This guide details how to kill a Bile Titan. Its head is a critical weak point (1500 HP, Class 4 Armor) that can be one-shot by anti-tank launchers for an instant kill.",
            "keywords": ["Bile Titan", "Terminid", "boss weakness", "anti-tank", "headshot"],
            "structured_data": {
                "enemy_name": "Bile Titan",
                "faction": "Terminid",
                "weak_points": [
                    {
                        "name": "Head/Face",
                        "health": 1500,
                        "notes": "Instant kill if destroyed. Ideal target for anti-tank launchers."
                    }
                ],
                "recommended_weapons": ["EAT", "Recoilless Rifle", "Quasar Cannon"]
            }
        }
    ]
    
    # DST æµ‹è¯•æ•°æ®
    dst_chunks = [
        {
            "chunk_id": "dst_winter_test",
            "topic": "Winter Survival: Managing Temperature and Deerclops",
            "summary": "Surviving winter requires thermal stone management and preparing for Deerclops boss fight around day 30.",
            "keywords": ["winter", "temperature", "deerclops", "boss", "thermal stone"],
            "data": {
                "season": "Winter",
                "boss_name": "Deerclops",
                "key_items": ["Thermal Stone", "Winter Hat", "Fire Pit"]
            }
        }
    ]
    
    # Elden Ring æµ‹è¯•æ•°æ®
    eldenring_chunks = [
        {
            "chunk_id": "malenia_test",
            "topic": "Malenia Boss Strategy: Waterfowl Dance Counter",
            "summary": "Malenia's waterfowl dance can be dodged by running away during the first flurry, then dodging through the second and third attacks.",
            "keywords": ["Malenia", "waterfowl dance", "boss strategy", "dodge", "timing"],
            "structured_data": {
                "boss_name": "Malenia",
                "difficulty": "Very Hard",
                "key_attacks": ["Waterfowl Dance", "Scarlet Rot"]
            }
        }
    ]
    
    # æµ‹è¯•ä¸åŒæ¸¸æˆçš„ç´¢å¼•å™¨
    test_cases = [
        ("helldiver2", helldivers_chunks, ["how to kill bile titan", "anti-tank weapons"]),
        ("dst", dst_chunks, ["winter survival", "deerclops strategy"]),
        ("eldenring", eldenring_chunks, ["malenia boss fight", "waterfowl dance counter"])
    ]
    
    print("=== å¤šæ¸¸æˆå¢å¼ºBM25ç´¢å¼•å™¨æµ‹è¯• ===\n")
    
    for game_name, chunks, queries in test_cases:
        print(f"ğŸ® æµ‹è¯•æ¸¸æˆ: {game_name.upper()}")
        print(f"ğŸ“š çŸ¥è¯†å—æ•°é‡: {len(chunks)}")
        
        # åˆ›å»ºæ¸¸æˆç‰¹å®šçš„ç´¢å¼•å™¨
        indexer = EnhancedBM25Indexer(game_name=game_name)
        
        # æ„å»ºç´¢å¼•
        indexer.build_index(chunks)
        
        # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
        stats = indexer.get_stats()
        print(f"ğŸ“Š ç´¢å¼•ç»Ÿè®¡: æ–‡æ¡£æ•°={stats['document_count']}, å…³é”®è¯æ•°={stats['keyword_weights_count']}")
        
        # æµ‹è¯•æŸ¥è¯¢
        for query in queries:
            print(f"\nğŸ” æŸ¥è¯¢: {query}")
            results = indexer.search(query, top_k=2)
            for i, result in enumerate(results, 1):
                print(f"   {i}. åˆ†æ•°={result['score']:.3f} | {result['chunk']['topic']}")
                print(f"      ç›¸å…³æ€§: {result['match_info']['relevance_reason']}")
        
        print("\n" + "="*50 + "\n")


if __name__ == "__main__":
    test_enhanced_bm25() 