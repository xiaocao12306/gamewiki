"""
å¢å¼ºBM25ç´¢å¼•å™¨ - é’ˆå¯¹æ¸¸æˆæˆ˜æœ¯ä¿¡æ¯ä¼˜åŒ–
=====================================

åŠŸèƒ½ï¼š
1. æ•Œäººç‰¹å®šå…³é”®è¯æƒé‡æå‡
2. æˆ˜æœ¯æœ¯è¯­æƒé‡å¢å¼º  
3. æ™ºèƒ½æ–‡æœ¬é¢„å¤„ç†
4. å¤šè¯­è¨€æ”¯æŒï¼ˆä¸­è‹±æ–‡ï¼‰
"""

import jieba
import json
import pickle
import re
import logging
from rank_bm25 import BM25Okapi
from typing import List, Dict, Any, Optional, Set, Tuple
from pathlib import Path

logger = logging.getLogger(__name__)

class EnhancedBM25Indexer:
    """å¢å¼ºBM25ç´¢å¼•å™¨ï¼Œä¸“é—¨ä¼˜åŒ–æ¸¸æˆæˆ˜æœ¯ä¿¡æ¯æ£€ç´¢"""
    
    # æ•Œäººåç§°æ˜ å°„å’Œæƒé‡
    ENEMY_KEYWORDS = {
        # Terminidæ•Œäºº
        'bile titan': 5.0,
        'biletitan': 5.0, 
        'bile_titan': 5.0,
        'èƒ†æ±æ³°å¦': 5.0,
        'bt': 4.0,  # å¸¸è§ç¼©å†™
        
        'charger': 4.0,
        'å†²é”‹è€…': 4.0,
        
        'hulk': 5.0,
        'å·¨äººæœºç”²': 5.0,
        'hulk devastator': 5.0,
        
        'impaler': 4.0,
        'ç©¿åˆºè€…': 4.0,
        
        'brood commander': 3.5,
        'æ—ç¾¤æŒ‡æŒ¥å®˜': 3.5,
        
        'stalker': 3.5,
        'æ½œè¡Œè€…': 3.5,
        
        # Automatonæ•Œäºº  
        'factory strider': 4.5,
        'å·¥å‚è¡Œè€…': 4.5,
        
        'devastator': 4.0,
        'æ¯ç­è€…': 4.0,
        
        'berserker': 3.5,
        'ç‹‚æˆ˜å£«': 3.5,
        
        'gunship': 3.5,
        'æ­¦è£…ç›´å‡æœº': 3.5,
        
        'tank': 4.0,
        'å¦å…‹': 4.0,
        
        'dropship': 3.5,
        'è¿è¾“èˆ°': 3.5,
    }
    
    # æˆ˜æœ¯æœ¯è¯­æƒé‡
    TACTICAL_KEYWORDS = {
        # å¼±ç‚¹ç›¸å…³
        'weak point': 4.0,
        'weakness': 4.0,
        'vulnerable': 3.5,
        'weak spot': 4.0,
        'critical': 3.5,
        'å¼±ç‚¹': 4.0,
        'è¦å®³': 3.5,
        'è‡´å‘½': 3.5,
        
        # å‡»æ€ç›¸å…³
        'kill': 3.5,
        'destroy': 3.5,
        'eliminate': 3.0,
        'defeat': 3.0,
        'how to kill': 4.0,
        'å‡»æ€': 3.5,
        'æ¶ˆç­': 3.5,
        'å‡»è´¥': 3.0,
        
        # æˆ˜æœ¯ç›¸å…³
        'strategy': 3.5,
        'tactic': 3.5,
        'counter': 3.0,
        'effective': 3.0,
        'recommended': 3.0,
        'ç­–ç•¥': 3.5,
        'æˆ˜æœ¯': 3.5,
        'å¯¹æŠ—': 3.0,
        'æ¨è': 3.0,
        
        # æ­¦å™¨è£…å¤‡
        'weapon': 3.0,
        'loadout': 3.0,
        'build': 3.0,
        'equipment': 2.5,
        'æ­¦å™¨': 3.0,
        'é…è£…': 3.0,
        'è£…å¤‡': 2.5,
        
        # éƒ¨ä½ç›¸å…³
        'head': 3.0,
        'headshot': 3.5,
        'belly': 3.0,
        'eye': 3.5,
        'back': 2.5,
        'leg': 2.5,
        'å¤´éƒ¨': 3.0,
        'è…¹éƒ¨': 3.0,
        'çœ¼ç›': 3.5,
        'èƒŒéƒ¨': 2.5,
        'è…¿éƒ¨': 2.5,
    }
    
    # æ­¦å™¨åˆ†ç±»æƒé‡
    WEAPON_KEYWORDS = {
        # åå¦å…‹æ­¦å™¨
        'anti-tank': 2.5,
        'railgun': 2.5,
        'recoilless rifle': 2.5,
        'eat': 2.5,
        'quasar cannon': 2.5,
        'åå¦å…‹': 2.5,
        'è½¨é“ç‚®': 2.5,
        
        # çˆ†ç‚¸æ­¦å™¨
        'explosive': 2.0,
        'grenade launcher': 2.0,
        'autocannon': 2.0,
        'çˆ†ç‚¸': 2.0,
        'æ¦´å¼¹å‘å°„å™¨': 2.0,
        'è‡ªåŠ¨åŠ å†œç‚®': 2.0,
        
        # ç²¾ç¡®æ­¦å™¨
        'precision': 2.0,
        'sniper': 2.0,
        'anti-materiel': 2.0,
        'ç²¾ç¡®': 2.0,
        'ç‹™å‡»': 2.0,
        'åå™¨æ': 2.0,
    }
    
    def __init__(self, stop_words: Optional[List[str]] = None):
        """
        åˆå§‹åŒ–å¢å¼ºBM25ç´¢å¼•å™¨
        
        Args:
            stop_words: åœç”¨è¯åˆ—è¡¨
        """
        self.bm25 = None
        self.documents = []
        self.stop_words = self._load_stop_words(stop_words)
        
        # åˆå¹¶æ‰€æœ‰å…³é”®è¯æƒé‡
        self.keyword_weights = {}
        self.keyword_weights.update(self.ENEMY_KEYWORDS)
        self.keyword_weights.update(self.TACTICAL_KEYWORDS) 
        self.keyword_weights.update(self.WEAPON_KEYWORDS)
        
    def _load_stop_words(self, stop_words: Optional[List[str]] = None) -> Set[str]:
        """åŠ è½½åœç”¨è¯ï¼Œä½†ä¿ç•™é‡è¦çš„æˆ˜æœ¯æœ¯è¯­"""
        default_stop_words = {
            # ä¸­æ–‡åœç”¨è¯
            'çš„', 'äº†', 'åœ¨', 'æ˜¯', 'æˆ‘', 'æœ‰', 'å’Œ', 'å°±', 'ä¸', 'äºº', 'éƒ½', 'ä¸€', 'ä¸€ä¸ª', 'ä¸Š', 'ä¹Ÿ', 'å¾ˆ', 'åˆ°', 'è¯´', 'è¦', 'å»', 'ä½ ', 'ä¼š', 'ç€', 'æ²¡æœ‰', 'çœ‹', 'å¥½', 'è‡ªå·±', 'è¿™',
            # è‹±æ–‡åœç”¨è¯  
            'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by', 'from', 'as', 'is', 'was', 'are', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'should', 'could', 'can', 'may', 'might', 'must', 'shall',
            # é€šç”¨æ¸¸æˆè¯æ±‡ï¼ˆä½†ä¸åŒ…æ‹¬æˆ˜æœ¯æœ¯è¯­ï¼‰
            'game', 'player', 'mission', 'level'
        }
        
        if stop_words:
            default_stop_words.update(stop_words)
            
        return default_stop_words
        
    def _normalize_enemy_name(self, text: str) -> str:
        """æ ‡å‡†åŒ–æ•Œäººåç§°"""
        text = text.lower()
        
        # æ•Œäººåç§°æ ‡å‡†åŒ–
        enemy_mappings = {
            'bt': 'bile titan',
            'biletitan': 'bile titan',
            'bile_titan': 'bile titan',
            'èƒ†æ±æ³°å¦': 'bile titan',
            
            'å·¨äººæœºç”²': 'hulk',
            'hulk devastator': 'hulk',
            
            'å†²é”‹è€…': 'charger',
            'ç©¿åˆºè€…': 'impaler',
            'æ½œè¡Œè€…': 'stalker',
            'æ—ç¾¤æŒ‡æŒ¥å®˜': 'brood commander',
            
            'å·¥å‚è¡Œè€…': 'factory strider',
            'æ¯ç­è€…': 'devastator', 
            'ç‹‚æˆ˜å£«': 'berserker',
            'æ­¦è£…ç›´å‡æœº': 'gunship',
            'å¦å…‹': 'tank',
            'è¿è¾“èˆ°': 'dropship',
        }
        
        for original, normalized in enemy_mappings.items():
            text = text.replace(original, normalized)
            
        return text
        
    def preprocess_text(self, text: str) -> List[str]:
        """
        å¢å¼ºæ–‡æœ¬é¢„å¤„ç†ï¼Œé‡ç‚¹å¤„ç†æˆ˜æœ¯ä¿¡æ¯
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            
        Returns:
            å¤„ç†åçš„tokenåˆ—è¡¨ï¼ŒåŒ…å«æƒé‡ä¿¡æ¯
        """
        if not text:
            return []
            
        # è½¬æ¢ä¸ºå°å†™å¹¶æ ‡å‡†åŒ–æ•Œäººåç§°
        text = self._normalize_enemy_name(text.lower())
        
        # ç§»é™¤ç‰¹æ®Šå­—ç¬¦ï¼Œä½†ä¿ç•™ä¸­æ–‡ã€è‹±æ–‡ã€æ•°å­—å’Œç©ºæ ¼
        text = re.sub(r'[^\u4e00-\u9fa5a-zA-Z0-9\s]', ' ', text)
        
        # ä¸­æ–‡åˆ†è¯
        tokens = list(jieba.cut(text))
        
        # å¤„ç†tokenå¹¶åº”ç”¨æƒé‡
        weighted_tokens = []
        for token in tokens:
            token = token.strip()
            
            # è¿‡æ»¤æ¡ä»¶ï¼šéç©ºã€ä¸æ˜¯åœç”¨è¯ã€é•¿åº¦>1æˆ–è€…æ˜¯æ•°å­—
            if (token and 
                token not in self.stop_words and 
                (len(token) > 1 or token.isdigit())):
                
                # æ£€æŸ¥æ˜¯å¦æ˜¯é«˜æƒé‡å…³é”®è¯
                weight = self.keyword_weights.get(token, 1.0)
                
                # æ ¹æ®æƒé‡é‡å¤token
                repeat_count = int(weight)
                weighted_tokens.extend([token] * repeat_count)
        
        return weighted_tokens
    
    def build_enhanced_text(self, chunk: Dict[str, Any]) -> str:
        """
        æ„å»ºå¢å¼ºçš„æœç´¢æ–‡æœ¬ï¼Œä¼˜åŒ–æ•Œäººå’Œæˆ˜æœ¯ä¿¡æ¯
        
        Args:
            chunk: çŸ¥è¯†å—
            
        Returns:
            å¢å¼ºçš„æœç´¢æ–‡æœ¬
        """
        text_parts = []
        
        # 1. Topic (æœ€é«˜æƒé‡ - é‡å¤5æ¬¡)
        topic = chunk.get("topic", "")
        if topic:
            text_parts.extend([topic] * 5)
            
        # 2. å…³é”®è¯ (é«˜æƒé‡ - é‡å¤3æ¬¡)
        keywords = chunk.get("keywords", [])
        if keywords:
            text_parts.extend(keywords * 3)
            
        # 3. Summary (æ­£å¸¸æƒé‡)
        summary = chunk.get("summary", "")
        if summary:
            text_parts.append(summary)
            
        # 4. ç»“æ„åŒ–æ•°æ®å¤„ç†
        self._extract_structured_content(chunk, text_parts)
        
        return " ".join(text_parts)
    
    def _extract_structured_content(self, chunk: Dict[str, Any], text_parts: List[str]) -> None:
        """æå–ç»“æ„åŒ–å†…å®¹å¹¶æ·»åŠ åˆ°æ–‡æœ¬éƒ¨åˆ†"""
        
        # æ•Œäººå¼±ç‚¹ä¿¡æ¯
        if "structured_data" in chunk:
            structured = chunk["structured_data"]
            
            # æ•Œäººåç§° (é‡å¤4æ¬¡)
            if "enemy_name" in structured:
                text_parts.extend([structured["enemy_name"]] * 4)
                
            # å¼±ç‚¹ä¿¡æ¯ (é‡å¤3æ¬¡)
            if "weak_points" in structured:
                for weak_point in structured["weak_points"]:
                    if "name" in weak_point:
                        text_parts.extend([weak_point["name"]] * 3)
                    if "notes" in weak_point:
                        text_parts.append(weak_point["notes"])
                        
            # æ¨èæ­¦å™¨ (é‡å¤2æ¬¡)
            if "recommended_weapons" in structured:
                for weapon in structured["recommended_weapons"]:
                    text_parts.extend([weapon] * 2)
                    
        # Buildä¿¡æ¯
        if "build" in chunk:
            build = chunk["build"]
            
            # Buildåç§° (é‡å¤3æ¬¡)
            if "name" in build:
                text_parts.extend([build["name"]] * 3)
                
            # æˆ˜æœ¯ç„¦ç‚¹ (é‡å¤2æ¬¡)
            if "focus" in build:
                text_parts.extend([build["focus"]] * 2)
                
            # ç­–ç•¥ä¿¡æ¯
            if "stratagems" in build:
                for stratagem in build["stratagems"]:
                    if "name" in stratagem:
                        text_parts.extend([stratagem["name"]] * 2)
                    if "rationale" in stratagem:
                        text_parts.append(stratagem["rationale"])
    
    def build_index(self, chunks: List[Dict[str, Any]]) -> None:
        """
        æ„å»ºå¢å¼ºBM25ç´¢å¼•
        
        Args:
            chunks: çŸ¥è¯†å—åˆ—è¡¨
        """
        logger.info(f"å¼€å§‹æ„å»ºå¢å¼ºBM25ç´¢å¼•ï¼Œå…± {len(chunks)} ä¸ªçŸ¥è¯†å—")
        
        self.documents = chunks
        
        # æ„å»ºå¢å¼ºæœç´¢æ–‡æœ¬
        search_texts = []
        for i, chunk in enumerate(chunks):
            try:
                # æ„å»ºå¢å¼ºæ–‡æœ¬
                enhanced_text = self.build_enhanced_text(chunk)
                
                # é¢„å¤„ç†å’Œæƒé‡åŒ–
                tokenized = self.preprocess_text(enhanced_text)
                search_texts.append(tokenized)
                
                # è°ƒè¯•ä¿¡æ¯
                if i < 3:  # åªæ‰“å°å‰3ä¸ªç”¨äºè°ƒè¯•
                    logger.info(f"æ ·æœ¬ {i}: {chunk.get('topic', 'Unknown')}")
                    logger.info(f"Tokenæ ·æœ¬: {tokenized[:10]}")
                
            except Exception as e:
                logger.error(f"å¤„ç†ç¬¬ {i} ä¸ªçŸ¥è¯†å—æ—¶å‡ºé”™: {e}")
                search_texts.append([])
        
        # åˆ›å»ºBM25ç´¢å¼•
        try:
            self.bm25 = BM25Okapi(search_texts)
            logger.info("å¢å¼ºBM25ç´¢å¼•æ„å»ºå®Œæˆ")
        except Exception as e:
            logger.error(f"å¢å¼ºBM25ç´¢å¼•æ„å»ºå¤±è´¥: {e}")
            raise
    
    def search(self, query: str, top_k: int = 10) -> List[Dict[str, Any]]:
        """
        å¢å¼ºBM25æœç´¢
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            top_k: è¿”å›ç»“æœæ•°é‡
            
        Returns:
            æœç´¢ç»“æœåˆ—è¡¨
        """
        if not self.bm25:
            logger.warning("å¢å¼ºBM25ç´¢å¼•æœªåˆå§‹åŒ–")
            return []
            
        # é¢„å¤„ç†æŸ¥è¯¢
        normalized_query = self._normalize_enemy_name(query.lower())
        tokenized_query = self.preprocess_text(normalized_query)
        
        if not tokenized_query:
            logger.warning("æŸ¥è¯¢é¢„å¤„ç†åä¸ºç©º")
            return []
        
        print(f"ğŸ” [BM25-DEBUG] å¢å¼ºBM25æœç´¢ - åŸå§‹æŸ¥è¯¢: {query}")
        print(f"   ğŸ“ [BM25-DEBUG] æ ‡å‡†åŒ–æŸ¥è¯¢: {normalized_query}")
        print(f"   ğŸ”¤ [BM25-DEBUG] æƒé‡åŒ–åˆ†è¯: {tokenized_query}")
        logger.info(f"å¢å¼ºBM25æœç´¢ - åŸå§‹æŸ¥è¯¢: {query}")
        logger.info(f"æ ‡å‡†åŒ–æŸ¥è¯¢: {normalized_query}")
        logger.info(f"æƒé‡åŒ–åˆ†è¯: {tokenized_query}")
        
        # è·å–åˆ†æ•°
        scores = self.bm25.get_scores(tokenized_query)
        print(f"   ğŸ“Š [BM25-DEBUG] æ‰€æœ‰æ–‡æ¡£åˆ†æ•°èŒƒå›´: {scores.min():.4f} - {scores.max():.4f}")
        
        # è·å–top_kç»“æœ
        top_indices = scores.argsort()[-top_k:][::-1]
        print(f"   ğŸ“‹ [BM25-DEBUG] Top {top_k} ç´¢å¼•: {top_indices}")
        print(f"   ğŸ“‹ [BM25-DEBUG] å¯¹åº”åˆ†æ•°: {scores[top_indices]}")
        
        results = []
        for i, idx in enumerate(top_indices):
            score = scores[idx]
            if score > 0:
                chunk = self.documents[idx]
                match_info = {
                    "topic": chunk.get("topic", ""),
                    "enemy": self._extract_enemy_from_chunk(chunk),
                    "relevance_reason": self._explain_relevance(tokenized_query, chunk)
                }
                result = {
                    "chunk": chunk,
                    "score": float(score),
                    "rank": i + 1,
                    "match_info": match_info
                }
                results.append(result)
                
                # è¯¦ç»†çš„åŒ¹é…è°ƒè¯•ä¿¡æ¯
                print(f"   ğŸ“‹ [BM25-DEBUG] ç»“æœ {i+1}:")
                print(f"      - ç´¢å¼•: {idx}")
                print(f"      - åˆ†æ•°: {score:.4f}")
                print(f"      - ä¸»é¢˜: {chunk.get('topic', 'Unknown')}")
                print(f"      - æ•Œäºº: {match_info['enemy']}")
                print(f"      - åŒ¹é…ç†ç”±: {match_info['relevance_reason']}")
                print(f"      - æ‘˜è¦: {chunk.get('summary', '')[:100]}...")
                
                # æ˜¾ç¤ºå…³é”®è¯æƒé‡åŒ¹é…ä¿¡æ¯
                chunk_text = self.build_enhanced_text(chunk).lower()
                matched_keywords = []
                for token in set(tokenized_query):
                    if token in chunk_text:
                        weight = self.keyword_weights.get(token, 1.0)
                        if weight > 1.0:
                            matched_keywords.append(f"{token}({weight}x)")
                        else:
                            matched_keywords.append(token)
                if matched_keywords:
                    print(f"      - åŒ¹é…å…³é”®è¯: {', '.join(matched_keywords[:10])}")
        
        print(f"âœ… [BM25-DEBUG] å¢å¼ºBM25æœç´¢å®Œæˆï¼Œæ‰¾åˆ° {len(results)} ä¸ªç»“æœ")
        logger.info(f"å¢å¼ºBM25æœç´¢å®Œæˆï¼Œæ‰¾åˆ° {len(results)} ä¸ªç»“æœ")
        return results
    
    def _extract_enemy_from_chunk(self, chunk: Dict[str, Any]) -> str:
        """ä»chunkä¸­æå–æ•Œäººåç§°"""
        # æ£€æŸ¥ç»“æ„åŒ–æ•°æ®
        if "structured_data" in chunk and "enemy_name" in chunk["structured_data"]:
            return chunk["structured_data"]["enemy_name"]
            
        # æ£€æŸ¥topic
        topic = chunk.get("topic", "").lower()
        for enemy in self.ENEMY_KEYWORDS:
            if enemy in topic:
                return enemy.title()
                
        return "Unknown"
    
    def _explain_relevance(self, query_tokens: List[str], chunk: Dict[str, Any]) -> str:
        """è§£é‡ŠåŒ¹é…ç›¸å…³æ€§"""
        chunk_text = self.build_enhanced_text(chunk).lower()
        
        matched_terms = []
        for token in set(query_tokens):  # å»é‡
            if token in chunk_text:
                if token in self.keyword_weights:
                    matched_terms.append(f"{token}(æƒé‡:{self.keyword_weights[token]})")
                else:
                    matched_terms.append(token)
        
        return f"åŒ¹é…: {', '.join(matched_terms[:5])}"  # é™åˆ¶æ˜¾ç¤ºæ•°é‡
    
    def save_index(self, path: str) -> None:
        """ä¿å­˜å¢å¼ºBM25ç´¢å¼•"""
        try:
            data = {
                'bm25': self.bm25,
                'documents': self.documents,
                'stop_words': list(self.stop_words),
                'keyword_weights': self.keyword_weights
            }
            
            with open(path, 'wb') as f:
                pickle.dump(data, f)
            
            logger.info(f"å¢å¼ºBM25ç´¢å¼•å·²ä¿å­˜åˆ°: {path}")
            
        except Exception as e:
            logger.error(f"ä¿å­˜å¢å¼ºBM25ç´¢å¼•å¤±è´¥: {e}")
            raise
    
    def load_index(self, path: str) -> None:
        """åŠ è½½å¢å¼ºBM25ç´¢å¼•"""
        try:
            with open(path, 'rb') as f:
                data = pickle.load(f)
            
            self.bm25 = data['bm25']
            self.documents = data['documents']
            self.stop_words = set(data.get('stop_words', []))
            self.keyword_weights = data.get('keyword_weights', {})
            
            logger.info(f"å¢å¼ºBM25ç´¢å¼•å·²åŠ è½½: {path}")
            
        except Exception as e:
            logger.error(f"åŠ è½½å¢å¼ºBM25ç´¢å¼•å¤±è´¥: {e}")
            raise
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–å¢å¼ºç´¢å¼•ç»Ÿè®¡ä¿¡æ¯"""
        if not self.bm25:
            return {"status": "æœªåˆå§‹åŒ–"}
        
        # åˆ†ææ•Œäººåˆ†å¸ƒ
        enemy_distribution = {}
        for chunk in self.documents:
            enemy = self._extract_enemy_from_chunk(chunk)
            enemy_distribution[enemy] = enemy_distribution.get(enemy, 0) + 1
        
        # è®¡ç®—å¹³å‡æ–‡æ¡£é•¿åº¦ï¼ˆä¿®å¤corpus_sizeè®¿é—®é”™è¯¯ï¼‰
        try:
            # BM25Okapiçš„corpusæ˜¯æ–‡æ¡£tokenåˆ—è¡¨çš„åˆ—è¡¨
            if hasattr(self.bm25, 'corpus') and self.bm25.corpus:
                avg_doc_length = sum(len(doc) for doc in self.bm25.corpus) / len(self.bm25.corpus)
            elif hasattr(self.bm25, 'corpus_size') and isinstance(self.bm25.corpus_size, int):
                # å¦‚æœcorpus_sizeæ˜¯æ•´æ•°ï¼Œè¡¨ç¤ºæ–‡æ¡£æ•°é‡
                avg_doc_length = float(self.bm25.corpus_size)
            else:
                avg_doc_length = 0.0
        except Exception as e:
            logger.warning(f"è®¡ç®—å¹³å‡æ–‡æ¡£é•¿åº¦å¤±è´¥: {e}")
            avg_doc_length = 0.0
        
        return {
            "status": "å·²åˆå§‹åŒ–",
            "document_count": len(self.documents),
            "stop_words_count": len(self.stop_words),
            "keyword_weights_count": len(self.keyword_weights),
            "enemy_distribution": enemy_distribution,
            "average_document_length": avg_doc_length,
            "top_enemies": list(sorted(enemy_distribution.items(), key=lambda x: x[1], reverse=True)[:5])
        }


def test_enhanced_bm25():
    """æµ‹è¯•å¢å¼ºBM25ç´¢å¼•å™¨"""
    # ä½¿ç”¨å®é™…çš„chunkæ•°æ®è¿›è¡Œæµ‹è¯•
    test_chunks = [
        {
            "chunk_id": "bile_titan_test",
            "topic": "Terminid: Bile Titan Weaknesses",
            "summary": "This guide details how to kill a Bile Titan. Its head is a critical weak point (1500 HP, Class 4 Armor) that can be one-shot by anti-tank launchers for an instant kill.",
            "keywords": ["Bile Titan", "Terminid", "boss weakness", "anti-tank", "headshot"],
            "structured_data": {
                "enemy_name": "Bile Titan",
                "faction": "Terminid",
                "weak_points": [
                    {
                        "name": "Head/Face",
                        "health": 1500,
                        "notes": "Instant kill if destroyed. Ideal target for anti-tank launchers."
                    }
                ],
                "recommended_weapons": ["EAT", "Recoilless Rifle", "Quasar Cannon"]
            }
        },
        {
            "chunk_id": "hulk_test", 
            "topic": "Automaton: Hulk Weaknesses",
            "summary": "The Hulk's primary weak point is its glowing red eye socket. Stun grenades can immobilize it for easy targeting.",
            "keywords": ["Hulk", "Automaton", "eye socket", "stun grenade"],
            "structured_data": {
                "enemy_name": "Hulk",
                "faction": "Automaton",
                "weak_points": [
                    {
                        "name": "Eye Socket",
                        "notes": "Glowing red eye socket on head"
                    }
                ],
                "recommended_weapons": ["Railgun", "Anti-Materiel Rifle"]
            }
        }
    ]
    
    # åˆ›å»ºç´¢å¼•å™¨
    indexer = EnhancedBM25Indexer()
    
    # æ„å»ºç´¢å¼•
    indexer.build_index(test_chunks)
    
    # æµ‹è¯•æœç´¢
    print("=== å¢å¼ºBM25ç´¢å¼•å™¨æµ‹è¯• ===")
    print(f"ç´¢å¼•ç»Ÿè®¡: {indexer.get_stats()}")
    
    # æµ‹è¯•æŸ¥è¯¢
    test_queries = [
        "how to kill bile titan",
        "bile titan weakness",
        "hulk eye weak point",
        "btå¤´éƒ¨å¼±ç‚¹"
    ]
    
    for query in test_queries:
        print(f"\næŸ¥è¯¢: {query}")
        results = indexer.search(query, top_k=3)
        for result in results:
            print(f"  - åˆ†æ•°: {result['score']:.3f}")
            print(f"    ä¸»é¢˜: {result['chunk']['topic']}")
            print(f"    ç›¸å…³æ€§: {result['match_info']['relevance_reason']}")


if __name__ == "__main__":
    test_enhanced_bm25() 