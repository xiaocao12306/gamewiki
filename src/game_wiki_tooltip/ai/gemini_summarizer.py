"""
Gemini Flash 2.5 Lite Summarizer for RAG-retrieved knowledge chunks
"""
import os
import json
import logging
from typing import List, Dict, Optional, Any
import google.generativeai as genai
from dataclasses import dataclass

logger = logging.getLogger(__name__)


@dataclass
class SummarizationConfig:
    """Configuration for Gemini summarization"""
    api_key: str
    model_name: str = "gemini-2.5-flash-lite-preview-06-17"
    temperature: float = 0.3
    include_sources: bool = True
    language: str = "auto"  # auto, zh, en


class GeminiSummarizer:
    """Summarizes multiple knowledge chunks using Gemini Flash 2.5 Lite"""
    
    def __init__(self, config: SummarizationConfig):
        """Initialize Gemini summarizer with configuration"""
        self.config = config
        
        # Configure Gemini API
        genai.configure(api_key=config.api_key)
        
        # Initialize model with safety settings (no max_output_tokens limit)
        self.model = genai.GenerativeModel(
            model_name=config.model_name,
            generation_config={
                "temperature": config.temperature,
                # Let the model decide output length based on query requirements
            }
        )
        
        logger.info(f"Initialized GeminiSummarizer with model: {config.model_name}")
    
    def summarize_chunks(
        self, 
        chunks: List[Dict[str, Any]], 
        query: str,
        original_query: Optional[str] = None,
        context: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        Summarize multiple knowledge chunks into a coherent answer
        
        Args:
            chunks: List of retrieved chunks with content and metadata
            query: Original user query
            context: Optional game context
            
        Returns:
            Dictionary with summary and metadata
        """
        print(f"ðŸ“ [SUMMARY-DEBUG] å¼€å§‹é€šç”¨Geminiæ‘˜è¦ç”Ÿæˆ")
        print(f"   - æ£€ç´¢æŸ¥è¯¢: '{query}'")
        if original_query and original_query != query:
            print(f"   - åŽŸå§‹æŸ¥è¯¢: '{original_query}'")
            print(f"   - åŒæŸ¥è¯¢æ¨¡å¼: å¯ç”¨")
        else:
            print(f"   - åŒæŸ¥è¯¢æ¨¡å¼: æœªå¯ç”¨ (åŽŸå§‹æŸ¥è¯¢ä¸Žæ£€ç´¢æŸ¥è¯¢ç›¸åŒæˆ–æœªæä¾›)")
        print(f"   - çŸ¥è¯†å—æ•°é‡: {len(chunks)}")
        print(f"   - ä¸Šä¸‹æ–‡: {context or 'None'}")
        print(f"   - æ¨¡åž‹: {self.config.model_name}")
        
        if not chunks:
            print(f"âš ï¸ [SUMMARY-DEBUG] æ²¡æœ‰çŸ¥è¯†å—å¯ç”¨äºŽæ‘˜è¦")
            return {
                "summary": "No relevant information found.",
                "chunks_used": 0,
                "sources": []
            }
        
        # æ˜¾ç¤ºçŸ¥è¯†å—ä¿¡æ¯
        print(f"ðŸ“‹ [SUMMARY-DEBUG] è¾“å…¥çŸ¥è¯†å—è¯¦æƒ…:")
        for i, chunk in enumerate(chunks, 1):
            print(f"   {i}. ä¸»é¢˜: {chunk.get('topic', 'Unknown')}")
            print(f"      åˆ†æ•°: {chunk.get('score', 0):.4f}")
            print(f"      ç±»åž‹: {chunk.get('type', 'General')}")
            print(f"      å…³é”®è¯: {chunk.get('keywords', [])}")
            print(f"      æ‘˜è¦: {chunk.get('summary', '')[:100]}...")
        
        try:
            # æ£€æµ‹è¯­è¨€
            language = self._detect_language(query) if self.config.language == "auto" else self.config.language
            print(f"ðŸŒ [SUMMARY-DEBUG] æ£€æµ‹åˆ°è¯­è¨€: {language}")
            
            # Build the summarization prompt
            print(f"ðŸ“ [SUMMARY-DEBUG] æž„å»ºé€šç”¨æ‘˜è¦æç¤ºè¯")
            prompt = self._build_summarization_prompt(chunks, query, original_query, context)
            print(f"   - æç¤ºè¯é•¿åº¦: {len(prompt)} å­—ç¬¦")
            print(f"   - æ¸©åº¦è®¾ç½®: {self.config.temperature}")
            print(f"   - æ— è¾“å‡ºé•¿åº¦é™åˆ¶ï¼Œç”±LLMè‡ªè¡Œåˆ¤æ–­")
            
            # Generate summary
            print(f"ðŸ¤– [SUMMARY-DEBUG] è°ƒç”¨Geminiç”Ÿæˆæ‘˜è¦")
            response = self.model.generate_content(prompt)
            
            print(f"âœ… [SUMMARY-DEBUG] Geminiå“åº”æˆåŠŸ")
            print(f"   - å“åº”é•¿åº¦: {len(response.text)} å­—ç¬¦")
            print(f"   - å®Œæ•´å“åº”å†…å®¹:")
            print(f"{response.text}")
            print(f"   - [å“åº”å†…å®¹ç»“æŸ]")
            
            # Parse and format the response
            formatted_response = self._format_summary_response(response.text, chunks)
            
            print(f"ðŸ“Š [SUMMARY-DEBUG] æ‘˜è¦ç”Ÿæˆå®Œæˆ")
            print(f"   - ä½¿ç”¨çš„çŸ¥è¯†å—æ•°: {formatted_response['chunks_used']}")
            print(f"   - æ¥æºæ•°: {len(formatted_response['sources'])}")
            print(f"   - æœ€ç»ˆæ‘˜è¦é•¿åº¦: {len(formatted_response['summary'])} å­—ç¬¦")
            
            return formatted_response
            
        except Exception as e:
            print(f"âŒ [SUMMARY-DEBUG] æ‘˜è¦ç”Ÿæˆå¤±è´¥: {e}")
            logger.error(f"Error in summarization: {str(e)}")
            
            # Fallback to simple concatenation
            print(f"ðŸ”„ [SUMMARY-DEBUG] ä½¿ç”¨é™çº§æ‘˜è¦ç­–ç•¥")
            fallback_result = self._fallback_summary(chunks, query, original_query)
            
            print(f"ðŸ“Š [SUMMARY-DEBUG] é™çº§æ‘˜è¦å®Œæˆ")
            print(f"   - ä½¿ç”¨çš„çŸ¥è¯†å—æ•°: {fallback_result['chunks_used']}")
            print(f"   - é™çº§æ‘˜è¦é•¿åº¦: {len(fallback_result['summary'])} å­—ç¬¦")
            
            return fallback_result
    
    def _build_summarization_prompt(
        self, 
        chunks: List[Dict[str, Any]], 
        query: str,
        original_query: Optional[str] = None,
        context: Optional[str] = None
    ) -> str:
        """Build the universal prompt for Gemini summarization"""
        
        # Detect language from query or use config
        language = self._detect_language(query) if self.config.language == "auto" else self.config.language
        
        # Format chunks as raw JSON for the prompt
        chunks_json = self._format_chunks_as_json(chunks)
        
        # Detect if user is asking for detailed explanations
        is_detailed_query = self._is_detailed_query(query)
        
        # Build language-specific prompt
        if language == "zh":
            detail_instruction = "è¯¦ç»†è§£é‡Šé€‰æ‹©åŽŸå› å’Œç­–ç•¥" if is_detailed_query else "ç®€æ´æ˜Žäº†çš„å›žç­”"
            
            # æž„å»ºæŸ¥è¯¢ä¿¡æ¯éƒ¨åˆ†
            query_section = f"[æ£€ç´¢æŸ¥è¯¢]: {query}  â† ç”¨äºŽåˆ¤æ–­å“ªäº›ææ–™æ®µè½æœ€ç›¸å…³"
            if original_query and original_query != query:
                query_section += f"\n[åŽŸå§‹æŸ¥è¯¢]: {original_query}  â† ç”¨äºŽå†³å®šå›žç­”é£Žæ ¼ã€è¯¦ç»†ç¨‹åº¦å’ŒæŽªè¾žåå¥½"
            else:
                query_section = f"[ç”¨æˆ·æŸ¥è¯¢]: {query}"
            
            prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ¸¸æˆæ”»ç•¥åŠ©æ‰‹ã€‚åŸºäºŽä»¥ä¸‹JSONæ ¼å¼çš„æ¸¸æˆçŸ¥è¯†å—ï¼Œå›žç­”çŽ©å®¶çš„é—®é¢˜ã€‚

{query_section}
{f"æ¸¸æˆä¸Šä¸‹æ–‡ï¼š{context}" if context else ""}

å¯ç”¨çš„æ¸¸æˆçŸ¥è¯†å—ï¼ˆJSONæ ¼å¼ï¼‰ï¼š
{chunks_json}

å›žç­”æŒ‡å—ï¼š
1. **ç†è§£JSONç»“æž„**ï¼šæ¯ä¸ªçŸ¥è¯†å—åŒ…å«topicã€summaryã€keywordsã€typeã€scoreç­‰åŸºç¡€ä¿¡æ¯ï¼Œä»¥åŠstructured_dataè¯¦ç»†æ•°æ®
2. **åŒæŸ¥è¯¢ç†è§£**ï¼š
   - æ£€ç´¢æŸ¥è¯¢å¸®åŠ©ä½ ç†è§£"å¬å›žæ€è·¯"ï¼Œåˆ¤æ–­å“ªäº›æ®µè½æœ€ç›¸å…³
   - åŽŸå§‹æŸ¥è¯¢å¸®åŠ©ä½ å†³å®šå›žç­”çš„æŽªè¾žã€è¯¦ç»†ç¨‹åº¦ã€å†™ä½œé£Žæ ¼ï¼Œé¿å…"è¯­ä¹‰æ¼‚ç§»"
3. **æ ¹æ®é—®é¢˜ç±»åž‹è°ƒæ•´å›žç­”**ï¼š
   - é…è£…æŽ¨èï¼šå®Œæ•´åˆ—å‡ºæ‰€æœ‰è£…å¤‡/éƒ¨ä»¶ä¿¡æ¯
   - æ•Œäººæ”»ç•¥ï¼šæä¾›å¼±ç‚¹ã€è¡€é‡ã€æŽ¨èæ­¦å™¨ç­‰å…³é”®ä¿¡æ¯
   - æ¸¸æˆç­–ç•¥ï¼šç»™å‡ºå…·ä½“çš„æ“ä½œå»ºè®®å’ŒæŠ€å·§
   - ç‰©å“ä¿¡æ¯ï¼šè¯¦ç»†è¯´æ˜Žå±žæ€§ã€èŽ·å–æ–¹å¼ã€ç”¨é€”ç­‰
4. **å›žç­”è¯¦ç»†ç¨‹åº¦**ï¼š{detail_instruction}
5. **åˆ©ç”¨structured_data**ï¼šä¼˜å…ˆä½¿ç”¨ç»“æž„åŒ–æ•°æ®ä¸­çš„å…·ä½“æ•°å€¼ã€åç§°ã€é…ç½®ç­‰ä¿¡æ¯

æ ¼å¼è¦æ±‚ï¼š
â€¢ æŒ‰ç…§åŽŸå§‹æŸ¥è¯¢çš„æŽªè¾žå’Œç»†èŠ‚è¦æ±‚ç»„ç»‡ç­”æ¡ˆ
â€¢ ä½¿ç”¨å‹å¥½çš„æ¸¸æˆæœ¯è¯­å’Œè¡¨æƒ…ç¬¦å·
â€¢ åŸºäºŽJSONä¸­çš„å®žé™…æ•°æ®ï¼Œä¸è¦ç¼–é€ ä¿¡æ¯
â€¢ å¦‚æžœä¿¡æ¯ä¸ç›¸å…³æˆ–ä¸è¶³ï¼Œè¯·æ˜Žç¡®è¯´æ˜Ž

ä½ çš„å›žç­”ï¼š"""
        else:
            detail_instruction = "detailed explanations with reasons and strategies" if is_detailed_query else "concise and clear responses"
            
            # æž„å»ºæŸ¥è¯¢ä¿¡æ¯éƒ¨åˆ†
            query_section = f"[Retrieval Query]: {query}  â† for determining which material segments are most relevant"
            if original_query and original_query != query:
                query_section += f"\n[Original Query]: {original_query}  â† for determining response style, detail level, and wording preferences"
            else:
                query_section = f"[User Query]: {query}"
            
            prompt = f"""You are a professional game guide assistant. Based on the following JSON-formatted game knowledge chunks, answer the player's question.

{query_section}
{f"Game context: {context}" if context else ""}

Available game knowledge chunks (JSON format):
{chunks_json}

Response guidelines:
1. **Understand JSON structure**: Each chunk contains topic, summary, keywords, type, score and structured_data details
2. **Dual query understanding**:
   - Retrieval query helps you understand the "recall approach" to judge which segments are most relevant
   - Original query helps you decide response wording, detail level, and writing style to avoid "semantic drift"
3. **Adapt response based on question type**:
   - Build recommendations: List complete equipment/component information
   - Enemy guides: Provide weak points, health, recommended weapons
   - Game strategies: Give specific operation suggestions and tactics
   - Item information: Detail attributes, acquisition methods, uses
4. **Response detail level**: {detail_instruction}
5. **Utilize structured_data**: Prioritize specific values, names, configurations from structured data

Format requirements:
â€¢ Organize response according to original query's wording and detail requirements
â€¢ Use friendly gaming terminology and appropriate emojis
â€¢ Base on actual data from JSON, don't fabricate information
â€¢ If information is irrelevant or insufficient, clearly state so

Your response:"""
        
        return prompt
    
    def _format_chunks_as_json(self, chunks: List[Dict[str, Any]]) -> str:
        """Format chunks as clean JSON for the prompt"""
        formatted_chunks = []
        
        for i, chunk in enumerate(chunks, 1):
            # Create a clean chunk representation
            clean_chunk = {
                "chunk_id": i,
                "topic": chunk.get("topic", "Unknown Topic"),
                "summary": chunk.get("summary", ""),
                "keywords": chunk.get("keywords", []),
                "type": chunk.get("type", "General"),
                "relevance_score": chunk.get("score", 0),
                "structured_data": chunk.get("structured_data", {}),
                "content": chunk.get("content", "")
            }
            
            formatted_chunks.append(clean_chunk)
        
        try:
            return json.dumps(formatted_chunks, ensure_ascii=False, indent=2)
        except Exception as e:
            # Fallback to string representation if JSON serialization fails
            logger.warning(f"Failed to serialize chunks as JSON: {e}")
            return str(formatted_chunks)
    
    def _is_detailed_query(self, query: str) -> bool:
        """Detect if the query is asking for detailed explanations"""
        detailed_keywords = [
            # Chinese keywords
            "ä¸ºä»€ä¹ˆ", "åŽŸå› ", "è¯¦ç»†", "è§£é‡Š", "è¯´æ˜Ž", "åˆ†æž", "æœºåˆ¶", "æ·±å…¥",
            "æ€Žä¹ˆæ ·", "å¦‚ä½•", "ç­–ç•¥", "æŠ€å·§", "æ”»ç•¥", "æ•™ç¨‹",
            # English keywords  
            "why", "reason", "detailed", "explain", "explanation", "analysis", 
            "mechanism", "how", "strategy", "tactics", "guide", "tutorial",
            "in-depth", "comprehensive"
        ]
        
        query_lower = query.lower()
        return any(keyword in query_lower for keyword in detailed_keywords)
    
    def _format_summary_response(
        self, 
        summary_text: str, 
        chunks: List[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """Format the summary response with metadata"""
        
        # Extract source references if present
        sources = []
        if self.config.include_sources:
            # Extract chunk indices used in summary
            for i, chunk in enumerate(chunks, 1):
                if any(keyword in summary_text for keyword in chunk.get("keywords", [])):
                    sources.append({
                        "index": i,
                        "topic": chunk.get("topic", ""),
                        "score": chunk.get("score", 0)
                    })
        
        return {
            "summary": summary_text.strip(),
            "chunks_used": len(chunks),
            "sources": sources,
            "model": self.config.model_name,
            "language": self._detect_language(summary_text)
        }
    
    def _fallback_summary(
        self, 
        chunks: List[Dict[str, Any]], 
        query: str,
        original_query: Optional[str] = None
    ) -> Dict[str, Any]:
        """Fallback summary when Gemini fails"""
        # Simple concatenation of top chunks
        summary_parts = []
        
        for i, chunk in enumerate(chunks[:2], 1):  # Use top 2 chunks
            topic = chunk.get("topic", "")
            content = chunk.get("summary", chunk.get("content", ""))
            
            # Add basic info
            if topic:
                summary_parts.append(f"ðŸ’¡ {topic}")
            
            summary_parts.append(content)
            
            # Add key structured data if available
            structured_data = chunk.get("structured_data", {})
            if structured_data:
                # Extract some key information generically
                for key, value in list(structured_data.items())[:3]:  # Top 3 items
                    if isinstance(value, (str, int, float)):
                        summary_parts.append(f"ðŸ”§ {key}: {value}")
                    elif isinstance(value, dict) and value:
                        first_item = next(iter(value.items()))
                        summary_parts.append(f"ðŸ”§ {key}: {first_item[0]} = {first_item[1]}")
            
            summary_parts.append("")  # Add spacing between chunks
        
        summary = "\n".join(summary_parts).strip()
        
        return {
            "summary": summary,
            "chunks_used": min(2, len(chunks)),
            "sources": [{"index": i+1, "topic": c.get("topic", "")} for i, c in enumerate(chunks[:2])],
            "model": "fallback",
            "language": self._detect_language(summary)
        }
    
    def _detect_language(self, text: str) -> str:
        """Simple language detection based on character composition"""
        # Check for Chinese characters
        chinese_chars = sum(1 for char in text if '\u4e00' <= char <= '\u9fff')
        
        if chinese_chars > len(text) * 0.3:  # If more than 30% Chinese
            return "zh"
        else:
            return "en"


# Convenience function for creating summarizer
def create_gemini_summarizer(
    api_key: Optional[str] = None,
    model_name: str = "gemini-2.5-flash-lite-preview-06-17",
    **kwargs
) -> GeminiSummarizer:
    """
    Create a Gemini summarizer instance
    
    Args:
        api_key: Gemini API key (defaults to env var GEMINI_API_KEY)
        model_name: Model to use
        **kwargs: Additional config parameters
        
    Returns:
        GeminiSummarizer instance
    """
    if not api_key:
        api_key = os.environ.get("GEMINI_API_KEY")
        if not api_key:
            raise ValueError("Gemini API key not provided and GEMINI_API_KEY env var not set")
    
    config = SummarizationConfig(
        api_key=api_key,
        model_name=model_name,
        **kwargs
    )
    
    return GeminiSummarizer(config)