"""
Gemini Flash 2.5 Lite Summarizer for RAG-retrieved knowledge chunks
"""
import os
import json
import logging
from typing import List, Dict, Optional, Any, AsyncGenerator
import google.generativeai as genai
from dataclasses import dataclass
from pathlib import Path

logger = logging.getLogger(__name__)


@dataclass
class SummarizationConfig:
    """Configuration for Gemini summarization"""
    api_key: str
    model_name: str = "gemini-2.5-flash-lite-preview-06-17"
    temperature: float = 0.3
    include_sources: bool = True
    language: str = "auto"  # auto, zh, en


class GeminiSummarizer:
    """Summarizes multiple knowledge chunks using Gemini Flash 2.5 Lite"""
    
    def __init__(self, config: SummarizationConfig):
        """Initialize Gemini summarizer with configuration"""
        self.config = config
        
        # Configure Gemini API
        genai.configure(api_key=config.api_key)
        
        # Initialize model with safety settings (no max_output_tokens limit)
        self.model = genai.GenerativeModel(
            model_name=config.model_name,
            generation_config={
                "temperature": config.temperature,
                # Let the model decide output length based on query requirements
            }
        )
        
        logger.info(f"Initialized GeminiSummarizer with model: {config.model_name}")
    
    def summarize_chunks(
        self, 
        chunks: List[Dict[str, Any]], 
        query: str,
        original_query: Optional[str] = None,
        context: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        Summarize multiple knowledge chunks into a coherent answer
        
        Args:
            chunks: List of retrieved chunks with content and metadata
            query: Original user query
            context: Optional game context
            
        Returns:
            Dictionary with summary and metadata
        """
        print(f"üìù [SUMMARY-DEBUG] ÂºÄÂßãÈÄöÁî®GeminiÊëòË¶ÅÁîüÊàê")
        print(f"   - Ê£ÄÁ¥¢Êü•ËØ¢: '{query}'")
        if original_query and original_query != query:
            print(f"   - ÂéüÂßãÊü•ËØ¢: '{original_query}'")
            print(f"   - ÂèåÊü•ËØ¢Ê®°Âºè: ÂêØÁî®")
        else:
            print(f"   - ÂèåÊü•ËØ¢Ê®°Âºè: Êú™ÂêØÁî® (ÂéüÂßãÊü•ËØ¢‰∏éÊ£ÄÁ¥¢Êü•ËØ¢Áõ∏ÂêåÊàñÊú™Êèê‰æõ)")
        print(f"   - Áü•ËØÜÂùóÊï∞Èáè: {len(chunks)}")
        print(f"   - ‰∏ä‰∏ãÊñá: {context or 'None'}")
        print(f"   - Ê®°Âûã: {self.config.model_name}")
        
        # Store game context for video source extraction
        if context:
            self.current_game_name = context
            print(f"üéÆ [SUMMARY-DEBUG] Stored game name: {self.current_game_name}")
        else:
            print(f"‚ö†Ô∏è [SUMMARY-DEBUG] No context provided, game name not stored")
        
        if not chunks:
            print(f"‚ö†Ô∏è [SUMMARY-DEBUG] Ê≤°ÊúâÁü•ËØÜÂùóÂèØÁî®‰∫éÊëòË¶Å")
            return {
                "summary": "No relevant information found.",
                "chunks_used": 0,
                "sources": []
            }
        
        # ÊòæÁ§∫Áü•ËØÜÂùó‰ø°ÊÅØ
        print(f"üìã [SUMMARY-DEBUG] ËæìÂÖ•Áü•ËØÜÂùóËØ¶ÊÉÖ:")
        for i, chunk in enumerate(chunks, 1):
            print(f"   {i}. ‰∏ªÈ¢ò: {chunk.get('topic', 'Unknown')}")
            print(f"      ÂàÜÊï∞: {chunk.get('score', 0):.4f}")
            print(f"      Á±ªÂûã: {chunk.get('type', 'General')}")
            print(f"      ÂÖ≥ÈîÆËØç: {chunk.get('keywords', [])}")
            print(f"      ÊëòË¶Å: {chunk.get('summary', '')[:100]}...")
        
        try:
            # Ê£ÄÊµãËØ≠Ë®Ä
            language = self._detect_language(query) if self.config.language == "auto" else self.config.language
            print(f"üåê [SUMMARY-DEBUG] Ê£ÄÊµãÂà∞ËØ≠Ë®Ä: {language}")
            
            # Build the summarization prompt
            print(f"üìù [SUMMARY-DEBUG] ÊûÑÂª∫ÈÄöÁî®ÊëòË¶ÅÊèêÁ§∫ËØç")
            prompt = self._build_summarization_prompt(chunks, query, original_query, context)
            print(f"   - ÊèêÁ§∫ËØçÈïøÂ∫¶: {len(prompt)} Â≠óÁ¨¶")
            print(f"   - Ê∏©Â∫¶ËÆæÁΩÆ: {self.config.temperature}")
            print(f"   - Êó†ËæìÂá∫ÈïøÂ∫¶ÈôêÂà∂ÔºåÁî±LLMËá™Ë°åÂà§Êñ≠")
            
            # Generate summary
            print(f"ü§ñ [SUMMARY-DEBUG] Ë∞ÉÁî®GeminiÁîüÊàêÊëòË¶Å")
            response = self.model.generate_content(prompt)
            
            print(f"‚úÖ [SUMMARY-DEBUG] GeminiÂìçÂ∫îÊàêÂäü")
            print(f"   - ÂìçÂ∫îÈïøÂ∫¶: {len(response.text)} Â≠óÁ¨¶")
            print(f"   - ÂÆåÊï¥ÂìçÂ∫îÂÜÖÂÆπ:")
            print(f"{response.text}")
            print(f"   - [ÂìçÂ∫îÂÜÖÂÆπÁªìÊùü]")
            
            # Parse and format the response
            formatted_response = self._format_summary_response(response.text, chunks)
            
            print(f"üìä [SUMMARY-DEBUG] ÊëòË¶ÅÁîüÊàêÂÆåÊàê")
            print(f"   - ‰ΩøÁî®ÁöÑÁü•ËØÜÂùóÊï∞: {formatted_response['chunks_used']}")
            print(f"   - Êù•Ê∫êÊï∞: {len(formatted_response['sources'])}")
            print(f"   - ÊúÄÁªàÊëòË¶ÅÈïøÂ∫¶: {len(formatted_response['summary'])} Â≠óÁ¨¶")
            
            return formatted_response
            
        except Exception as e:
            print(f"‚ùå [SUMMARY-DEBUG] ÊëòË¶ÅÁîüÊàêÂ§±Ë¥•: {e}")
            logger.error(f"Error in summarization: {str(e)}")
            
            # Fallback to simple concatenation
            print(f"üîÑ [SUMMARY-DEBUG] ‰ΩøÁî®ÈôçÁ∫ßÊëòË¶ÅÁ≠ñÁï•")
            fallback_result = self._fallback_summary(chunks, query, original_query)
            
            print(f"üìä [SUMMARY-DEBUG] ÈôçÁ∫ßÊëòË¶ÅÂÆåÊàê")
            print(f"   - ‰ΩøÁî®ÁöÑÁü•ËØÜÂùóÊï∞: {fallback_result['chunks_used']}")
            print(f"   - ÈôçÁ∫ßÊëòË¶ÅÈïøÂ∫¶: {len(fallback_result['summary'])} Â≠óÁ¨¶")
            
            return fallback_result
    
    async def summarize_chunks_stream(
        self,
        chunks: List[Dict[str, Any]],
        query: str,
        original_query: Optional[str] = None,
        context: Optional[str] = None
    ) -> AsyncGenerator[str, None]:
        """
        ÊµÅÂºèÁîüÊàêÊëòË¶ÅÔºå‰ΩøÁî®ÁúüÊ≠£ÁöÑGeminiÊµÅÂºèAPI
        
        Args:
            chunks: Ê£ÄÁ¥¢Âà∞ÁöÑÁü•ËØÜÂùó
            query: Â§ÑÁêÜÂêéÁöÑÊü•ËØ¢
            original_query: ÂéüÂßãÊü•ËØ¢
            context: Ê∏∏Êàè‰∏ä‰∏ãÊñá
            
        Yields:
            ÊëòË¶ÅÂÜÖÂÆπÁöÑÊµÅÂºèÁâáÊÆµ
        """
        print(f"üåä [STREAM-DEBUG] ÂºÄÂßãÊµÅÂºèÊëòË¶ÅÁîüÊàê")
        print(f"   - Áü•ËØÜÂùóÊï∞Èáè: {len(chunks)}")
        print(f"   - Êü•ËØ¢: {query}")
        if original_query and original_query != query:
            print(f"   - ÂéüÂßãÊü•ËØ¢: {original_query}")
        print(f"   - Ê∏∏Êàè‰∏ä‰∏ãÊñá: {context}")
        
        # Store game context for video source extraction
        if context:
            self.current_game_name = context
            print(f"üéÆ [STREAM-DEBUG] Stored game name: {self.current_game_name}")
        else:
            print(f"‚ö†Ô∏è [STREAM-DEBUG] No context provided, game name not stored")
        
        if not chunks:
            yield "Êä±Ê≠âÔºåÊ≤°ÊúâÊâæÂà∞Áõ∏ÂÖ≥ÁöÑÊ∏∏Êàè‰ø°ÊÅØ„ÄÇ"
            return
            
        try:
            print(f"üöÄ [STREAM-DEBUG] Ë∞ÉÁî®GeminiÊµÅÂºèAPI")
            
            # ÊûÑÂª∫prompt
            prompt = self._build_summarization_prompt(chunks, query, original_query, context)
            
            # Áî®‰∫éÊî∂ÈõÜÂÆåÊï¥ÂìçÂ∫îÊñáÊú¨‰ª•ÊèêÂèñËßÜÈ¢ëÊ∫ê
            complete_response = ""
            
            # ‰ΩøÁî®Êñ∞ÁöÑClient APIËøõË°åÊµÅÂºèË∞ÉÁî®
            import google.generativeai as genai
            from google import genai as new_genai
            
            try:
                # Â∞ùËØï‰ΩøÁî®Êñ∞ÁöÑClient APIÔºàÊé®ËçêÊñπÂºèÔºâ
                client = new_genai.Client(api_key=self.config.api_key)
                
                # ÊµÅÂºèÁîüÊàêÂÜÖÂÆπ
                response = client.models.generate_content_stream(
                    model=self.config.model_name,
                    contents=[prompt]
                )
                
                print(f"‚úÖ [STREAM-DEBUG] ÂºÄÂßãÊé•Êî∂ÊµÅÂºèÂìçÂ∫îÔºàÊñ∞Client APIÔºâ")
                
                # ÂÆûÊó∂‰∫ßÂá∫ÊµÅÂºèÂÜÖÂÆπ
                for chunk in response:
                    if chunk.text:
                        print(f"üìù [STREAM-DEBUG] Êé•Êî∂Âà∞ÊµÅÂºèÁâáÊÆµ: {len(chunk.text)} Â≠óÁ¨¶")
                        complete_response += chunk.text
                        yield chunk.text
                    
                print(f"üéâ [STREAM-DEBUG] ÊµÅÂºèÂìçÂ∫îÂÆåÊàêÔºàÊñ∞Client APIÔºâ")
                
            except (ImportError, AttributeError) as e:
                # Â¶ÇÊûúÊñ∞API‰∏çÂèØÁî®ÔºåÂõûÈÄÄÂà∞Â∞ùËØïÊóßAPI
                print(f"‚ö†Ô∏è [STREAM-DEBUG] Êñ∞Client API‰∏çÂèØÁî®({e})ÔºåÂ∞ùËØïÊóßAPIÊñπÂºè")
                
                # ÈÖçÁΩÆÁîüÊàêÂèÇÊï∞
                generation_config = genai.types.GenerationConfig(
                    temperature=self.config.temperature,
                    max_output_tokens=8192,
                )
                
                # ‰ΩøÁî®ÊóßÁöÑGenerativeModel API
                model = genai.GenerativeModel(
                    model_name=self.config.model_name,
                    generation_config=generation_config,
                )
                
                # Ê£ÄÊü•ÊòØÂê¶ÊúâÊµÅÂºèÊñπÊ≥ï
                if hasattr(model, 'generate_content_stream'):
                    print(f"‚úÖ [STREAM-DEBUG] ‰ΩøÁî®ÊóßAPIÁöÑÊµÅÂºèÊñπÊ≥ï")
                    response = model.generate_content_stream(prompt)
                    
                    for chunk in response:
                        if chunk.text:
                            print(f"üìù [STREAM-DEBUG] Êé•Êî∂Âà∞ÊµÅÂºèÁâáÊÆµ: {len(chunk.text)} Â≠óÁ¨¶")
                            complete_response += chunk.text
                            yield chunk.text
                else:
                    print(f"‚ùå [STREAM-DEBUG] ÊóßAPI‰πü‰∏çÊîØÊåÅÊµÅÂºèÔºåÂõûÈÄÄÂà∞ÂêåÊ≠•ÊñπÊ≥ï")
                    # ÂÆåÂÖ®ÂõûÈÄÄÂà∞ÂêåÊ≠•ÊñπÊ≥ï
                    response = model.generate_content(prompt)
                    if response and response.text:
                        complete_response = response.text
                        yield response.text
            
            # ÊµÅÂºèËæìÂá∫ÂÆåÊàêÂêéÔºåÊ∑ªÂä†ËßÜÈ¢ëÊù•Ê∫ê‰ø°ÊÅØ
            print(f"üé¨ [STREAM-DEBUG] ÊµÅÂºèËæìÂá∫ÂÆåÊàêÔºåÂºÄÂßãÊèêÂèñËßÜÈ¢ëÊù•Ê∫ê")
            video_sources_text = self._extract_video_sources(chunks, complete_response)
            if video_sources_text:
                print(f"‚úÖ [STREAM-DEBUG] ÊâæÂà∞ËßÜÈ¢ëÊù•Ê∫êÔºåÊ∑ªÂä†Âà∞ÊµÅÂºèËæìÂá∫")
                # Ê∑ªÂä†ÂàÜÈöîÁ¨¶Á°Æ‰øùËÉΩË¢´Ê≠£Á°ÆËØÜÂà´
                separator = "\n\n---\n"
                yield separator + video_sources_text
            else:
                print(f"‚ùå [STREAM-DEBUG] Êú™ÊâæÂà∞ËßÜÈ¢ëÊù•Ê∫ê")
                    
        except Exception as e:
            print(f"‚ùå [STREAM-DEBUG] ÊµÅÂºèAPIË∞ÉÁî®Â§±Ë¥•: {e}")
            print(f"üîÑ [STREAM-DEBUG] ÂõûÈÄÄÂà∞ÂêåÊ≠•ÊñπÊ≥ï")
            import traceback
            print(f"‚ùå [STREAM-DEBUG] ËØ¶ÁªÜÈîôËØØ‰ø°ÊÅØ: {traceback.format_exc()}")
            
            # Ê£ÄÊü•ÊòØÂê¶ÊòØAPIÂØÜÈí•ÈóÆÈ¢ò
            error_msg = str(e).lower()
            if 'api_key' in error_msg or 'authentication' in error_msg or 'unauthorized' in error_msg or 'inputs argument' in error_msg:
                print(f"üîë [STREAM-DEBUG] Ê£ÄÊµãÂà∞APIÂØÜÈí•Áõ∏ÂÖ≥ÈîôËØØ")
                yield "‚ùå APIÂØÜÈí•ÈÖçÁΩÆÊúâÈóÆÈ¢òÔºåËØ∑Ê£ÄÊü•Gemini APIÂØÜÈí•ÊòØÂê¶Ê≠£Á°ÆÈÖçÁΩÆ„ÄÇ\n\n"
                return
            
            # ÂõûÈÄÄÂà∞ÂéüÊúâÁöÑÂêåÊ≠•ÊñπÊ≥ï
            try:
                result = self.summarize_chunks(chunks, query, original_query, context)
                yield result.get('summary', str(result))
            except Exception as sync_error:
                print(f"‚ùå [STREAM-DEBUG] ÂêåÊ≠•ÊñπÊ≥ï‰πüÂ§±Ë¥•: {sync_error}")
                yield "Êä±Ê≠âÔºåAIÊëòË¶ÅÊúçÂä°ÊöÇÊó∂‰∏çÂèØÁî®ÔºåËØ∑Á®çÂêéÈáçËØï„ÄÇ"
    
    def _build_summarization_prompt(
        self, 
        chunks: List[Dict[str, Any]], 
        query: str,
        original_query: Optional[str] = None,
        context: Optional[str] = None
    ) -> str:
        """Build the universal prompt for Gemini summarization"""
        
        # Detect language from query or use config
        language = self._detect_language(query) if self.config.language == "auto" else self.config.language
        
        # Format chunks as raw JSON for the prompt
        chunks_json = self._format_chunks_as_json(chunks)
        
        # Detect if user is asking for detailed explanations
        is_detailed_query = self._is_detailed_query(query)
        
        # Build language-specific prompt
        if language == "zh":
            detail_instruction = "ËØ¶ÁªÜËß£ÈáäÈÄâÊã©ÂéüÂõ†ÂíåÁ≠ñÁï•" if is_detailed_query else "ÁÆÄÊ¥ÅÊòé‰∫ÜÁöÑÂõûÁ≠î"
            
            # ÊûÑÂª∫Êü•ËØ¢‰ø°ÊÅØÈÉ®ÂàÜ
            query_section = f"[Ê£ÄÁ¥¢Êü•ËØ¢]: {query}  ‚Üê Áî®‰∫éÂà§Êñ≠Âì™‰∫õÊùêÊñôÊÆµËêΩÊúÄÁõ∏ÂÖ≥"
            if original_query and original_query != query:
                query_section += f"\n[ÂéüÂßãÊü•ËØ¢]: {original_query}  ‚Üê Áî®‰∫éÂÜ≥ÂÆöÂõûÁ≠îÈ£éÊ†º„ÄÅËØ¶ÁªÜÁ®ãÂ∫¶ÂíåÊé™ËæûÂÅèÂ•Ω"
            else:
                query_section = f"[Áî®Êà∑Êü•ËØ¢]: {query}"
            
            prompt = f"""‰Ω†ÊòØ‰∏Ä‰∏™‰∏ì‰∏öÁöÑÊ∏∏ÊàèÊîªÁï•Âä©Êâã„ÄÇÂü∫‰∫é‰ª•‰∏ãJSONÊ†ºÂºèÁöÑÊ∏∏ÊàèÁü•ËØÜÂùóÔºåÂõûÁ≠îÁé©ÂÆ∂ÁöÑÈóÆÈ¢ò„ÄÇ

{query_section}
{f"Ê∏∏Êàè‰∏ä‰∏ãÊñáÔºö{context}" if context else ""}

ÂèØÁî®ÁöÑÊ∏∏ÊàèÁü•ËØÜÂùóÔºàJSONÊ†ºÂºèÔºâÔºö
{chunks_json}

ÂõûÁ≠îÊåáÂçóÔºö
1. **ÂºÄÂ§¥ÂøÖÈ°ªÊèê‰æõ‰∏ÄÂè•ËØùÊÄªÁªì**ÔºöÂú®Ê≠£ÂºèÂõûÁ≠î‰πãÂâçÔºåÁî®‰∏ÄÂè•ËØùÊ¶ÇÊã¨Á≠îÊ°àË¶ÅÁÇπÔºà‰æãÂ¶ÇÔºö"üí° **ÊÄªÁªì**ÔºöÊé®Ëçê‰ΩøÁî®ÁÅ´ÁÆ≠Á≠íÈÖçÂêàÈáçË£ÖÁî≤Êù•ÂØπ‰ªòËøô‰∏™BOSS"Ôºâ
2. **ÁêÜËß£JSONÁªìÊûÑ**ÔºöÊØè‰∏™Áü•ËØÜÂùóÂåÖÂê´topic„ÄÅsummary„ÄÅkeywords„ÄÅtype„ÄÅscoreÁ≠âÂü∫Á°Ä‰ø°ÊÅØÔºå‰ª•Âèästructured_dataËØ¶ÁªÜÊï∞ÊçÆ
3. **ÂèåÊü•ËØ¢ÁêÜËß£**Ôºö
   - Ê£ÄÁ¥¢Êü•ËØ¢Â∏ÆÂä©‰Ω†ÁêÜËß£"Âè¨ÂõûÊÄùË∑Ø"ÔºåÂà§Êñ≠Âì™‰∫õÊÆµËêΩÊúÄÁõ∏ÂÖ≥
   - ÂéüÂßãÊü•ËØ¢Â∏ÆÂä©‰Ω†ÂÜ≥ÂÆöÂõûÁ≠îÁöÑÊé™Ëæû„ÄÅËØ¶ÁªÜÁ®ãÂ∫¶„ÄÅÂÜô‰ΩúÈ£éÊ†ºÔºåÈÅøÂÖç"ËØ≠‰πâÊºÇÁßª"
4. **Ê†πÊçÆÈóÆÈ¢òÁ±ªÂûãË∞ÉÊï¥ÂõûÁ≠î**Ôºö
   - ÈÖçË£ÖÊé®ËçêÔºöÂÆåÊï¥ÂàóÂá∫ÊâÄÊúâË£ÖÂ§á/ÈÉ®‰ª∂‰ø°ÊÅØ
   - Êïå‰∫∫ÊîªÁï•ÔºöÊèê‰æõÂº±ÁÇπ„ÄÅË°ÄÈáè„ÄÅÊé®ËçêÊ≠¶Âô®Á≠âÂÖ≥ÈîÆ‰ø°ÊÅØ
   - Ê∏∏ÊàèÁ≠ñÁï•ÔºöÁªôÂá∫ÂÖ∑‰ΩìÁöÑÊìç‰ΩúÂª∫ËÆÆÂíåÊäÄÂ∑ß
   - Áâ©ÂìÅ‰ø°ÊÅØÔºöËØ¶ÁªÜËØ¥ÊòéÂ±ûÊÄß„ÄÅËé∑ÂèñÊñπÂºè„ÄÅÁî®ÈÄîÁ≠â
5. **ÂõûÁ≠îËØ¶ÁªÜÁ®ãÂ∫¶**Ôºö{detail_instruction}
6. **Âà©Áî®structured_data**Ôºö‰ºòÂÖà‰ΩøÁî®ÁªìÊûÑÂåñÊï∞ÊçÆ‰∏≠ÁöÑÂÖ∑‰ΩìÊï∞ÂÄº„ÄÅÂêçÁß∞„ÄÅÈÖçÁΩÆÁ≠â‰ø°ÊÅØ

Ê†ºÂºèË¶ÅÊ±ÇÔºö
‚Ä¢ ÂºÄÂ§¥ÂÖàÁªôÂá∫‰∏ÄÂè•ËØùÊÄªÁªìÔºàÁî®üí°Ê†áËÆ∞Ôºâ
‚Ä¢ ÊåâÁÖßÂéüÂßãÊü•ËØ¢ÁöÑÊé™ËæûÂíåÁªÜËäÇË¶ÅÊ±ÇÁªÑÁªáÁ≠îÊ°à
‚Ä¢ ‰ΩøÁî®ÂèãÂ•ΩÁöÑÊ∏∏ÊàèÊúØËØ≠ÂíåË°®ÊÉÖÁ¨¶Âè∑
‚Ä¢ Âü∫‰∫éJSON‰∏≠ÁöÑÂÆûÈôÖÊï∞ÊçÆÔºå‰∏çË¶ÅÁºñÈÄ†‰ø°ÊÅØ
‚Ä¢ Â¶ÇÊûú‰ø°ÊÅØ‰∏çÁõ∏ÂÖ≥Êàñ‰∏çË∂≥ÔºåËØ∑ÊòéÁ°ÆËØ¥Êòé

‰Ω†ÁöÑÂõûÁ≠îÔºö"""
        else:
            detail_instruction = "detailed explanations with reasons and strategies" if is_detailed_query else "concise and clear responses"
            
            # ÊûÑÂª∫Êü•ËØ¢‰ø°ÊÅØÈÉ®ÂàÜ
            query_section = f"[Retrieval Query]: {query}  ‚Üê for determining which material segments are most relevant"
            if original_query and original_query != query:
                query_section += f"\n[Original Query]: {original_query}  ‚Üê for determining response style, detail level, and wording preferences"
            else:
                query_section = f"[User Query]: {query}"
            
            prompt = f"""You are a professional game guide assistant. Based on the following JSON-formatted game knowledge chunks, answer the player's question.

{query_section}
{f"Game context: {context}" if context else ""}

Available game knowledge chunks (JSON format):
{chunks_json}

Response guidelines:
1. **Start with a one-sentence summary**: Before the detailed answer, provide a one-sentence summary of the key points (e.g., "üí° **Summary**: Recommended to use rocket launcher with heavy armor against this boss")
2. **Understand JSON structure**: Each chunk contains topic, summary, keywords, type, score and structured_data details
3. **Dual query understanding**:
   - Retrieval query helps you understand the "recall approach" to judge which segments are most relevant
   - Original query helps you decide response wording, detail level, and writing style to avoid "semantic drift"
4. **Adapt response based on question type**:
   - Build recommendations: List complete equipment/component information
   - Enemy guides: Provide weak points, health, recommended weapons
   - Game strategies: Give specific operation suggestions and tactics
   - Item information: Detail attributes, acquisition methods, uses
5. **Response detail level**: {detail_instruction}
6. **Utilize structured_data**: Prioritize specific values, names, configurations from structured data

Format requirements:
‚Ä¢ Start with a one-sentence summary (marked with üí°)
‚Ä¢ Organize response according to original query's wording and detail requirements
‚Ä¢ Use friendly gaming terminology and appropriate emojis
‚Ä¢ Base on actual data from JSON, don't fabricate information
‚Ä¢ If information is irrelevant or insufficient, clearly state so

Your response:"""
        
        return prompt
    
    def _format_chunks_as_json(self, chunks: List[Dict[str, Any]]) -> str:
        """Format chunks as clean JSON for the prompt"""
        formatted_chunks = []
        
        for i, chunk in enumerate(chunks, 1):
            # Create a clean chunk representation
            clean_chunk = {
                "chunk_id": i,
                "topic": chunk.get("topic", "Unknown Topic"),
                "summary": chunk.get("summary", ""),
                "keywords": chunk.get("keywords", []),
                "type": chunk.get("type", "General"),
                "relevance_score": chunk.get("score", 0),
                "structured_data": chunk.get("structured_data", {}),
                "content": chunk.get("content", "")
            }
            
            formatted_chunks.append(clean_chunk)
        
        try:
            return json.dumps(formatted_chunks, ensure_ascii=False, indent=2)
        except Exception as e:
            # Fallback to string representation if JSON serialization fails
            logger.warning(f"Failed to serialize chunks as JSON: {e}")
            return str(formatted_chunks)
    
    def _is_detailed_query(self, query: str) -> bool:
        """Detect if the query is asking for detailed explanations"""
        detailed_keywords = [
            # Chinese keywords
            "‰∏∫‰ªÄ‰πà", "ÂéüÂõ†", "ËØ¶ÁªÜ", "Ëß£Èáä", "ËØ¥Êòé", "ÂàÜÊûê", "Êú∫Âà∂", "Ê∑±ÂÖ•",
            "Á≠ñÁï•", "ÊäÄÂ∑ß", "ÊîªÁï•", "ÊïôÁ®ã",
            # English keywords  
            "why", "reason", "detailed", "explain",  "analysis",
            "mechanism", "strategy", "tactics",
            "in-depth", "comprehensive"
        ]
        
        query_lower = query.lower()
        return any(keyword in query_lower for keyword in detailed_keywords)
    
    def _format_summary_response(
        self, 
        summary_text: str, 
        chunks: List[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """Format the summary response with metadata"""
        
        print(f"üì¶ [FORMAT-DEBUG] Formatting summary response")
        print(f"   - Has current_game_name: {hasattr(self, 'current_game_name')}")
        if hasattr(self, 'current_game_name'):
            print(f"   - current_game_name value: {self.current_game_name}")
        
        # Extract source references if present
        sources = []
        if self.config.include_sources:
            # Extract chunk indices used in summary
            for i, chunk in enumerate(chunks, 1):
                if any(keyword in summary_text for keyword in chunk.get("keywords", [])):
                    sources.append({
                        "index": i,
                        "topic": chunk.get("topic", ""),
                        "score": chunk.get("score", 0)
                    })
        
        # Add video sources to the summary
        print(f"üé¨ [FORMAT-DEBUG] Extracting video sources...")
        video_sources_text = self._extract_video_sources(chunks, summary_text)
        if video_sources_text:
            print(f"‚úÖ [FORMAT-DEBUG] Video sources found, adding to summary")
            summary_text = summary_text.strip() + "\n\n" + video_sources_text
        else:
            print(f"‚ùå [FORMAT-DEBUG] No video sources returned")
        
        return {
            "summary": summary_text.strip(),
            "chunks_used": len(chunks),
            "sources": sources,
            "model": self.config.model_name,
            "language": self._detect_language(summary_text)
        }
    
    def _fallback_summary(
        self, 
        chunks: List[Dict[str, Any]], 
        query: str,
        original_query: Optional[str] = None
    ) -> Dict[str, Any]:
        """Fallback summary when Gemini fails"""
        # Simple concatenation of top chunks
        summary_parts = []
        
        for i, chunk in enumerate(chunks[:2], 1):  # Use top 2 chunks
            topic = chunk.get("topic", "")
            content = chunk.get("summary", chunk.get("content", ""))
            
            # Add basic info
            if topic:
                summary_parts.append(f"üí° {topic}")
            
            summary_parts.append(content)
            
            # Add key structured data if available
            structured_data = chunk.get("structured_data", {})
            if structured_data:
                # Extract some key information generically
                for key, value in list(structured_data.items())[:3]:  # Top 3 items
                    if isinstance(value, (str, int, float)):
                        summary_parts.append(f"üîß {key}: {value}")
                    elif isinstance(value, dict) and value:
                        first_item = next(iter(value.items()))
                        summary_parts.append(f"üîß {key}: {first_item[0]} = {first_item[1]}")
            
            summary_parts.append("")  # Add spacing between chunks
        
        summary = "\n".join(summary_parts).strip()
        
        return {
            "summary": summary,
            "chunks_used": min(2, len(chunks)),
            "sources": [{"index": i+1, "topic": c.get("topic", "")} for i, c in enumerate(chunks[:2])],
            "model": "fallback",
            "language": self._detect_language(summary)
        }
    
    def _extract_video_sources(self, chunks: List[Dict[str, Any]], summary_text: str) -> str:
        """Extract video source information from chunks"""
        try:
            # Get game name from config
            game_name = None
            if hasattr(self, 'current_game_name'):
                game_name = self.current_game_name
            
            print(f"üé• [VIDEO-DEBUG] Starting video source extraction")
            print(f"   - Game name: {game_name}")
            print(f"   - Number of chunks: {len(chunks)}")
            
            if not game_name:
                logger.debug("No game name available for video source extraction")
                print(f"‚ùå [VIDEO-DEBUG] No game name available")
                return ""
            
            # Load original knowledge chunk file
            # __file__ is in src/game_wiki_tooltip/ai/, need to go up to project root
            kb_path = Path(__file__).parent.parent.parent.parent / "data" / "knowledge_chunk" / f"{game_name}.json"
            print(f"üìÅ [VIDEO-DEBUG] Looking for knowledge chunk file: {kb_path}")
            
            if not kb_path.exists():
                logger.debug(f"Knowledge chunk file not found: {kb_path}")
                print(f"‚ùå [VIDEO-DEBUG] Knowledge chunk file not found")
                return ""
            
            with open(kb_path, 'r', encoding='utf-8') as f:
                kb_data = json.load(f)
            
            # Collect video sources
            video_sources = {}  # URL -> {title, timestamps}
            
            # Check which chunks were actually used in the summary
            used_chunks = []
            for chunk in chunks:
                # Check if chunk content appears in summary or has high score
                chunk_keywords = chunk.get("keywords", [])
                chunk_topic = chunk.get("topic", "")
                chunk_score = chunk.get("score", 0)
                
                # Simple heuristic: check if keywords or topic appear in summary
                keyword_match = any(keyword.lower() in summary_text.lower() for keyword in chunk_keywords)
                topic_match = chunk_topic.lower() in summary_text.lower()
                high_score = chunk_score > 0.5
                
                if keyword_match or topic_match or high_score:
                    used_chunks.append(chunk)
                    print(f"‚úÖ [VIDEO-DEBUG] Chunk used: {chunk_topic} (score: {chunk_score:.3f})")
                    print(f"   - Keyword match: {keyword_match}, Topic match: {topic_match}, High score: {high_score}")
            
            print(f"üìä [VIDEO-DEBUG] Used chunks count: {len(used_chunks)}")
            
            # Match chunks with original data using topic matching
            for chunk in used_chunks:
                chunk_topic = chunk.get("topic", "")
                
                print(f"üîç [VIDEO-DEBUG] Matching chunk by topic: {chunk_topic}")
                
                if not chunk_topic:
                    print(f"‚ö†Ô∏è [VIDEO-DEBUG] Chunk missing topic, skipping")
                    continue
                
                # Search in all videos' knowledge chunks
                for video_entry in kb_data:
                    video_info = video_entry.get("video_info", {})
                    if not video_info:
                        continue
                    
                    for kb_chunk in video_entry.get("knowledge_chunks", []):
                        # Match by topic only
                        if kb_chunk.get("topic", "").strip() == chunk_topic.strip():
                            
                            video_url = video_info.get("url", "")
                            if video_url:
                                if video_url not in video_sources:
                                    video_sources[video_url] = {
                                        "title": video_info.get("title", "Unknown Video"),
                                        "timestamps": []
                                    }
                                
                                # Add timestamp
                                timestamp = kb_chunk.get("timestamp", {})
                                start = timestamp.get("start", "")
                                end = timestamp.get("end", "")
                                if start and end:
                                    video_sources[video_url]["timestamps"].append(f"{start}-{end}")
            
            # Format video sources
            print(f"üìπ [VIDEO-DEBUG] Found {len(video_sources)} video sources")
            
            if not video_sources:
                print(f"‚ùå [VIDEO-DEBUG] No video sources found")
                return ""
            
            # Build the sources text
            sources_lines = ["---", "<small>", "üì∫ **‰ø°ÊÅØÊù•Ê∫êÔºö**"]
            
            for url, info in video_sources.items():
                title = info["title"]
                timestamps = info["timestamps"]
                
                # Sort timestamps
                timestamps = sorted(set(timestamps))  # Remove duplicates and sort
                
                # Format timestamps
                if timestamps:
                    timestamp_str = "; ".join(timestamps)
                    sources_lines.append(f"- [{title} ({timestamp_str})]({url})")
                else:
                    sources_lines.append(f"- [{title}]({url})")
            
            sources_lines.append("</small>")
            
            return "\n".join(sources_lines)
            
        except Exception as e:
            logger.error(f"Error extracting video sources: {e}")
            print(f"‚ùå [VIDEO-DEBUG] Error extracting video sources: {e}")
            import traceback
            traceback.print_exc()
            return ""
    
    def _detect_language(self, text: str) -> str:
        """Simple language detection based on character composition"""
        # Check for Chinese characters
        chinese_chars = sum(1 for char in text if '\u4e00' <= char <= '\u9fff')
        
        if chinese_chars > len(text) * 0.3:  # If more than 30% Chinese
            return "zh"
        else:
            return "en"


# Convenience function for creating summarizer
def create_gemini_summarizer(
    api_key: Optional[str] = None,
    model_name: str = "gemini-2.5-flash-lite-preview-06-17",
    **kwargs
) -> GeminiSummarizer:
    """
    Create a Gemini summarizer instance
    
    Args:
        api_key: Gemini API key (defaults to env var GEMINI_API_KEY)
        model_name: Model to use
        **kwargs: Additional config parameters
        
    Returns:
        GeminiSummarizer instance
    """
    if not api_key:
        api_key = os.environ.get("GEMINI_API_KEY")
        if not api_key:
            raise ValueError("Gemini API key not provided and GEMINI_API_KEY env var not set")
    
    config = SummarizationConfig(
        api_key=api_key,
        model_name=model_name,
        **kwargs
    )
    
    return GeminiSummarizer(config)