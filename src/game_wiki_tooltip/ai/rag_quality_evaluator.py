"""
RAGè¾“å‡ºè´¨é‡è¯„ä¼°å™¨
=====================

ç”¨äºè¯„ä¼°RAGç³»ç»Ÿçš„è¾“å‡ºè´¨é‡ï¼Œé€šè¿‡å¯¹æ¯”å®é™…è¾“å‡ºå’ŒæœŸæœ›ç­”æ¡ˆï¼Œ
ä½¿ç”¨LLMè¿›è¡Œè´¨é‡è¯„åˆ†å’Œé—®é¢˜åˆ†æã€‚
"""

import json
import asyncio
import logging
from pathlib import Path
from typing import Dict, List, Any, Optional
from dataclasses import dataclass, asdict
from datetime import datetime

import google.generativeai as genai

try:
    # å°è¯•ç›¸å¯¹å¯¼å…¥ï¼ˆåŒ…å†…è¿è¡Œï¼‰
    from .rag_query import EnhancedRagQuery
    from .unified_query_processor import UnifiedQueryProcessor
    from .hybrid_retriever import HybridSearchRetriever
    from .gemini_summarizer import create_gemini_summarizer
    from ..config import LLMConfig
except ImportError:
    # ç›¸å¯¹å¯¼å…¥å¤±è´¥ï¼Œä½¿ç”¨ç»å¯¹å¯¼å…¥ï¼ˆç›´æ¥è¿è¡Œè„šæœ¬ï¼‰
    import sys
    from pathlib import Path
    
    # æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
    project_root = Path(__file__).parent.parent.parent.parent
    if str(project_root) not in sys.path:
        sys.path.insert(0, str(project_root))
    
    from src.game_wiki_tooltip.ai.rag_query import EnhancedRagQuery
    from src.game_wiki_tooltip.ai.unified_query_processor import UnifiedQueryProcessor
    from src.game_wiki_tooltip.ai.hybrid_retriever import HybridSearchRetriever
    from src.game_wiki_tooltip.ai.gemini_summarizer import create_gemini_summarizer
    from src.game_wiki_tooltip.config import LLMConfig

logger = logging.getLogger(__name__)

@dataclass
class EvaluationResult:
    """å•ä¸ªæµ‹è¯•ç”¨ä¾‹çš„è¯„ä¼°ç»“æœ"""
    query: str
    expected_answer: str
    generated_answer: str
    scores: Dict[str, float]  # å„ç»´åº¦å¾—åˆ†
    overall_score: float
    evaluation_reasoning: str
    issues: List[str]
    suggestions: List[str]
    rag_metadata: Dict[str, Any]  # RAGæŸ¥è¯¢çš„å…ƒæ•°æ®
    processing_time: float

@dataclass
class QualityReport:
    """æ•´ä½“è´¨é‡æŠ¥å‘Š"""
    total_cases: int
    average_score: float
    scores_by_dimension: Dict[str, float]
    common_issues: List[str]
    improvement_suggestions: List[str]
    detailed_results: List[EvaluationResult]
    metadata: Dict[str, Any]
    generated_at: str

class RAGQualityEvaluator:
    """RAGè¾“å‡ºè´¨é‡è¯„ä¼°å™¨"""
    
    EVALUATION_DIMENSIONS = {
        "accuracy": "ç­”æ¡ˆçš„å‡†ç¡®æ€§å’Œäº‹å®æ­£ç¡®æ€§",
        "completeness": "ç­”æ¡ˆæ˜¯å¦æ¶µç›–äº†æ‰€æœ‰è¦ç‚¹",
        "relevance": "ç­”æ¡ˆä¸é—®é¢˜çš„ç›¸å…³æ€§",
        "practicality": "ç­”æ¡ˆå¯¹ç©å®¶çš„å®ç”¨æ€§",
        "clarity": "ç­”æ¡ˆçš„æ¸…æ™°åº¦å’Œå¯è¯»æ€§"
    }
    
    def __init__(self, game: str = "helldiver2", llm_config: Optional[LLMConfig] = None):
        self.game = game
        self.llm_config = llm_config or LLMConfig()
        
        # RAGç»„ä»¶
        self.rag_engine = None
        self.evaluator_llm = None
        
        # æµ‹è¯•æ•°æ®è·¯å¾„
        self.test_data_path = Path(__file__).parent.parent.parent.parent / "data" / "sample_inoutput" / f"{game}.json"
        
        # å‘é‡åº“å…ƒæ•°æ®è·¯å¾„
        self.metadata_path = Path(__file__).parent / "vectorstore" / f"{game}_vectors" / "metadata.json"
        
        # ç»“æœå­˜å‚¨
        self.evaluation_results = []
        
    async def initialize(self):
        """åˆå§‹åŒ–RAGå¼•æ“å’Œè¯„ä¼°LLM"""
        try:
            # åˆå§‹åŒ–RAGå¼•æ“ï¼ˆå¯ç”¨æ··åˆæœç´¢å’Œå¢å¼ºåŠŸèƒ½ï¼‰
            self.rag_engine = EnhancedRagQuery(
                enable_summarization=True,
                enable_hybrid_search=True,   # å¯ç”¨æ··åˆæœç´¢
                enable_intent_reranking=True,  # å¯ç”¨æ„å›¾é‡æ’åº
                llm_config=self.llm_config,
                hybrid_config={
                    "fusion_method": "rrf",
                    "vector_weight": 0.5,
                    "bm25_weight": 0.5,
                    "rrf_k": 60
                }
            )
            await self.rag_engine.initialize(game_name=self.game)
            logger.info(f"RAGå¼•æ“åˆå§‹åŒ–æˆåŠŸ: {self.game}")
            
            # åˆå§‹åŒ–è¯„ä¼°LLM (Gemini-2.5-pro)
            api_key = self.llm_config.get_api_key()
            if not api_key:
                raise ValueError("æœªæ‰¾åˆ°Gemini APIå¯†é’¥")
                
            genai.configure(api_key=api_key)
            
            # ä½¿ç”¨gemini-2.0-flash-expè¿›è¡Œè¯„ä¼°
            self.evaluator_llm = genai.GenerativeModel(
                model_name="gemini-2.0-flash-exp",
                generation_config=genai.types.GenerationConfig(
                    max_output_tokens=2048,
                    temperature=0.1,  # ä½æ¸©åº¦ä»¥ç¡®ä¿è¯„ä¼°ä¸€è‡´æ€§
                )
            )
            logger.info("è¯„ä¼°LLMåˆå§‹åŒ–æˆåŠŸ")
            
        except Exception as e:
            logger.error(f"åˆå§‹åŒ–å¤±è´¥: {e}")
            raise
    
    def load_test_data(self) -> List[Dict[str, str]]:
        """åŠ è½½æµ‹è¯•æ•°æ®"""
        if not self.test_data_path.exists():
            raise FileNotFoundError(f"æµ‹è¯•æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {self.test_data_path}")
            
        with open(self.test_data_path, 'r', encoding='utf-8') as f:
            test_data = json.load(f)
            
        logger.info(f"åŠ è½½äº† {len(test_data)} ä¸ªæµ‹è¯•ç”¨ä¾‹")
        return test_data
    
    async def run_rag_query(self, query: str) -> Dict[str, Any]:
        """è¿è¡ŒRAGæŸ¥è¯¢å¹¶è¿”å›ç»“æœåŠå…ƒæ•°æ®"""
        print(f"ğŸ§ª [EVALUATOR-DEBUG] è¿è¡ŒRAGæŸ¥è¯¢: '{query}'")
        start_time = asyncio.get_event_loop().time()
        
        try:
            # æ‰§è¡ŒRAGæŸ¥è¯¢
            print(f"ğŸ” [EVALUATOR-DEBUG] è°ƒç”¨rag_engine.query")
            result = await self.rag_engine.query(query)
            
            # è®¡ç®—å¤„ç†æ—¶é—´
            processing_time = asyncio.get_event_loop().time() - start_time
            
            # æ·»åŠ å¤„ç†æ—¶é—´åˆ°ç»“æœ
            result['processing_time'] = processing_time
            
            print(f"ğŸ“Š [EVALUATOR-DEBUG] RAGæŸ¥è¯¢ç»“æœ: ç½®ä¿¡åº¦={result.get('confidence', 0):.3f}, ç»“æœæ•°={result.get('results_count', 0)}")
            print(f"â±ï¸ [EVALUATOR-DEBUG] æŸ¥è¯¢è€—æ—¶: {processing_time:.3f}ç§’")
            
            return result
            
        except Exception as e:
            print(f"âŒ [EVALUATOR-DEBUG] RAGæŸ¥è¯¢å¤±è´¥: {e}")
            logger.error(f"RAGæŸ¥è¯¢å¤±è´¥: {e}")
            return {
                "answer": f"æŸ¥è¯¢å¤±è´¥: {str(e)}",
                "sources": [],
                "confidence": 0.0,
                "processing_time": asyncio.get_event_loop().time() - start_time,
                "error": str(e)
            }
    
    async def evaluate_answer(self, query: str, expected: str, generated: str, rag_metadata: Dict[str, Any]) -> EvaluationResult:
        """ä½¿ç”¨LLMè¯„ä¼°å•ä¸ªç­”æ¡ˆçš„è´¨é‡"""
        
        # æ„å»ºè¯„ä¼°æç¤º
        evaluation_prompt = f"""
ä½ æ˜¯ä¸€ä¸ªæ¸¸æˆæ”»ç•¥è´¨é‡è¯„ä¼°ä¸“å®¶ã€‚è¯·è¯„ä¼°ä»¥ä¸‹RAGç³»ç»Ÿç”Ÿæˆçš„ç­”æ¡ˆè´¨é‡ã€‚

ã€ç”¨æˆ·é—®é¢˜ã€‘
{query}

ã€æœŸæœ›ç­”æ¡ˆã€‘
{expected}

ã€ç³»ç»Ÿç”Ÿæˆçš„ç­”æ¡ˆã€‘
{generated}

ã€RAGæ£€ç´¢ä¿¡æ¯ã€‘
- æ£€ç´¢åˆ°çš„æ–‡æ¡£æ•°: {rag_metadata.get('results_count', 0)}
- ç½®ä¿¡åº¦: {rag_metadata.get('confidence', 0):.2f}
- ä½¿ç”¨çš„æ£€ç´¢ç±»å‹: {rag_metadata.get('search_metadata', {}).get('search_type', 'unknown')}
- é‡å†™åçš„æŸ¥è¯¢: {rag_metadata.get('search_metadata', {}).get('query', {}).get('rewritten', query)}

è¯·ä»ä»¥ä¸‹ç»´åº¦è¯„ä¼°ç­”æ¡ˆè´¨é‡ï¼ˆæ¯ä¸ªç»´åº¦0-10åˆ†ï¼‰ï¼š
1. å‡†ç¡®æ€§(accuracy): ç­”æ¡ˆçš„äº‹å®æ­£ç¡®æ€§ï¼Œä¸æœŸæœ›ç­”æ¡ˆçš„ä¸€è‡´æ€§
2. å®Œæ•´æ€§(completeness): æ˜¯å¦æ¶µç›–äº†æœŸæœ›ç­”æ¡ˆä¸­çš„æ‰€æœ‰è¦ç‚¹
3. ç›¸å…³æ€§(relevance): ç­”æ¡ˆæ˜¯å¦ç›´æ¥å›ç­”äº†ç”¨æˆ·çš„é—®é¢˜
4. å®ç”¨æ€§(practicality): ç­”æ¡ˆå¯¹æ¸¸æˆç©å®¶çš„å®é™…å¸®åŠ©ç¨‹åº¦
5. æ¸…æ™°åº¦(clarity): ç­”æ¡ˆçš„è¡¨è¾¾æ˜¯å¦æ¸…æ™°æ˜“æ‡‚

ç‰¹åˆ«æ³¨æ„ï¼š
- å¯¹äºæ¨èç±»é—®é¢˜ï¼ˆå¦‚"æœ€ä½³æˆ˜çº½æ¨è"ï¼‰ï¼Œé‡ç‚¹è¯„ä¼°æ¨èçš„åˆç†æ€§è€Œéä¸æœŸæœ›ç­”æ¡ˆçš„å®Œå…¨ä¸€è‡´
- å¦‚æœç”Ÿæˆç­”æ¡ˆæä¾›äº†ä¸æœŸæœ›ç­”æ¡ˆä¸åŒä½†åŒæ ·æœ‰æ•ˆçš„å»ºè®®ï¼Œåº”ç»™äºˆç§¯æè¯„ä»·
- è€ƒè™‘ç­”æ¡ˆæ˜¯å¦æä¾›äº†é¢å¤–çš„æœ‰ç”¨ä¿¡æ¯

è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¾“å‡ºè¯„ä¼°ç»“æœï¼š
{{
    "scores": {{
        "accuracy": <åˆ†æ•°>,
        "completeness": <åˆ†æ•°>,
        "relevance": <åˆ†æ•°>,
        "practicality": <åˆ†æ•°>,
        "clarity": <åˆ†æ•°>
    }},
    "overall_score": <æ€»ä½“åˆ†æ•°ï¼Œ0-10>,
    "evaluation_reasoning": "<è¯¦ç»†è¯´æ˜è¯„åˆ†ç†ç”±>",
    "issues": ["<é—®é¢˜1>", "<é—®é¢˜2>", ...],
    "suggestions": ["<æ”¹è¿›å»ºè®®1>", "<æ”¹è¿›å»ºè®®2>", ...]
}}
"""
        
        try:
            # è°ƒç”¨LLMè¿›è¡Œè¯„ä¼°
            response = self.evaluator_llm.generate_content(evaluation_prompt)
            
            # è§£æJSONå“åº”
            # å°è¯•æå–JSONéƒ¨åˆ†
            response_text = response.text.strip()
            if "```json" in response_text:
                json_start = response_text.find("```json") + 7
                json_end = response_text.find("```", json_start)
                response_text = response_text[json_start:json_end].strip()
            elif "```" in response_text:
                json_start = response_text.find("```") + 3
                json_end = response_text.find("```", json_start)
                response_text = response_text[json_start:json_end].strip()
                
            evaluation_data = json.loads(response_text)
            
            # åˆ›å»ºè¯„ä¼°ç»“æœ
            return EvaluationResult(
                query=query,
                expected_answer=expected,
                generated_answer=generated,
                scores=evaluation_data["scores"],
                overall_score=evaluation_data["overall_score"],
                evaluation_reasoning=evaluation_data["evaluation_reasoning"],
                issues=evaluation_data.get("issues", []),
                suggestions=evaluation_data.get("suggestions", []),
                rag_metadata=rag_metadata,
                processing_time=rag_metadata.get('processing_time', 0)
            )
            
        except Exception as e:
            logger.error(f"LLMè¯„ä¼°å¤±è´¥: {e}")
            # è¿”å›é»˜è®¤è¯„ä¼°ç»“æœ
            return EvaluationResult(
                query=query,
                expected_answer=expected,
                generated_answer=generated,
                scores={dim: 0.0 for dim in self.EVALUATION_DIMENSIONS},
                overall_score=0.0,
                evaluation_reasoning=f"è¯„ä¼°å¤±è´¥: {str(e)}",
                issues=["è¯„ä¼°è¿‡ç¨‹å‡ºé”™"],
                suggestions=["æ£€æŸ¥LLMé…ç½®å’Œå“åº”æ ¼å¼"],
                rag_metadata=rag_metadata,
                processing_time=rag_metadata.get('processing_time', 0)
            )
    
    async def analyze_common_issues(self, results: List[EvaluationResult]) -> tuple[List[str], List[str]]:
        """åˆ†æå¸¸è§é—®é¢˜å¹¶ç”Ÿæˆæ”¹è¿›å»ºè®®"""
        
        # æ”¶é›†æ‰€æœ‰é—®é¢˜
        all_issues = []
        for result in results:
            all_issues.extend(result.issues)
            
        # ç»Ÿè®¡é—®é¢˜é¢‘ç‡
        issue_counts = {}
        for issue in all_issues:
            issue_counts[issue] = issue_counts.get(issue, 0) + 1
            
        # æ‰¾å‡ºæœ€å¸¸è§çš„é—®é¢˜
        common_issues = sorted(issue_counts.items(), key=lambda x: x[1], reverse=True)[:5]
        
        # åŠ è½½å‘é‡åº“å…ƒæ•°æ®ä»¥äº†è§£çŸ¥è¯†åº“æƒ…å†µ
        knowledge_base_info = self._analyze_knowledge_base()
        
        # åŸºäºé—®é¢˜åˆ†æç”Ÿæˆæ”¹è¿›å»ºè®®
        improvement_prompt = f"""
åŸºäºä»¥ä¸‹RAGç³»ç»Ÿçš„è¯„ä¼°ç»“æœï¼Œåˆ†æé—®é¢˜åŸå› å¹¶æä¾›æ”¹è¿›å»ºè®®ã€‚

ã€å¸¸è§é—®é¢˜ã€‘
{json.dumps([issue[0] for issue in common_issues], ensure_ascii=False, indent=2)}

ã€çŸ¥è¯†åº“ä¿¡æ¯ã€‘
{json.dumps(knowledge_base_info, ensure_ascii=False, indent=2)}

ã€å„ç»´åº¦å¹³å‡å¾—åˆ†ã€‘
{json.dumps(self._calculate_dimension_averages(results), ensure_ascii=False, indent=2)}

è¯·åˆ†æè¿™äº›é—®é¢˜çš„æ ¹æœ¬åŸå› ï¼Œå¯èƒ½åŒ…æ‹¬ï¼š
1. çŸ¥è¯†åº“å†…å®¹ä¸è¶³æˆ–è´¨é‡é—®é¢˜
2. æ£€ç´¢ç®—æ³•æ•ˆæœä¸ä½³
3. æŸ¥è¯¢ç†è§£å’Œé‡å†™çš„é—®é¢˜
4. ç­”æ¡ˆç”Ÿæˆå’Œæ‘˜è¦çš„é—®é¢˜

è¯·æŒ‰ä»¥ä¸‹JSONæ ¼å¼è¾“å‡ºåˆ†æç»“æœï¼š
{{
    "root_causes": ["<æ ¹æœ¬åŸå› 1>", "<æ ¹æœ¬åŸå› 2>", ...],
    "improvement_suggestions": [
        {{
            "area": "<æ”¹è¿›é¢†åŸŸ>",
            "suggestion": "<å…·ä½“å»ºè®®>",
            "priority": "<high/medium/low>"
        }},
        ...
    ]
}}
"""
        
        try:
            response = self.evaluator_llm.generate_content(improvement_prompt)
            
            # è§£æå“åº”
            response_text = response.text.strip()
            if "```json" in response_text:
                json_start = response_text.find("```json") + 7
                json_end = response_text.find("```", json_start)
                response_text = response_text[json_start:json_end].strip()
            elif "```" in response_text:
                json_start = response_text.find("```") + 3
                json_end = response_text.find("```", json_start)
                response_text = response_text[json_start:json_end].strip()
                
            analysis = json.loads(response_text)
            
            # æå–æ ¹æœ¬åŸå› å’Œå»ºè®®
            root_causes = analysis.get("root_causes", [])
            suggestions = [s["suggestion"] for s in analysis.get("improvement_suggestions", [])]
            
            # æ·»åŠ åŸºäºæ•°æ®çš„å…·ä½“å»ºè®®
            if knowledge_base_info["total_chunks"] < 100:
                suggestions.append("çŸ¥è¯†åº“å†…å®¹è¾ƒå°‘ï¼Œå»ºè®®å¢åŠ æ›´å¤šæ¸¸æˆæ”»ç•¥å†…å®¹")
            
            avg_scores = self._calculate_dimension_averages(results)
            if avg_scores.get("completeness", 10) < 6:
                suggestions.append("ç­”æ¡ˆå®Œæ•´æ€§ä¸è¶³ï¼Œå»ºè®®æ”¹è¿›æ£€ç´¢ç­–ç•¥æˆ–å¢åŠ ç›¸å…³å†…å®¹")
                
            return root_causes + [issue[0] for issue in common_issues[:3]], suggestions
            
        except Exception as e:
            logger.error(f"é—®é¢˜åˆ†æå¤±è´¥: {e}")
            return (
                [issue[0] for issue in common_issues[:3]], 
                ["ä¼˜åŒ–çŸ¥è¯†åº“å†…å®¹", "æ”¹è¿›æ£€ç´¢ç®—æ³•", "å¢å¼ºæŸ¥è¯¢ç†è§£èƒ½åŠ›"]
            )
    
    def _analyze_knowledge_base(self) -> Dict[str, Any]:
        """åˆ†æçŸ¥è¯†åº“çš„åŸºæœ¬ä¿¡æ¯"""
        try:
            if self.metadata_path.exists():
                with open(self.metadata_path, 'r', encoding='utf-8') as f:
                    metadata = json.load(f)
                    
                # ç»Ÿè®¡ä¿¡æ¯
                total_chunks = len(metadata)
                topics = set()
                avg_summary_length = 0
                
                for chunk in metadata:
                    topics.add(chunk.get("topic", ""))
                    avg_summary_length += len(chunk.get("summary", ""))
                    
                avg_summary_length = avg_summary_length / total_chunks if total_chunks > 0 else 0
                
                return {
                    "total_chunks": total_chunks,
                    "unique_topics": len(topics),
                    "average_summary_length": avg_summary_length,
                    "has_keywords": any(chunk.get("keywords") for chunk in metadata)
                }
            else:
                return {
                    "total_chunks": 0,
                    "unique_topics": 0,
                    "average_summary_length": 0,
                    "has_keywords": False
                }
        except Exception as e:
            logger.error(f"çŸ¥è¯†åº“åˆ†æå¤±è´¥: {e}")
            return {
                "total_chunks": 0,
                "unique_topics": 0, 
                "average_summary_length": 0,
                "has_keywords": False
            }
    
    def _calculate_dimension_averages(self, results: List[EvaluationResult]) -> Dict[str, float]:
        """è®¡ç®—å„ç»´åº¦çš„å¹³å‡å¾—åˆ†"""
        dimension_scores = {dim: [] for dim in self.EVALUATION_DIMENSIONS}
        
        for result in results:
            for dim, score in result.scores.items():
                if dim in dimension_scores:
                    dimension_scores[dim].append(score)
                    
        return {
            dim: sum(scores) / len(scores) if scores else 0.0
            for dim, scores in dimension_scores.items()
        }
    
    async def evaluate_all(self) -> QualityReport:
        """æ‰§è¡Œå®Œæ•´çš„è´¨é‡è¯„ä¼°"""
        
        # ç¡®ä¿å·²åˆå§‹åŒ–
        if not self.rag_engine or not self.evaluator_llm:
            await self.initialize()
            
        # åŠ è½½æµ‹è¯•æ•°æ®
        test_cases = self.load_test_data()
        
        # è¯„ä¼°æ¯ä¸ªæµ‹è¯•ç”¨ä¾‹
        logger.info("å¼€å§‹è¯„ä¼°æµ‹è¯•ç”¨ä¾‹...")
        for i, test_case in enumerate(test_cases):
            logger.info(f"è¯„ä¼°è¿›åº¦: {i+1}/{len(test_cases)}")
            
            # è¿è¡ŒRAGæŸ¥è¯¢
            rag_result = await self.run_rag_query(test_case["query"])
            
            # è¯„ä¼°ç­”æ¡ˆè´¨é‡
            evaluation = await self.evaluate_answer(
                query=test_case["query"],
                expected=test_case["answer"],
                generated=rag_result["answer"],
                rag_metadata=rag_result
            )
            
            self.evaluation_results.append(evaluation)
            
        # è®¡ç®—æ€»ä½“ç»Ÿè®¡
        total_score = sum(r.overall_score for r in self.evaluation_results)
        average_score = total_score / len(self.evaluation_results) if self.evaluation_results else 0.0
        
        # è®¡ç®—å„ç»´åº¦å¹³å‡åˆ†
        dimension_averages = self._calculate_dimension_averages(self.evaluation_results)
        
        # åˆ†æå¸¸è§é—®é¢˜å’Œç”Ÿæˆå»ºè®®
        common_issues, suggestions = await self.analyze_common_issues(self.evaluation_results)
        
        # ç”Ÿæˆè´¨é‡æŠ¥å‘Š
        report = QualityReport(
            total_cases=len(test_cases),
            average_score=average_score,
            scores_by_dimension=dimension_averages,
            common_issues=common_issues,
            improvement_suggestions=suggestions,
            detailed_results=self.evaluation_results,
            metadata={
                "game": self.game,
                "rag_config": {
                    "hybrid_search": self.rag_engine.enable_hybrid_search,
                    "intent_reranking": self.rag_engine.enable_intent_reranking,
                    "summarization": self.rag_engine.enable_summarization
                },
                "knowledge_base_info": self._analyze_knowledge_base()
            },
            generated_at=datetime.now().isoformat()
        )
        
        return report
    
    def save_report(self, report: QualityReport, output_path: Optional[Path] = None):
        """ä¿å­˜è¯„ä¼°æŠ¥å‘Š"""
        if output_path is None:
            output_path = Path(__file__).parent / f"quality_report_{self.game}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            
        # è½¬æ¢ä¸ºå¯åºåˆ—åŒ–çš„å­—å…¸
        report_dict = asdict(report)
        
        # ä¿å­˜JSONæŠ¥å‘Š
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(report_dict, f, ensure_ascii=False, indent=2)
            
        logger.info(f"è¯„ä¼°æŠ¥å‘Šå·²ä¿å­˜è‡³: {output_path}")
        
        # åŒæ—¶ç”Ÿæˆç®€åŒ–çš„MarkdownæŠ¥å‘Š
        md_path = output_path.with_suffix('.md')
        self._generate_markdown_report(report, md_path)
        
    def _generate_markdown_report(self, report: QualityReport, output_path: Path):
        """ç”ŸæˆMarkdownæ ¼å¼çš„æŠ¥å‘Šæ‘˜è¦"""
        md_content = f"""# RAGè´¨é‡è¯„ä¼°æŠ¥å‘Š - {self.game}

ç”Ÿæˆæ—¶é—´: {report.generated_at}

## æ€»ä½“è¯„åˆ†

- **å¹³å‡å¾—åˆ†**: {report.average_score:.2f}/10
- **æµ‹è¯•ç”¨ä¾‹æ•°**: {report.total_cases}

## å„ç»´åº¦å¾—åˆ†

| ç»´åº¦ | å¹³å‡åˆ† | è¯´æ˜ |
|------|--------|------|
"""
        
        for dim, score in report.scores_by_dimension.items():
            md_content += f"| {dim} | {score:.2f} | {self.EVALUATION_DIMENSIONS.get(dim, '')} |\n"
            
        md_content += f"""
## ä¸»è¦é—®é¢˜

"""
        for i, issue in enumerate(report.common_issues[:5], 1):
            md_content += f"{i}. {issue}\n"
            
        md_content += f"""
## æ”¹è¿›å»ºè®®

"""
        for i, suggestion in enumerate(report.improvement_suggestions[:5], 1):
            md_content += f"{i}. {suggestion}\n"
            
        md_content += f"""
## è¯¦ç»†è¯„ä¼°ç»“æœ

"""
        for i, result in enumerate(report.detailed_results, 1):
            md_content += f"""
### æµ‹è¯•ç”¨ä¾‹ {i}

**é—®é¢˜**: {result.query}

**æ€»ä½“å¾—åˆ†**: {result.overall_score:.1f}/10

**è¯„ä¼°ç†ç”±**: {result.evaluation_reasoning}

**æ£€ç´¢ä¿¡æ¯**:
- ç½®ä¿¡åº¦: {result.rag_metadata.get('confidence', 0):.2f}
- å¤„ç†æ—¶é—´: {result.processing_time:.2f}ç§’
- æ£€ç´¢åˆ°çš„æ–‡æ¡£æ•°: {result.rag_metadata.get('results_count', 0)}

---
"""
        
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(md_content)
            
        logger.info(f"MarkdownæŠ¥å‘Šå·²ä¿å­˜è‡³: {output_path}")


async def main():
    """ä¸»å‡½æ•° - è¿è¡Œè´¨é‡è¯„ä¼°"""
    import sys
    
    # é…ç½®æ—¥å¿—
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    # è·å–æ¸¸æˆåç§°ï¼ˆé»˜è®¤ä¸ºhelldiver2ï¼‰
    game = sys.argv[1] if len(sys.argv) > 1 else "helldiver2"
    
    # åˆ›å»ºè¯„ä¼°å™¨
    evaluator = RAGQualityEvaluator(game=game)
    
    try:
        # è¿è¡Œè¯„ä¼°
        logger.info(f"å¼€å§‹è¯„ä¼° {game} çš„RAGè¾“å‡ºè´¨é‡...")
        report = await evaluator.evaluate_all()
        
        # ä¿å­˜æŠ¥å‘Š
        evaluator.save_report(report)
        
        # æ‰“å°æ‘˜è¦
        print(f"\n{'='*50}")
        print(f"RAGè´¨é‡è¯„ä¼°å®Œæˆ - {game}")
        print(f"{'='*50}")
        print(f"å¹³å‡å¾—åˆ†: {report.average_score:.2f}/10")
        print(f"æµ‹è¯•ç”¨ä¾‹æ•°: {report.total_cases}")
        print(f"\nå„ç»´åº¦å¾—åˆ†:")
        for dim, score in report.scores_by_dimension.items():
            print(f"  - {dim}: {score:.2f}")
        print(f"\nä¸»è¦é—®é¢˜:")
        for issue in report.common_issues[:3]:
            print(f"  - {issue}")
        print(f"\næ”¹è¿›å»ºè®®:")
        for suggestion in report.improvement_suggestions[:3]:
            print(f"  - {suggestion}")
            
    except Exception as e:
        logger.error(f"è¯„ä¼°è¿‡ç¨‹å‡ºé”™: {e}")
        raise


if __name__ == "__main__":
    asyncio.run(main())