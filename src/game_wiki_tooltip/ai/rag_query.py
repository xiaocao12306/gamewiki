"""
å¢å¼ºçš„RAGæŸ¥è¯¢æ¥å£ - é›†æˆæ‰¹é‡åµŒå…¥å’Œå‘é‡åº“æ£€ç´¢
============================================

åŠŸèƒ½ï¼š
1. åŠ è½½é¢„æ„å»ºçš„å‘é‡åº“
2. æ‰§è¡Œè¯­ä¹‰æ£€ç´¢
3. æ”¯æŒLLMæŸ¥è¯¢é‡å†™
4. æ··åˆæœç´¢ï¼ˆå‘é‡+BM25ï¼‰
5. è¿”å›ç›¸å…³æ¸¸æˆæ”»ç•¥ä¿¡æ¯
"""

import logging
import asyncio
import json
import numpy as np
from typing import Optional, Dict, Any, List, AsyncGenerator
from pathlib import Path
import time
import sys
import os

class VectorStoreUnavailableError(Exception):
    """å‘é‡åº“ä¸å¯ç”¨é”™è¯¯"""
    pass

def get_resource_path(relative_path: str) -> Path:
    """
    è·å–èµ„æºæ–‡ä»¶çš„ç»å¯¹è·¯å¾„ï¼Œå…¼å®¹å¼€å‘ç¯å¢ƒå’ŒPyInstalleræ‰“åŒ…ç¯å¢ƒ
    
    Args:
        relative_path: ç›¸å¯¹äºé¡¹ç›®æ ¹ç›®å½•æˆ–ä¸´æ—¶ç›®å½•çš„è·¯å¾„
        
    Returns:
        èµ„æºæ–‡ä»¶çš„ç»å¯¹è·¯å¾„
    """
    try:
        # PyInstalleræ‰“åŒ…åçš„ä¸´æ—¶ç›®å½•
        base_path = Path(sys._MEIPASS)
        resource_path = base_path / relative_path
        print(f"ğŸ”§ [RAG-DEBUG] ä½¿ç”¨PyInstallerä¸´æ—¶ç›®å½•: {base_path}")
        print(f"ğŸ”§ [RAG-DEBUG] æ„å»ºèµ„æºè·¯å¾„: {resource_path}")
    except AttributeError:
        # å¼€å‘ç¯å¢ƒï¼šä»å½“å‰æ–‡ä»¶ä½ç½®å‘ä¸Šæ‰¾åˆ°é¡¹ç›®æ ¹ç›®å½•
        current_file = Path(__file__).parent  # .../ai/
        base_path = current_file  # å¯¹äºaiç›®å½•ä¸‹çš„æ–‡ä»¶ï¼Œç›´æ¥ä½¿ç”¨å½“å‰ç›®å½•
        # å¦‚æœrelative_pathä»¥"ai/"å¼€å¤´ï¼Œéœ€è¦å»æ‰è¿™ä¸ªå‰ç¼€
        if relative_path.startswith("ai/"):
            relative_path = relative_path[3:]  # å»æ‰"ai/"å‰ç¼€
        resource_path = base_path / relative_path
        print(f"ğŸ”§ [RAG-DEBUG] ä½¿ç”¨å¼€å‘ç¯å¢ƒè·¯å¾„: {base_path}")
        print(f"ğŸ”§ [RAG-DEBUG] è°ƒæ•´åçš„ç›¸å¯¹è·¯å¾„: {relative_path}")
        print(f"ğŸ”§ [RAG-DEBUG] æ„å»ºèµ„æºè·¯å¾„: {resource_path}")
    
    return resource_path

# å¯¼å…¥æ‰¹é‡åµŒå…¥å¤„ç†å™¨
try:
    from .batch_embedding import BatchEmbeddingProcessor
    BATCH_EMBEDDING_AVAILABLE = True
except ImportError:
    BATCH_EMBEDDING_AVAILABLE = False
    logging.warning("æ‰¹é‡åµŒå…¥æ¨¡å—ä¸å¯ç”¨")

# å‘é‡åº“æ”¯æŒ - å»¶è¿Ÿå¯¼å…¥ä»¥é¿å…å¯åŠ¨æ—¶å´©æºƒ
FAISS_AVAILABLE = None

def _check_faiss_available():
    """æ£€æŸ¥å¹¶å»¶è¿Ÿå¯¼å…¥faiss"""
    global FAISS_AVAILABLE
    if FAISS_AVAILABLE is None:
        try:
            import faiss
            FAISS_AVAILABLE = True
        except ImportError:
            FAISS_AVAILABLE = False
            logging.warning("FAISSä¸å¯ç”¨")
    return FAISS_AVAILABLE

try:
    import qdrant_client
    QDRANT_AVAILABLE = True
except ImportError:
    QDRANT_AVAILABLE = False
    logging.warning("Qdrantä¸å¯ç”¨")

# å¯¼å…¥Geminiæ‘˜è¦å™¨
try:
    from .gemini_summarizer import create_gemini_summarizer, SummarizationConfig
    GEMINI_AVAILABLE = True
except ImportError:
    GEMINI_AVAILABLE = False
    logging.warning("Geminiæ‘˜è¦æ¨¡å—ä¸å¯ç”¨")

# å¯¼å…¥æ„å›¾æ„ŸçŸ¥é‡æ’åºå™¨
try:
    from .intent_aware_reranker import IntentAwareReranker
    RERANKER_AVAILABLE = True
except ImportError:
    RERANKER_AVAILABLE = False
    logging.warning("æ„å›¾é‡æ’åºæ¨¡å—ä¸å¯ç”¨")

# å¯¼å…¥æ··åˆæ£€ç´¢å™¨å’ŒBM25é”™è¯¯ç±»
try:
    from .hybrid_retriever import HybridSearchRetriever, VectorRetrieverAdapter
    from .enhanced_bm25_indexer import BM25UnavailableError
    HYBRID_RETRIEVER_AVAILABLE = True
except ImportError as e:
    HybridSearchRetriever = None
    VectorRetrieverAdapter = None
    BM25UnavailableError = Exception  # å›é€€åˆ°åŸºç¡€å¼‚å¸¸ç±»
    HYBRID_RETRIEVER_AVAILABLE = False
    logging.warning(f"æ··åˆæ£€ç´¢å™¨æ¨¡å—ä¸å¯ç”¨: {e}")

# å¯¼å…¥é…ç½®å’ŒæŸ¥è¯¢é‡å†™
from ..config import LLMConfig

logger = logging.getLogger(__name__)

# å…¨å±€ç¼“å­˜å‘é‡åº“æ˜ å°„é…ç½®
_vector_mappings_cache = None
_vector_mappings_last_modified = None

def load_vector_mappings() -> Dict[str, str]:
    """
    åŠ è½½å‘é‡åº“æ˜ å°„é…ç½®
    
    Returns:
        çª—å£æ ‡é¢˜åˆ°å‘é‡åº“åç§°çš„æ˜ å°„å­—å…¸
    """
    global _vector_mappings_cache, _vector_mappings_last_modified
    
    try:
        # è·å–é…ç½®æ–‡ä»¶è·¯å¾„ - ä»aiç›®å½•å‘ä¸Šæ‰¾åˆ°assetsç›®å½•
        current_dir = Path(__file__).parent  # .../ai/
        assets_dir = current_dir.parent / "assets"  # .../game_wiki_tooltip/assets/
        mapping_file = assets_dir / "vector_mappings.json"
        
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not mapping_file.exists():
            logger.warning(f"å‘é‡åº“æ˜ å°„é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {mapping_file}")
            return
        
        # æ£€æŸ¥æ–‡ä»¶ä¿®æ”¹æ—¶é—´ï¼Œå®ç°ç¼“å­˜æœºåˆ¶
        current_modified = mapping_file.stat().st_mtime
        if (_vector_mappings_cache is not None and 
            _vector_mappings_last_modified == current_modified):
            return _vector_mappings_cache
        
        # è¯»å–é…ç½®æ–‡ä»¶
        with open(mapping_file, 'r', encoding='utf-8') as f:
            config = json.load(f)
        
        # æ„å»ºæ˜ å°„å­—å…¸
        mappings = {}
        for mapping in config.get("mappings", []):
            vector_db_name = mapping.get("vector_db_name")
            window_titles = mapping.get("window_titles", [])
            
            for title in window_titles:
                mappings[title.lower()] = vector_db_name
        
        # æ›´æ–°ç¼“å­˜
        _vector_mappings_cache = mappings
        _vector_mappings_last_modified = current_modified
        
        logger.info(f"æˆåŠŸåŠ è½½å‘é‡åº“æ˜ å°„é…ç½®ï¼ŒåŒ…å« {len(mappings)} ä¸ªæ˜ å°„")
        return mappings
    except Exception as e:
        return

def map_window_title_to_game_name(window_title: str) -> Optional[str]:
    """
    å°†çª—å£æ ‡é¢˜æ˜ å°„åˆ°å‘é‡åº“æ–‡ä»¶å
    
    Args:
        window_title: çª—å£æ ‡é¢˜
        
    Returns:
        å¯¹åº”çš„å‘é‡åº“æ–‡ä»¶åï¼ˆä¸åŒ…å«.jsonæ‰©å±•åï¼‰ï¼Œå¦‚æœæœªæ‰¾åˆ°åˆ™è¿”å›None
    """
    # è½¬æ¢ä¸ºå°å†™è¿›è¡ŒåŒ¹é…
    title_lower = window_title.lower()
    
    # åŠ è½½å‘é‡åº“æ˜ å°„é…ç½®
    title_to_vectordb_mapping = load_vector_mappings()
    
    # å°è¯•ç²¾ç¡®åŒ¹é…
    for title_key, vectordb_name in title_to_vectordb_mapping.items():
        if title_key in title_lower:
            logger.info(f"çª—å£æ ‡é¢˜ '{window_title}' æ˜ å°„åˆ°å‘é‡åº“ '{vectordb_name}'")
            return vectordb_name
    
    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ˜ å°„ï¼Œè®°å½•è­¦å‘Šå¹¶è¿”å›None
    logger.warning(f"æœªæ‰¾åˆ°çª—å£æ ‡é¢˜ '{window_title}' å¯¹åº”çš„å‘é‡åº“æ˜ å°„")
    return None

class EnhancedRagQuery:
    """å¢å¼ºçš„RAGæŸ¥è¯¢æ¥å£ï¼Œæ”¯æŒå‘é‡åº“æ£€ç´¢å’ŒLLMæŸ¥è¯¢é‡å†™"""
    
    def __init__(self, vector_store_path: Optional[str] = None,
                 enable_hybrid_search: bool = True,
                 hybrid_config: Optional[Dict] = None,
                 llm_config: Optional[LLMConfig] = None,
                 jina_api_key: Optional[str] = None,
                 enable_query_rewrite: bool = True,
                 enable_summarization: bool = True,
                 summarization_config: Optional[Dict] = None,
                 enable_intent_reranking: bool = True,
                 reranking_config: Optional[Dict] = None):
        """
        åˆå§‹åŒ–RAGæŸ¥è¯¢å™¨
        
        Args:
            vector_store_path: å‘é‡åº“è·¯å¾„ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤è·¯å¾„
            enable_hybrid_search: æ˜¯å¦å¯ç”¨æ··åˆæœç´¢
            hybrid_config: æ··åˆæœç´¢é…ç½®
            llm_config: LLMé…ç½®
            enable_query_rewrite: æ˜¯å¦å¯ç”¨æŸ¥è¯¢é‡å†™
            enable_summarization: æ˜¯å¦å¯ç”¨Geminiæ‘˜è¦
            summarization_config: æ‘˜è¦é…ç½®
            enable_intent_reranking: æ˜¯å¦å¯ç”¨æ„å›¾æ„ŸçŸ¥é‡æ’åº
            reranking_config: é‡æ’åºé…ç½®
        """
        self.is_initialized = False
        self.vector_store_path = vector_store_path
        self.vector_store = None
        self.metadata = None
        self.config = None
        self.processor = None
        self.enable_hybrid_search = enable_hybrid_search
        self.hybrid_config = hybrid_config or {
            "fusion_method": "rrf",
            "vector_weight": 0.3,
            "bm25_weight": 0.7,
            "rrf_k": 60
        }
        self.llm_config = llm_config
        self.jina_api_key = jina_api_key
        self.enable_query_rewrite = enable_query_rewrite
        self.hybrid_retriever = None
        
        # æ‘˜è¦é…ç½®
        self.enable_summarization = enable_summarization and GEMINI_AVAILABLE
        self.summarization_config = summarization_config or {}
        self.summarizer = None
        
        # æ„å›¾é‡æ’åºé…ç½®
        self.enable_intent_reranking = enable_intent_reranking and RERANKER_AVAILABLE
        self.reranking_config = reranking_config or {
            "intent_weight": 0.4,
            "semantic_weight": 0.6
        }
        self.reranker = None
        
        # åˆå§‹åŒ–æ‘˜è¦å™¨
        if self.enable_summarization:
            self._initialize_summarizer()
            
        # åˆå§‹åŒ–é‡æ’åºå™¨
        if self.enable_intent_reranking:
            self._initialize_reranker()
        
    async def initialize(self, game_name: Optional[str] = None):
        """
        åˆå§‹åŒ–RAGç³»ç»Ÿ
        
        Args:
            game_name: æ¸¸æˆåç§°ï¼Œç”¨äºè‡ªåŠ¨æŸ¥æ‰¾å‘é‡åº“
        """
        try:
            print(f"ğŸ”§ [RAG-DEBUG] å¼€å§‹åˆå§‹åŒ–RAGç³»ç»Ÿ - æ¸¸æˆ: {game_name}")
            logger.info("åˆå§‹åŒ–å¢å¼ºRAGç³»ç»Ÿ...")
            
            if not BATCH_EMBEDDING_AVAILABLE:
                error_msg = "å‘é‡æœç´¢åŠŸèƒ½ä¸å¯ç”¨: æ‰¹é‡åµŒå…¥æ¨¡å—å¯¼å…¥å¤±è´¥ã€‚è¯·æ£€æŸ¥ä»¥ä¸‹ä¾èµ–æ˜¯å¦æ­£ç¡®å®‰è£…:\n1. numpy\n2. faiss-cpu\n3. å…¶ä»–åµŒå…¥ç›¸å…³ä¾èµ–"
                print(f"âŒ [RAG-DEBUG] {error_msg}")
                logger.error(error_msg)
                raise VectorStoreUnavailableError(error_msg)
            
            # ç¡®å®šå‘é‡åº“è·¯å¾„
            if self.vector_store_path is None and game_name:
                # è‡ªåŠ¨æŸ¥æ‰¾å‘é‡åº“ - ä½¿ç”¨èµ„æºè·¯å¾„å‡½æ•°
                vector_dir = get_resource_path("ai/vectorstore")
                
                print(f"ğŸ” [RAG-DEBUG] æŸ¥æ‰¾å‘é‡åº“ç›®å½•: {vector_dir}")
                logger.info(f"æŸ¥æ‰¾å‘é‡åº“ç›®å½•: {vector_dir}")
                config_files = list(vector_dir.glob(f"{game_name}_vectors_config.json"))
                
                if config_files:
                    self.vector_store_path = str(config_files[0])
                    print(f"âœ… [RAG-DEBUG] æ‰¾åˆ°å‘é‡åº“é…ç½®: {self.vector_store_path}")
                    logger.info(f"æ‰¾åˆ°å‘é‡åº“é…ç½®: {self.vector_store_path}")
                else:
                    error_msg = f"å‘é‡åº“ä¸å­˜åœ¨: æœªæ‰¾åˆ°æ¸¸æˆ '{game_name}' çš„å‘é‡åº“é…ç½®æ–‡ä»¶\næœç´¢è·¯å¾„: {vector_dir}\næŸ¥æ‰¾æ¨¡å¼: {game_name}_vectors_config.json"
                    
                    # åˆ—å‡ºç°æœ‰çš„æ–‡ä»¶ç”¨äºè°ƒè¯•
                    try:
                        existing_files = list(vector_dir.glob("*_vectors_config.json"))
                        if existing_files:
                            available_games = [f.stem.replace("_vectors_config", "") for f in existing_files]
                            error_msg += f"\nå¯ç”¨çš„æ¸¸æˆå‘é‡åº“: {', '.join(available_games)}"
                        else:
                            error_msg += "\næœªæ‰¾åˆ°ä»»ä½•å‘é‡åº“é…ç½®æ–‡ä»¶"
                    except Exception as e:
                        error_msg += f"\næ— æ³•åˆ—å‡ºç°æœ‰æ–‡ä»¶: {e}"
                    
                    print(f"âŒ [RAG-DEBUG] {error_msg}")
                    logger.error(error_msg)
                    raise VectorStoreUnavailableError(error_msg)
            
            if not self.vector_store_path or not Path(self.vector_store_path).exists():
                error_msg = f"å‘é‡åº“é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {self.vector_store_path}"
                logger.error(error_msg)
                raise VectorStoreUnavailableError(error_msg)
            
            # åŠ è½½å‘é‡åº“
            try:
                self.processor = BatchEmbeddingProcessor(api_key=self.jina_api_key)
                self.vector_store = self.processor.load_vector_store(self.vector_store_path)
                
                # åŠ è½½é…ç½®å’Œå…ƒæ•°æ®
                with open(self.vector_store_path, 'r', encoding='utf-8') as f:
                    self.config = json.load(f)
                
                if self.config["vector_store_type"] == "faiss":
                    self.metadata = self.vector_store["metadata"]
                
                logger.info(f"å‘é‡åº“åŠ è½½å®Œæˆ: {self.config['chunk_count']} ä¸ªçŸ¥è¯†å—")
                
                # Store game name from initial parameter
                self.game_name = game_name
                
                # åˆå§‹åŒ–æ··åˆæ£€ç´¢å™¨
                if self.enable_hybrid_search:
                    self._initialize_hybrid_retriever()
                    
            except Exception as e:
                error_msg = f"å‘é‡åº“åŠ è½½å¤±è´¥: {e}"
                logger.error(error_msg)
                raise VectorStoreUnavailableError(error_msg)
            
            self.is_initialized = True
            logger.info("å¢å¼ºRAGç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
            
        except VectorStoreUnavailableError:
            # é‡æ–°æŠ›å‡ºå‘é‡åº“ç‰¹å®šé”™è¯¯
            self.is_initialized = False
            raise
        except Exception as e:
            error_msg = f"RAGç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥: {e}"
            logger.error(error_msg)
            self.is_initialized = False
            raise VectorStoreUnavailableError(error_msg)
    
    def _initialize_hybrid_retriever(self):
        """
        åˆå§‹åŒ–æ··åˆæ£€ç´¢å™¨
        
        Raises:
            VectorStoreUnavailableError: å½“æ··åˆæœç´¢åˆå§‹åŒ–å¤±è´¥æ—¶
        """
        if not self.enable_hybrid_search:
            logger.warning("æ··åˆæœç´¢æœªå¯ç”¨ï¼Œå°†ä»…ä½¿ç”¨å‘é‡æœç´¢")
            return
        
        if not HYBRID_RETRIEVER_AVAILABLE:
            error_msg = "æ··åˆæœç´¢åˆå§‹åŒ–å¤±è´¥: æ··åˆæ£€ç´¢å™¨æ¨¡å—ä¸å¯ç”¨"
            logger.error(error_msg)
            raise VectorStoreUnavailableError(error_msg)
        
        try:
            # æ£€æŸ¥BM25ç´¢å¼•æ–‡ä»¶æ˜¯å¦å­˜åœ¨ - ä¿®å¤è·¯å¾„è§£æé—®é¢˜
            from pathlib import Path
            bm25_index_path = self.config.get("bm25_index_path")
            if not bm25_index_path:
                error_msg = "æ··åˆæœç´¢åˆå§‹åŒ–å¤±è´¥: BM25ç´¢å¼•è·¯å¾„æœªåœ¨é…ç½®ä¸­æ‰¾åˆ°"
                logger.error(error_msg)
                raise VectorStoreUnavailableError(error_msg)
            
            # å¦‚æœæ˜¯ç›¸å¯¹è·¯å¾„ï¼ŒåŸºäºèµ„æºè·¯å¾„æ„å»ºç»å¯¹è·¯å¾„
            bm25_path = Path(bm25_index_path)
            if not bm25_path.is_absolute():
                # ä½¿ç”¨èµ„æºè·¯å¾„å‡½æ•°æ„å»ºè·¯å¾„
                vectorstore_dir = get_resource_path("ai/vectorstore")
                # å°è¯•åŸºäºvectorstoreç›®å½•
                bm25_path = vectorstore_dir / bm25_index_path
            
            # åˆ›å»ºå‘é‡æ£€ç´¢å™¨é€‚é…å™¨
            vector_retriever = VectorRetrieverAdapter(self)
            
            # åˆ›å»ºæ··åˆæ£€ç´¢å™¨ - ä»é…ç½®ä¸­è¯»å–ç»Ÿä¸€å¤„ç†è®¾ç½®
            enable_unified_processing = self.hybrid_config.get("enable_unified_processing", True)
            enable_query_rewrite = self.hybrid_config.get("enable_query_rewrite", self.enable_query_rewrite)
            
            self.hybrid_retriever = HybridSearchRetriever(
                vector_retriever=vector_retriever,
                bm25_index_path=str(bm25_path),
                fusion_method=self.hybrid_config.get("fusion_method", "rrf"),
                vector_weight=self.hybrid_config.get("vector_weight", 0.3),
                bm25_weight=self.hybrid_config.get("bm25_weight", 0.7),
                rrf_k=self.hybrid_config.get("rrf_k", 60),
                llm_config=self.llm_config,
                enable_unified_processing=enable_unified_processing,  # ä»é…ç½®ä¸­è¯»å–
                enable_query_rewrite=enable_query_rewrite
            )
            
            if enable_unified_processing:
                logger.info("æ··åˆæ£€ç´¢å™¨åˆå§‹åŒ–æˆåŠŸï¼ˆç»Ÿä¸€å¤„ç†æ¨¡å¼ï¼‰")
            else:
                logger.info("æ··åˆæ£€ç´¢å™¨åˆå§‹åŒ–æˆåŠŸï¼ˆç‹¬ç«‹å¤„ç†æ¨¡å¼ï¼Œç¦ç”¨ç»Ÿä¸€å¤„ç†ï¼‰")
            
        except BM25UnavailableError as e:
            # BM25ç‰¹å®šé”™è¯¯ï¼Œé‡æ–°åŒ…è£…ä¸ºå‘é‡åº“é”™è¯¯
            error_msg = f"æ··åˆæœç´¢åˆå§‹åŒ–å¤±è´¥: {e}"
            logger.error(error_msg)
            raise VectorStoreUnavailableError(error_msg)
        except (FileNotFoundError, RuntimeError) as e:
            # æ–‡ä»¶ä¸å­˜åœ¨æˆ–å…¶ä»–è¿è¡Œæ—¶é”™è¯¯
            error_msg = f"æ··åˆæœç´¢åˆå§‹åŒ–å¤±è´¥: {e}"
            logger.error(error_msg)
            raise VectorStoreUnavailableError(error_msg)
        except Exception as e:
            error_msg = f"æ··åˆæ£€ç´¢å™¨åˆå§‹åŒ–å¤±è´¥: {e}"
            logger.error(error_msg)
            raise VectorStoreUnavailableError(error_msg)
    
    def _initialize_summarizer(self):
        """åˆå§‹åŒ–Geminiæ‘˜è¦å™¨"""
        try:
            import os
            
            # è·å–APIå¯†é’¥ï¼Œä¼˜å…ˆçº§ï¼šLLMé…ç½® > æ‘˜è¦é…ç½® > ç¯å¢ƒå˜é‡
            api_key = None
            if self.llm_config and hasattr(self.llm_config, 'get_api_key'):
                api_key = self.llm_config.get_api_key()
            
            if not api_key:
                api_key = self.summarization_config.get("api_key") or os.environ.get("GEMINI_API_KEY")
            
            if not api_key:
                logger.warning("æœªæ‰¾åˆ°Gemini APIå¯†é’¥ï¼Œæ‘˜è¦åŠŸèƒ½å°†è¢«ç¦ç”¨")
                self.enable_summarization = False
                return
            
            # åˆ›å»ºæ‘˜è¦é…ç½® (ç§»é™¤å·²åºŸå¼ƒçš„max_summary_lengthå‚æ•°)
            config = SummarizationConfig(
                api_key=api_key,
                model_name=self.summarization_config.get("model_name", "gemini-2.5-flash-lite-preview-06-17"),
                temperature=self.summarization_config.get("temperature", 0.3),
                include_sources=self.summarization_config.get("include_sources", True),
                language=self.summarization_config.get("language", "auto")
            )
            
            # åˆ›å»ºæ‘˜è¦å™¨ (ç§»é™¤å·²åºŸå¼ƒçš„max_summary_lengthå‚æ•°)
            self.summarizer = create_gemini_summarizer(
                api_key=api_key,
                model_name=config.model_name,
                temperature=config.temperature,
                include_sources=config.include_sources,
                language=config.language
            )
            
            logger.info(f"Geminiæ‘˜è¦å™¨åˆå§‹åŒ–æˆåŠŸ: {config.model_name}")
            
        except Exception as e:
            logger.error(f"Geminiæ‘˜è¦å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            self.enable_summarization = False
    
    def _initialize_reranker(self):
        """åˆå§‹åŒ–æ„å›¾æ„ŸçŸ¥é‡æ’åºå™¨"""
        try:
            self.reranker = IntentAwareReranker()
            logger.info("æ„å›¾æ„ŸçŸ¥é‡æ’åºå™¨åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            logger.error(f"æ„å›¾é‡æ’åºå™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            self.enable_intent_reranking = False
    
    def _search_faiss(self, query: str, top_k: int = 3) -> List[Dict[str, Any]]:
        """
        ä½¿ç”¨FAISSè¿›è¡Œå‘é‡æ£€ç´¢
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            top_k: è¿”å›ç»“æœæ•°é‡
            
        Returns:
            æ£€ç´¢ç»“æœåˆ—è¡¨
        """
        print(f"ğŸ” [VECTOR-DEBUG] å¼€å§‹FAISSå‘é‡æ£€ç´¢: query='{query}', top_k={top_k}")
        
        if not self.vector_store or not self.metadata:
            print(f"âš ï¸ [VECTOR-DEBUG] å‘é‡åº“æˆ–å…ƒæ•°æ®æœªåˆå§‹åŒ–")
            logger.warning("å‘é‡åº“æˆ–å…ƒæ•°æ®æœªåˆå§‹åŒ–")
            return []
        
        try:
            # è·å–æŸ¥è¯¢å‘é‡
            query_text = self.processor.build_text({"topic": query, "summary": query, "keywords": []})
            print(f"ğŸ“„ [VECTOR-DEBUG] æ„å»ºæŸ¥è¯¢æ–‡æœ¬: '{query_text[:100]}...'")
            
            query_vectors = self.processor.embed_batch([query_text])
            query_vector = np.array(query_vectors[0], dtype=np.float32).reshape(1, -1)
            print(f"ğŸ”¢ [VECTOR-DEBUG] æŸ¥è¯¢å‘é‡ç»´åº¦: {query_vector.shape}, å‰5ä¸ªå€¼: {query_vector[0][:5]}")
            
            # æ„å»ºæ­£ç¡®çš„ç´¢å¼•æ–‡ä»¶è·¯å¾„
            # ä½¿ç”¨ä¸BatchEmbeddingProcessor._load_faiss_storeç›¸åŒçš„è·¯å¾„é€»è¾‘
            index_path_str = self.config["index_path"]
            if not Path(index_path_str).is_absolute():
                # ä½¿ç”¨èµ„æºè·¯å¾„å‡½æ•°æ¥æ„å»ºç»å¯¹è·¯å¾„
                vectorstore_dir = get_resource_path("ai/vectorstore")
                index_path = vectorstore_dir / Path(index_path_str).name
            else:
                index_path = Path(index_path_str)
            
            index_file_path = index_path / "index.faiss"
            print(f"ğŸ“‚ [VECTOR-DEBUG] FAISSç´¢å¼•æ–‡ä»¶è·¯å¾„: {index_file_path}")
            logger.info(f"å°è¯•åŠ è½½FAISSç´¢å¼•æ–‡ä»¶: {index_file_path}")
            
            if not index_file_path.exists():
                print(f"âŒ [VECTOR-DEBUG] FAISSç´¢å¼•æ–‡ä»¶ä¸å­˜åœ¨: {index_file_path}")
                logger.error(f"FAISSç´¢å¼•æ–‡ä»¶ä¸å­˜åœ¨: {index_file_path}")
                return []
            
            # åŠ è½½FAISSç´¢å¼•
            try:
                import faiss
            except ImportError:
                logger.error("æ— æ³•å¯¼å…¥faissåº“")
                print(f"âŒ [VECTOR-DEBUG] æ— æ³•å¯¼å…¥faissåº“ï¼Œè¯·ç¡®ä¿å·²å®‰è£…faiss-cpu")
                return []
            
            index = faiss.read_index(str(index_file_path))
            print(f"ğŸ“Š [VECTOR-DEBUG] FAISSç´¢å¼•ä¿¡æ¯: æ€»å‘é‡æ•°={index.ntotal}, ç»´åº¦={index.d}")
            
            # æ‰§è¡Œæ£€ç´¢
            scores, indices = index.search(query_vector, top_k)
            print(f"ğŸ” [VECTOR-DEBUG] FAISSæ£€ç´¢åŸå§‹ç»“æœ:")
            print(f"   - æ£€ç´¢åˆ°çš„ç´¢å¼•: {indices[0]}")
            print(f"   - ç›¸ä¼¼åº¦åˆ†æ•°: {scores[0]}")
            
            # è¿”å›ç»“æœ
            results = []
            for i, (score, idx) in enumerate(zip(scores[0], indices[0])):
                if idx < len(self.metadata):
                    chunk = self.metadata[idx]
                    chunk_info = {
                        "chunk": chunk,
                        "score": float(score),
                        "rank": i + 1
                    }
                    results.append(chunk_info)
                    
                    # è¯¦ç»†çš„ç»“æœè°ƒè¯•ä¿¡æ¯
                    print(f"   ğŸ“‹ [VECTOR-DEBUG] ç»“æœ {i+1}:")
                    print(f"      - ç›¸ä¼¼åº¦åˆ†æ•°: {score:.4f}")
                    print(f"      - ç´¢å¼•ID: {idx}")
                    print(f"      - ä¸»é¢˜: {chunk.get('topic', 'Unknown')}")
                    print(f"      - æ‘˜è¦: {chunk.get('summary', '')[:100]}...")
                    print(f"      - å…³é”®è¯: {chunk.get('keywords', [])}")
                    
                    # å¦‚æœæ˜¯ç»“æ„åŒ–æ•°æ®ï¼Œæ˜¾ç¤ºæ•Œäººä¿¡æ¯
                    if "structured_data" in chunk:
                        structured = chunk["structured_data"]
                        if "enemy_name" in structured:
                            print(f"      - æ•Œäººåç§°: {structured['enemy_name']}")
                        if "weak_points" in structured:
                            weak_points = [wp.get("name", "Unknown") for wp in structured["weak_points"]]
                            print(f"      - å¼±ç‚¹: {weak_points}")
            
            print(f"âœ… [VECTOR-DEBUG] FAISSæ£€ç´¢å®Œæˆï¼Œæ‰¾åˆ° {len(results)} ä¸ªç»“æœ")
            logger.info(f"FAISSæ£€ç´¢å®Œæˆï¼Œæ‰¾åˆ° {len(results)} ä¸ªç»“æœ")
            return results
            
        except Exception as e:
            print(f"âŒ [VECTOR-DEBUG] FAISSæ£€ç´¢å¤±è´¥: {e}")
            logger.error(f"FAISSæ£€ç´¢å¤±è´¥: {e}")
            return []
    
    def _search_qdrant(self, query: str, top_k: int = 3) -> List[Dict[str, Any]]:
        """
        ä½¿ç”¨Qdrantè¿›è¡Œå‘é‡æ£€ç´¢
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            top_k: è¿”å›ç»“æœæ•°é‡
            
        Returns:
            æ£€ç´¢ç»“æœåˆ—è¡¨
        """
        print(f"ğŸ” [VECTOR-DEBUG] å¼€å§‹Qdrantå‘é‡æ£€ç´¢: query='{query}', top_k={top_k}")
        
        if not self.vector_store or not QDRANT_AVAILABLE:
            print(f"âš ï¸ [VECTOR-DEBUG] Qdrantå‘é‡åº“æœªåˆå§‹åŒ–æˆ–ä¸å¯ç”¨")
            logger.warning("Qdrantå‘é‡åº“æœªåˆå§‹åŒ–æˆ–ä¸å¯ç”¨")
            return []
        
        try:
            # è·å–æŸ¥è¯¢å‘é‡
            query_text = self.processor.build_text({"topic": query, "summary": query, "keywords": []})
            print(f"ğŸ“„ [VECTOR-DEBUG] æ„å»ºæŸ¥è¯¢æ–‡æœ¬: '{query_text[:100]}...'")
            
            query_vectors = self.processor.embed_batch([query_text])
            query_vector = query_vectors[0]
            print(f"ğŸ”¢ [VECTOR-DEBUG] æŸ¥è¯¢å‘é‡ç»´åº¦: {len(query_vector)}, å‰5ä¸ªå€¼: {query_vector[:5]}")
            
            # æ‰§è¡Œæ£€ç´¢
            print(f"ğŸ” [VECTOR-DEBUG] è°ƒç”¨Qdrantæœç´¢: collection={self.config['collection_name']}")
            results = self.vector_store.search(
                collection_name=self.config["collection_name"],
                query_vector=query_vector,
                limit=top_k
            )
            
            print(f"ğŸ“Š [VECTOR-DEBUG] Qdrantæ£€ç´¢åŸå§‹ç»“æœæ•°é‡: {len(results)}")
            
            # æ ¼å¼åŒ–ç»“æœ
            formatted_results = []
            for i, result in enumerate(results):
                chunk_info = {
                    "chunk": result.payload,
                    "score": result.score,
                    "rank": i + 1
                }
                formatted_results.append(chunk_info)
                
                # è¯¦ç»†çš„ç»“æœè°ƒè¯•ä¿¡æ¯
                print(f"   ğŸ“‹ [VECTOR-DEBUG] ç»“æœ {i+1}:")
                print(f"      - ç›¸ä¼¼åº¦åˆ†æ•°: {result.score:.4f}")
                print(f"      - ä¸»é¢˜: {result.payload.get('topic', 'Unknown')}")
                print(f"      - æ‘˜è¦: {result.payload.get('summary', '')[:100]}...")
                print(f"      - å…³é”®è¯: {result.payload.get('keywords', [])}")
                
                # å¦‚æœæ˜¯ç»“æ„åŒ–æ•°æ®ï¼Œæ˜¾ç¤ºæ•Œäººä¿¡æ¯
                if "structured_data" in result.payload:
                    structured = result.payload["structured_data"]
                    if "enemy_name" in structured:
                        print(f"      - æ•Œäººåç§°: {structured['enemy_name']}")
                    if "weak_points" in structured:
                        weak_points = [wp.get("name", "Unknown") for wp in structured["weak_points"]]
                        print(f"      - å¼±ç‚¹: {weak_points}")
            
            print(f"âœ… [VECTOR-DEBUG] Qdrantæ£€ç´¢å®Œæˆï¼Œæ‰¾åˆ° {len(formatted_results)} ä¸ªç»“æœ")
            logger.info(f"Qdrantæ£€ç´¢å®Œæˆï¼Œæ‰¾åˆ° {len(formatted_results)} ä¸ªç»“æœ")
            return formatted_results
            
        except Exception as e:
            print(f"âŒ [VECTOR-DEBUG] Qdrantæ£€ç´¢å¤±è´¥: {e}")
            logger.error(f"Qdrantæ£€ç´¢å¤±è´¥: {e}")
            return []
    
    def _search_hybrid(self, query: str, top_k: int = 3) -> Dict[str, Any]:
        """
        ä½¿ç”¨æ··åˆæœç´¢è¿›è¡Œæ£€ç´¢
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            top_k: è¿”å›ç»“æœæ•°é‡
            
        Returns:
            æ··åˆæœç´¢ç»“æœï¼ˆåŒ…å«å…ƒæ•°æ®ï¼‰
        """
        print(f"ğŸ” [RAG-DEBUG] è¿›å…¥æ··åˆæœç´¢: query='{query}', top_k={top_k}")
        
        if not self.hybrid_retriever:
            print(f"âš ï¸ [RAG-DEBUG] æ··åˆæ£€ç´¢å™¨æœªåˆå§‹åŒ–ï¼Œå›é€€åˆ°å‘é‡æœç´¢")
            logger.warning("æ··åˆæ£€ç´¢å™¨æœªåˆå§‹åŒ–ï¼Œå›é€€åˆ°å‘é‡æœç´¢")
            results = self._search_faiss(query, top_k) if self.config["vector_store_type"] == "faiss" else self._search_qdrant(query, top_k)
            return {
                "results": results,
                "query": {"original": query, "rewritten": query, "rewrite_applied": False},
                "metadata": {
                    "total_results": len(results),
                    "search_type": "vector_fallback",
                    "fusion_method": "none",
                    "rewrite_info": {
                        "intent": "unknown",
                        "confidence": 0.0,
                        "reasoning": "æ··åˆæ£€ç´¢å™¨æœªåˆå§‹åŒ–"
                    }
                }
            }
        
        # æ‰§è¡Œæ··åˆæœç´¢
        try:
            print(f"ğŸš€ [RAG-DEBUG] å¼€å§‹æ‰§è¡Œæ··åˆæœç´¢")
            search_response = self.hybrid_retriever.search(query, top_k)
            result_count = len(search_response.get('results', []))
            print(f"âœ… [RAG-DEBUG] æ··åˆæœç´¢å®Œæˆï¼Œæ‰¾åˆ° {result_count} ä¸ªç»“æœ")
            logger.info(f"æ··åˆæœç´¢å®Œæˆï¼Œæ‰¾åˆ° {result_count} ä¸ªç»“æœ")
            return search_response
        except Exception as e:
            print(f"âŒ [RAG-DEBUG] æ··åˆæœç´¢å¤±è´¥: {e}")
            logger.error(f"æ··åˆæœç´¢å¤±è´¥: {e}")
            # å›é€€åˆ°å‘é‡æœç´¢
            results = self._search_faiss(query, top_k) if self.config["vector_store_type"] == "faiss" else self._search_qdrant(query, top_k)
            return {
                "results": results,
                "query": {"original": query, "rewritten": query, "rewrite_applied": False},
                "metadata": {
                    "total_results": len(results),
                    "search_type": "vector_fallback",
                    "fusion_method": "none",
                    "rewrite_info": {
                        "intent": "unknown",
                        "confidence": 0.0,
                        "reasoning": f"æ··åˆæœç´¢å¤±è´¥: {str(e)}"
                    }
                }
            }
    
    def _search_hybrid_with_processed_query(self, unified_query_result, top_k: int = 3) -> Dict[str, Any]:
        """
        ä½¿ç”¨é¢„å¤„ç†çš„ç»Ÿä¸€æŸ¥è¯¢ç»“æœè¿›è¡Œæ··åˆæœç´¢
        
        Args:
            unified_query_result: ç»Ÿä¸€æŸ¥è¯¢å¤„ç†ç»“æœå¯¹è±¡
            top_k: è¿”å›ç»“æœæ•°é‡
            
        Returns:
            æ··åˆæœç´¢ç»“æœï¼ˆåŒ…å«å…ƒæ•°æ®ï¼‰
        """
        print(f"ğŸ” [RAG-DEBUG] è¿›å…¥æ··åˆæœç´¢ï¼ˆé¢„å¤„ç†æ¨¡å¼ï¼‰: top_k={top_k}")
        
        if not self.hybrid_retriever:
            print(f"âš ï¸ [RAG-DEBUG] æ··åˆæ£€ç´¢å™¨æœªåˆå§‹åŒ–ï¼Œå›é€€åˆ°å‘é‡æœç´¢")
            logger.warning("æ··åˆæ£€ç´¢å™¨æœªåˆå§‹åŒ–ï¼Œå›é€€åˆ°å‘é‡æœç´¢")
            # ä½¿ç”¨é‡å†™åçš„æŸ¥è¯¢è¿›è¡Œå‘é‡æœç´¢
            semantic_query = unified_query_result.rewritten_query
            results = self._search_faiss(semantic_query, top_k) if self.config["vector_store_type"] == "faiss" else self._search_qdrant(semantic_query, top_k)
            return {
                "results": results,
                "query": {
                    "original": unified_query_result.original_query,
                    "processed_query": semantic_query,
                    "bm25_optimized_query": unified_query_result.bm25_optimized_query,
                    "translation_applied": unified_query_result.translation_applied,
                    "rewrite_applied": unified_query_result.rewrite_applied,
                    "intent": unified_query_result.intent,
                    "confidence": unified_query_result.confidence
                },
                "metadata": {
                    "total_results": len(results),
                    "search_type": "vector_fallback", 
                    "fusion_method": "none",
                    "rewrite_info": {
                        "intent": unified_query_result.intent,
                        "confidence": unified_query_result.confidence,
                        "reasoning": unified_query_result.reasoning
                    }
                }
            }
        
        # ç›´æ¥è°ƒç”¨æ··åˆæ£€ç´¢å™¨ï¼Œç¦ç”¨å…¶å†…éƒ¨çš„ç»Ÿä¸€å¤„ç†ï¼ˆé¿å…é‡å¤å¤„ç†ï¼‰
        try:
            print(f"ğŸš€ [RAG-DEBUG] å¼€å§‹æ‰§è¡Œæ··åˆæœç´¢ï¼ˆä½¿ç”¨é¢„å¤„ç†ç»“æœï¼‰")
            print(f"   - è¯­ä¹‰æŸ¥è¯¢: '{unified_query_result.rewritten_query}'")
            print(f"   - BM25æŸ¥è¯¢: '{unified_query_result.bm25_optimized_query}'")
            
            # æ‰‹åŠ¨æ‰§è¡Œæ··åˆæœç´¢æµç¨‹ï¼Œä½¿ç”¨é¢„å¤„ç†çš„æŸ¥è¯¢
            # å‘é‡æœç´¢ä½¿ç”¨é‡å†™æŸ¥è¯¢
            vector_search_count = 10
            bm25_search_count = 10
            
            print(f"ğŸ” [HYBRID-DEBUG] å¼€å§‹å‘é‡æœç´¢: query='{unified_query_result.rewritten_query}', top_k={vector_search_count}")
            vector_results = self.hybrid_retriever.vector_retriever.search(unified_query_result.rewritten_query, vector_search_count)
            print(f"ğŸ“Š [HYBRID-DEBUG] å‘é‡æœç´¢ç»“æœæ•°é‡: {len(vector_results)}")
            
            # BM25æœç´¢ä½¿ç”¨ä¼˜åŒ–æŸ¥è¯¢
            bm25_results = []
            if self.hybrid_retriever.bm25_indexer:
                print(f"ğŸ” [HYBRID-DEBUG] å¼€å§‹BM25æœç´¢:")
                print(f"   - åŸå§‹æŸ¥è¯¢: '{unified_query_result.original_query}'")
                print(f"   - è¯­ä¹‰æŸ¥è¯¢: '{unified_query_result.rewritten_query}'")
                print(f"   - BM25ä¼˜åŒ–: '{unified_query_result.bm25_optimized_query}'")
                print(f"   - æ£€ç´¢æ•°é‡: {bm25_search_count}")
                
                bm25_results = self.hybrid_retriever.bm25_indexer.search(unified_query_result.bm25_optimized_query, bm25_search_count)
                print(f"ğŸ“Š [HYBRID-DEBUG] BM25æœç´¢ç»“æœæ•°é‡: {len(bm25_results)}")
            else:
                print(f"âš ï¸ [HYBRID-DEBUG] BM25ç´¢å¼•å™¨æœªåˆå§‹åŒ–ï¼Œè·³è¿‡BM25æœç´¢")
            
            # åˆ†æ•°èåˆ
            final_result_count = 5
            print(f"ğŸ”„ [HYBRID-DEBUG] å¼€å§‹åˆ†æ•°èåˆ: æ–¹æ³•={self.hybrid_retriever.fusion_method}")
            
            final_results = self.hybrid_retriever._fuse_results(vector_results, bm25_results, final_result_count)
            
            print(f"âœ… [HYBRID-DEBUG] åˆ†æ•°èåˆå®Œæˆï¼Œæœ€ç»ˆç»“æœæ•°é‡: {len(final_results)}")
            
            # æ„å»ºè¿”å›ç»“æœ
            return {
                "results": final_results,
                "query": {
                    "original": unified_query_result.original_query,
                    "processed_query": unified_query_result.rewritten_query,
                    "bm25_optimized_query": unified_query_result.bm25_optimized_query,
                    "translation_applied": unified_query_result.translation_applied,
                    "rewrite_applied": unified_query_result.rewrite_applied,
                    "intent": unified_query_result.intent,
                    "confidence": unified_query_result.confidence,
                    "detected_language": unified_query_result.detected_language,
                    "processing_method": "preprocessed",
                    "reasoning": unified_query_result.reasoning
                },
                "metadata": {
                    "fusion_method": self.hybrid_retriever.fusion_method,
                    "vector_results_count": len(vector_results),
                    "bm25_results_count": len(bm25_results),
                    "final_results_count": len(final_results),
                    "vector_search_count": vector_search_count,
                    "bm25_search_count": bm25_search_count,
                    "target_final_count": final_result_count,
                    "processing_stats": {
                        "preprocessed_mode": True,
                        "avoided_duplicate_processing": True
                    }
                }
            }
            
        except Exception as e:
            print(f"âŒ [RAG-DEBUG] æ··åˆæœç´¢å¤±è´¥: {e}")
            logger.error(f"æ··åˆæœç´¢å¤±è´¥: {e}")
            # å›é€€åˆ°å‘é‡æœç´¢
            semantic_query = unified_query_result.rewritten_query
            results = self._search_faiss(semantic_query, top_k) if self.config["vector_store_type"] == "faiss" else self._search_qdrant(semantic_query, top_k)
            return {
                "results": results,
                "query": {
                    "original": unified_query_result.original_query,
                    "processed_query": semantic_query,
                    "bm25_optimized_query": unified_query_result.bm25_optimized_query,
                    "translation_applied": unified_query_result.translation_applied,
                    "rewrite_applied": unified_query_result.rewrite_applied
                },
                "metadata": {
                    "total_results": len(results),
                    "search_type": "vector_fallback",
                    "fusion_method": "none",
                    "rewrite_info": {
                        "intent": unified_query_result.intent,
                        "confidence": unified_query_result.confidence,
                        "reasoning": f"æ··åˆæœç´¢å¤±è´¥: {str(e)}"
                    }
                }
            }
    
    def _format_answer(self, search_response: Dict[str, Any], question: str) -> str:
        """
        æ ¼å¼åŒ–æ£€ç´¢ç»“æœä¸ºç­”æ¡ˆ
        
        Args:
            search_response: æœç´¢å“åº”ï¼ˆåŒ…å«resultså’Œmetadataï¼‰
            question: åŸå§‹é—®é¢˜
            
        Returns:
            æ ¼å¼åŒ–çš„ç­”æ¡ˆ
        """
        results = search_response.get("results", [])
        metadata = search_response.get("metadata", {})
        query_info = search_response.get("query", {})
        
        if not results:
            return f"æŠ±æ­‰ï¼Œæ²¡æœ‰æ‰¾åˆ°å…³äº'{question}'çš„ç›¸å…³ä¿¡æ¯ã€‚"
        
        # æ„å»ºç­”æ¡ˆ
        answer_parts = [f"å…³äº'{question}'çš„æ”»ç•¥ä¿¡æ¯ï¼š\n"]
        
        # å¦‚æœæŸ¥è¯¢è¢«ç¿»è¯‘ï¼Œæ˜¾ç¤ºç¿»è¯‘ä¿¡æ¯
        if query_info.get("translation_applied", False):
            translation_info = metadata.get("translation_info", {})
            translated_query = translation_info.get("translated_query", "")
            if translated_query:
                answer_parts.append(f"æŸ¥è¯¢ç¿»è¯‘: '{question}' -> '{translated_query}'")
        
        # å¦‚æœæŸ¥è¯¢è¢«é‡å†™ï¼Œæ˜¾ç¤ºç›¸å…³ä¿¡æ¯
        if query_info.get("rewrite_applied", False):
            rewrite_info = metadata.get("rewrite_info", {})
            answer_parts.append(f"æ„å›¾åˆ†æ: {rewrite_info.get('intent', 'unknown')}")
            answer_parts.append(f"æŸ¥è¯¢ä¼˜åŒ–: {rewrite_info.get('reasoning', 'æœªçŸ¥')}")
        
        # å¦‚æœæœ‰ç¿»è¯‘æˆ–é‡å†™ä¿¡æ¯ï¼Œæ·»åŠ ç©ºè¡Œ
        if query_info.get("translation_applied", False) or query_info.get("rewrite_applied", False):
            answer_parts.append("")
        
        for result in results:
            chunk = result["chunk"]
            score = result["score"]
            
            # æå–å…³é”®ä¿¡æ¯
            topic = chunk.get("topic", "æœªçŸ¥ä¸»é¢˜")
            summary = chunk.get("summary", "")
            
            answer_parts.append(f"\nã€{topic}ã€‘")
            
            # æ˜¾ç¤ºåˆ†æ•°ä¿¡æ¯ï¼ˆåŒºåˆ†æ··åˆæœç´¢å’Œå•ä¸€æœç´¢ï¼‰
            if "fusion_method" in result:
                # æ··åˆæœç´¢ç»“æœ
                fusion_method = result.get("fusion_method", "unknown")
                vector_score = result.get("vector_score", 0)
                bm25_score = result.get("bm25_score", 0)
                answer_parts.append(f"ç›¸å…³åº¦: {score:.3f}")
                if vector_score > 0 and bm25_score > 0:
                    answer_parts.append(f"(è¯­ä¹‰åŒ¹é…: {vector_score:.3f} | å…³é”®è¯åŒ¹é…: {bm25_score:.3f})")
            else:
                # å•ä¸€æœç´¢ç»“æœ
                answer_parts.append(f"ç›¸å…³åº¦: {score:.3f}")
            
            answer_parts.append(f"{summary}")
            
            # å¦‚æœæœ‰buildä¿¡æ¯ï¼Œæ·»åŠ é…è£…å»ºè®®
            if "build" in chunk:
                build = chunk["build"]
                if "name" in build:
                    answer_parts.append(f"\næ¨èé…è£…: {build['name']}")
                if "focus" in build:
                    answer_parts.append(f"é…è£…é‡ç‚¹: {build['focus']}")
                
                # æ·»åŠ å…³é”®è£…å¤‡ä¿¡æ¯
                if "stratagems" in build:
                    stratagems = [s["name"] for s in build["stratagems"]]
                    answer_parts.append(f"æ ¸å¿ƒè£…å¤‡: {', '.join(stratagems[:3])}")
        
        return "\n".join(answer_parts)
    
    async def _format_answer_with_summary(self, search_response: Dict[str, Any], question: str, original_query: str = None) -> str:
        """
        ä½¿ç”¨Geminiæ‘˜è¦å™¨æ ¼å¼åŒ–æ£€ç´¢ç»“æœ
        
        Args:
            search_response: æœç´¢å“åº”ï¼ˆåŒ…å«resultså’Œmetadataï¼‰
            question: åŸå§‹é—®é¢˜
            
        Returns:
            æ‘˜è¦åçš„ç­”æ¡ˆ
        """
        results = search_response.get("results", [])
        
        if not results:
            return "ğŸ˜” æŠ±æ­‰ï¼Œæˆ‘æ²¡æœ‰æ‰¾åˆ°ç›¸å…³çš„æ¸¸æˆæ”»ç•¥ä¿¡æ¯ã€‚å¯ä»¥è¯•è¯•æ¢ä¸ªå…³é”®è¯é—®æˆ‘å“¦ï¼"
        
        try:
            # å‡†å¤‡çŸ¥è¯†å—æ•°æ®ï¼ŒåŒ…å«å®Œæ•´çš„ç»“æ„åŒ–ä¿¡æ¯
            chunks = []
            for result in results:
                chunk_data = result["chunk"]
                
                # ä¼ é€’å®Œæ•´çš„ chunk æ•°æ®ï¼ŒåŒ…æ‹¬ structured_data
                chunk_for_summary = {
                    "topic": chunk_data.get("topic", "æœªçŸ¥ä¸»é¢˜"),
                    "summary": chunk_data.get("summary", ""),
                    "keywords": chunk_data.get("keywords", []),
                    "type": chunk_data.get("type", "General"),
                    "structured_data": chunk_data.get("structured_data", {}),
                    "score": result.get("score", 0),
                    "content": chunk_data.get("summary", "")
                }
                
                chunks.append(chunk_for_summary)
            
            # è·å–æ¸¸æˆä¸Šä¸‹æ–‡
            game_context = None
            # å°è¯•å¤šç§æ–¹å¼è·å–æ¸¸æˆä¸Šä¸‹æ–‡
            if chunks:
                first_chunk = chunks[0]
                # æ–¹å¼1: ç›´æ¥ä»chunkè·å–gameå­—æ®µ
                if "game" in first_chunk:
                    game_context = first_chunk["game"]
                # æ–¹å¼2: ä»video_infoä¸­è·å–
                elif "video_info" in first_chunk and isinstance(first_chunk["video_info"], dict):
                    game_context = first_chunk["video_info"].get("game")
            
            # æ–¹å¼3: ä»configè·å–
            if not game_context and hasattr(self, 'config') and self.config:
                game_context = self.config.get("game_name", None)
            
            # æ–¹å¼4: ä½¿ç”¨å­˜å‚¨çš„game_name
            if not game_context and hasattr(self, 'game_name'):
                game_context = self.game_name
            
            # Set game name in summarizer for video source extraction
            if game_context and hasattr(self.summarizer, 'current_game_name'):
                self.summarizer.current_game_name = game_context
            
            # è°ƒç”¨æ‘˜è¦å™¨ç”Ÿæˆç»“æ„åŒ–å›å¤
            summary_result = self.summarizer.summarize_chunks(
                chunks=chunks,
                query=question,
                original_query=original_query,
                context=game_context
            )
            
            # ç›´æ¥è¿”å›æ‘˜è¦å†…å®¹ï¼ˆä¸€å¥è¯æ€»ç»“+è¯¦ç»†è®²è§£æ ¼å¼ï¼‰
            return summary_result["summary"]
            
        except Exception as e:
            logger.error(f"æ‘˜è¦ç”Ÿæˆå¤±è´¥: {e}")
            # å›é€€åˆ°å‹å¥½çš„é”™è¯¯æ¶ˆæ¯
            return "ğŸ˜… æŠ±æ­‰ï¼Œæˆ‘åœ¨æ•´ç†ä¿¡æ¯æ—¶é‡åˆ°äº†ä¸€ç‚¹é—®é¢˜ã€‚è®©æˆ‘ç”¨ç®€å•çš„æ–¹å¼å›ç­”ä½ ï¼š\n\n" + self._format_simple_answer(results)
    
    async def _format_answer_with_summary_stream(self, search_response: Dict[str, Any], question: str, original_query: str = None) -> AsyncGenerator[str, None]:
        """
        ä½¿ç”¨Geminiæ‘˜è¦å™¨æµå¼æ ¼å¼åŒ–æ£€ç´¢ç»“æœ
        
        Args:
            search_response: æœç´¢å“åº”ï¼ˆåŒ…å«resultså’Œmetadataï¼‰
            question: åŸå§‹é—®é¢˜
            original_query: åŸå§‹æŸ¥è¯¢
            
        Yields:
            æµå¼æ‘˜è¦å†…å®¹
        """
        results = search_response.get("results", [])
        
        if not results:
            yield "æŠ±æ­‰ï¼Œæ²¡æœ‰æ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€‚è¯·å°è¯•å…¶ä»–å…³é”®è¯ã€‚"
            return
            
        try:
            print(f"ğŸŒŠ [RAG-STREAM-DEBUG] å¼€å§‹æµå¼æ‘˜è¦æ ¼å¼åŒ–")
            print(f"   - æ£€ç´¢ç»“æœæ•°é‡: {len(results)}")
            
            # æ„å»ºæ‘˜è¦æ•°æ®
            chunks = []
            for result in results:
                chunk = result.get("chunk", result)
                chunks.append(chunk)
            
            # æå–æ¸¸æˆä¸Šä¸‹æ–‡
            game_context = None
            # å°è¯•å¤šç§æ–¹å¼è·å–æ¸¸æˆä¸Šä¸‹æ–‡
            if chunks:
                first_chunk = chunks[0]
                # æ–¹å¼1: ç›´æ¥ä»chunkè·å–gameå­—æ®µ
                if "game" in first_chunk:
                    game_context = first_chunk["game"]
                # æ–¹å¼2: ä»video_infoä¸­è·å–
                elif "video_info" in first_chunk and isinstance(first_chunk["video_info"], dict):
                    game_context = first_chunk["video_info"].get("game")
            
            # æ–¹å¼3: ä»configæˆ–åˆå§‹åŒ–å‚æ•°è·å–
            if not game_context and hasattr(self, 'config') and self.config:
                game_context = self.config.get("game_name", None)
            
            # æ–¹å¼4: ä½¿ç”¨å­˜å‚¨çš„game_name
            if not game_context and hasattr(self, 'game_name'):
                game_context = self.game_name
            
            print(f"ğŸ® [RAG-STREAM-DEBUG] æ¸¸æˆä¸Šä¸‹æ–‡: {game_context}")
            
            # è®¾ç½®æ¸¸æˆåç§°åˆ°æ‘˜è¦å™¨ä¸­ç”¨äºè§†é¢‘æºæå–
            if game_context and hasattr(self.summarizer, 'current_game_name'):
                self.summarizer.current_game_name = game_context
            
            # è°ƒç”¨æµå¼æ‘˜è¦å™¨ç”Ÿæˆç»“æ„åŒ–å›å¤
            print(f"ğŸš€ [RAG-STREAM-DEBUG] è°ƒç”¨æµå¼æ‘˜è¦å™¨")
            async for chunk in self.summarizer.summarize_chunks_stream(
                chunks=chunks,
                query=question,
                original_query=original_query,
                context=game_context
            ):
                print(f"ğŸ“¦ [RAG-STREAM-DEBUG] æ”¶åˆ°æ‘˜è¦å—: {len(chunk)} å­—ç¬¦")
                yield chunk
            
            print(f"âœ… [RAG-STREAM-DEBUG] æµå¼æ‘˜è¦æ ¼å¼åŒ–å®Œæˆ")
            
        except Exception as e:
            logger.error(f"æµå¼æ‘˜è¦ç”Ÿæˆå¤±è´¥: {e}")
            print(f"âŒ [RAG-STREAM-DEBUG] æµå¼æ‘˜è¦ç”Ÿæˆå¤±è´¥: {e}")
            # å›é€€åˆ°å‹å¥½çš„é”™è¯¯æ¶ˆæ¯
            yield "ğŸ˜… æŠ±æ­‰ï¼Œæˆ‘åœ¨æ•´ç†ä¿¡æ¯æ—¶é‡åˆ°äº†ä¸€ç‚¹é—®é¢˜ã€‚è®©æˆ‘ç”¨ç®€å•çš„æ–¹å¼å›ç­”ä½ ï¼š\n\n"
            yield self._format_simple_answer(results)

    def _format_simple_answer(self, results: List[Dict[str, Any]]) -> str:
        """ç®€å•æ ¼å¼åŒ–ç­”æ¡ˆï¼ˆç”¨äºæ‘˜è¦å¤±è´¥æ—¶çš„é™çº§ï¼‰"""
        if not results:
            return "æ²¡æœ‰æ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€‚"
        
        # åªå–æœ€ç›¸å…³çš„ç»“æœ
        top_result = results[0]
        chunk = top_result.get("chunk", top_result)
        
        topic = chunk.get("topic", "")
        summary = chunk.get("summary", "")
        
        return f"æ ¹æ®{topic}ï¼š\n{summary}"
    
    async def query(self, question: str, top_k: int = 3, original_query: str = None, unified_query_result = None) -> Dict[str, Any]:
        """
        æ‰§è¡ŒRAGæŸ¥è¯¢
        
        Args:
            question: ç”¨æˆ·é—®é¢˜
            top_k: æ£€ç´¢ç»“æœæ•°é‡
            original_query: åŸå§‹æŸ¥è¯¢ï¼ˆç”¨äºæ‘˜è¦ï¼‰
            unified_query_result: é¢„å¤„ç†çš„ç»Ÿä¸€æŸ¥è¯¢ç»“æœï¼ˆæ¥è‡ªassistant_integrationï¼‰
            
        Returns:
            åŒ…å«ç­”æ¡ˆçš„å­—å…¸
        """
        if not self.is_initialized:
            await self.initialize()
            
        # å¦‚æœåˆå§‹åŒ–åä»ç„¶æ²¡æœ‰å‘é‡åº“ï¼Œè¿”å›fallback_to_wiki
        if not self.is_initialized or not self.vector_store:
            print(f"âŒ [RAG-DEBUG] RAGç³»ç»Ÿæœªæ­£ç¡®åˆå§‹åŒ–ï¼Œå»ºè®®åˆ‡æ¢åˆ°wikiæ¨¡å¼")
            return {
                "answer": "",
                "sources": [],
                "confidence": 0.0,
                "query_time": 0.0,
                "results_count": 0,
                "error": "RAG_NOT_INITIALIZED",
                "fallback_to_wiki": True
            }
        
        try:
            print(f"ğŸ” [RAG-DEBUG] å¼€å§‹RAGæŸ¥è¯¢: {question}")
            if unified_query_result:
                print(f"ğŸ“ [RAG-DEBUG] ä½¿ç”¨é¢„å¤„ç†çš„ç»Ÿä¸€æŸ¥è¯¢ç»“æœ:")
                print(f"   - åŸå§‹æŸ¥è¯¢: '{unified_query_result.original_query}'")
                print(f"   - ç¿»è¯‘æŸ¥è¯¢: '{unified_query_result.translated_query}'") 
                print(f"   - é‡å†™æŸ¥è¯¢: '{unified_query_result.rewritten_query}'")
                print(f"   - BM25ä¼˜åŒ–: '{unified_query_result.bm25_optimized_query}'")
                print(f"   - æ„å›¾: {unified_query_result.intent} (ç½®ä¿¡åº¦: {unified_query_result.confidence:.3f})")
            
            start_time = asyncio.get_event_loop().time()
            
            # æ‰§è¡Œæ£€ç´¢
            if self.vector_store and self.config:
                # é€‰æ‹©æœç´¢æ–¹å¼
                if self.enable_hybrid_search and self.hybrid_retriever:
                    print(f"ğŸ” [RAG-DEBUG] ä½¿ç”¨æ··åˆæœç´¢")
                    # å¦‚æœæœ‰é¢„å¤„ç†ç»“æœï¼Œä¼ é€’ç»™æ··åˆæœç´¢
                    if unified_query_result:
                        search_response = self._search_hybrid_with_processed_query(unified_query_result, top_k)
                    else:
                        search_response = self._search_hybrid(question, top_k)
                    
                    results = search_response.get("results", [])
                    
                    # åº”ç”¨æ„å›¾æ„ŸçŸ¥é‡æ’åº
                    if self.enable_intent_reranking and self.reranker and results:
                        print(f"ğŸ”„ [RAG-DEBUG] åº”ç”¨æ„å›¾æ„ŸçŸ¥é‡æ’åº")
                        results = self.reranker.rerank_results(
                            results, 
                            question,
                            intent_weight=self.reranking_config.get("intent_weight", 0.4),
                            semantic_weight=self.reranking_config.get("semantic_weight", 0.6)
                        )
                        search_response["results"] = results
                        # åœ¨å…ƒæ•°æ®ä¸­è®°å½•é‡æ’åºä¿¡æ¯
                        search_response.setdefault("metadata", {})["reranking_applied"] = True
                    
                    # æ ¼å¼åŒ–ç­”æ¡ˆï¼ˆä½¿ç”¨æ‘˜è¦æˆ–åŸå§‹æ ¼å¼ï¼‰
                    print(f"ğŸ” [SUMMARY-DEBUG] æ£€æŸ¥æ‘˜è¦æ¡ä»¶ (æ··åˆæœç´¢):")
                    print(f"   - enable_summarization: {self.enable_summarization}")
                    print(f"   - summarizerå­˜åœ¨: {self.summarizer is not None}")
                    print(f"   - ç»“æœæ•°é‡: {len(results)}")
                    
                    if self.enable_summarization and self.summarizer and len(results) > 0:
                        print(f"ğŸ’¬ [RAG-DEBUG] ä½¿ç”¨Geminiæ‘˜è¦æ ¼å¼åŒ–ç­”æ¡ˆ")
                        answer = await self._format_answer_with_summary(search_response, question, original_query=original_query)
                    else:
                        print(f"ğŸ’¬ [RAG-DEBUG] ä½¿ç”¨åŸå§‹æ ¼å¼åŒ–ç­”æ¡ˆ")
                        if not self.enable_summarization:
                            print(f"   åŸå› : æ‘˜è¦åŠŸèƒ½æœªå¯ç”¨")
                        elif not self.summarizer:
                            print(f"   åŸå› : æ‘˜è¦å™¨æœªåˆå§‹åŒ–")
                        elif len(results) == 0:
                            print(f"   åŸå› : æ²¡æœ‰æ£€ç´¢ç»“æœ")
                        answer = self._format_answer(search_response, question)
                    
                    confidence = max([r["score"] for r in results]) if results else 0.0
                    sources = [r["chunk"].get("topic", "æœªçŸ¥") for r in results]
                    
                    # æ·»åŠ æœç´¢å…ƒæ•°æ®
                    search_metadata = search_response.get("metadata", {})
                    
                else:
                    # å•ä¸€æœç´¢
                    print(f"ğŸ” [RAG-DEBUG] ä½¿ç”¨å•ä¸€å‘é‡æœç´¢")
                    if self.config["vector_store_type"] == "faiss":
                        results = self._search_faiss(question, top_k)
                    else:
                        results = self._search_qdrant(question, top_k)
                    
                    # åº”ç”¨æ„å›¾æ„ŸçŸ¥é‡æ’åº
                    if self.enable_intent_reranking and self.reranker and results:
                        print(f"ğŸ”„ [RAG-DEBUG] åº”ç”¨æ„å›¾æ„ŸçŸ¥é‡æ’åºï¼ˆå•ä¸€æœç´¢æ¨¡å¼ï¼‰")
                        results = self.reranker.rerank_results(
                            results, 
                            question,
                            intent_weight=self.reranking_config.get("intent_weight", 0.4),
                            semantic_weight=self.reranking_config.get("semantic_weight", 0.6)
                        )
                    
                    # æ„å»ºå…¼å®¹çš„search_responseæ ¼å¼
                    search_response = {
                        "results": results,
                        "query": {"original": question, "rewritten": question, "rewrite_applied": False},
                        "metadata": {
                            "total_results": len(results),
                            "search_type": "vector_only",
                            "fusion_method": "none",
                            "rewrite_info": {
                                "intent": "unknown",
                                "confidence": 0.0,
                                "reasoning": "æœªä½¿ç”¨æŸ¥è¯¢é‡å†™"
                            },
                            "reranking_applied": self.enable_intent_reranking and self.reranker is not None
                        }
                    }
                    
                    # æ ¼å¼åŒ–ç­”æ¡ˆï¼ˆä½¿ç”¨æ‘˜è¦æˆ–åŸå§‹æ ¼å¼ï¼‰
                    print(f"ğŸ” [SUMMARY-DEBUG] æ£€æŸ¥æ‘˜è¦æ¡ä»¶ (å•ä¸€æœç´¢):")
                    print(f"   - enable_summarization: {self.enable_summarization}")
                    print(f"   - summarizerå­˜åœ¨: {self.summarizer is not None}")
                    print(f"   - ç»“æœæ•°é‡: {len(results)}")
                    
                    if self.enable_summarization and self.summarizer and len(results) > 0:
                        print(f"ğŸ’¬ [RAG-DEBUG] ä½¿ç”¨Geminiæ‘˜è¦æ ¼å¼åŒ–ç­”æ¡ˆ")
                        answer = await self._format_answer_with_summary(search_response, question, original_query=original_query)
                    else:
                        print(f"ğŸ’¬ [RAG-DEBUG] ä½¿ç”¨åŸå§‹æ ¼å¼åŒ–ç­”æ¡ˆ")
                        if not self.enable_summarization:
                            print(f"   åŸå› : æ‘˜è¦åŠŸèƒ½æœªå¯ç”¨")
                        elif not self.summarizer:
                            print(f"   åŸå› : æ‘˜è¦å™¨æœªåˆå§‹åŒ–")
                        elif len(results) == 0:
                            print(f"   åŸå› : æ²¡æœ‰æ£€ç´¢ç»“æœ")
                        answer = self._format_answer(search_response, question)
                    
                    confidence = max([r["score"] for r in results]) if results else 0.0
                    sources = [r["chunk"].get("topic", "æœªçŸ¥") for r in results]
                    search_metadata = search_response.get("metadata", {})
                
            else:
                # æ£€æŸ¥æ˜¯å¦æ˜¯å› ä¸ºå‘é‡åº“ä¸å­˜åœ¨
                if (hasattr(self, 'vector_store_path') and self.vector_store_path is None) or not self.vector_store:
                    # æ²¡æœ‰æ‰¾åˆ°å¯¹åº”çš„å‘é‡åº“ï¼Œè¿”å›ç‰¹æ®Šé”™è¯¯æ ‡è¯†ï¼Œè®©è°ƒç”¨æ–¹åˆ‡æ¢åˆ°wikiæ¨¡å¼
                    print(f"âŒ [RAG-DEBUG] å‘é‡åº“æœªæ‰¾åˆ°ï¼Œå»ºè®®åˆ‡æ¢åˆ°wikiæ¨¡å¼")
                    return {
                        "answer": "",  # ç©ºç­”æ¡ˆï¼Œç”±è°ƒç”¨æ–¹å¤„ç†
                        "sources": [],
                        "confidence": 0.0,
                        "query_time": 0.0,
                        "results_count": 0,
                        "error": "VECTOR_STORE_NOT_FOUND",
                        "fallback_to_wiki": True  # æ·»åŠ æ ‡è¯†ï¼Œæç¤ºè°ƒç”¨æ–¹åˆ‡æ¢åˆ°wikiæ¨¡å¼
                    }
                else:
                    # å…¶ä»–æƒ…å†µçš„é”™è¯¯
                    print(f"âŒ [RAG-DEBUG] å‘é‡åº“æŸ¥è¯¢å¤±è´¥ï¼ŒåŸå› æœªçŸ¥")
                    return {
                        "answer": "æŠ±æ­‰ï¼Œæ”»ç•¥æŸ¥è¯¢ç³»ç»Ÿå‡ºç°é—®é¢˜ï¼Œè¯·ç¨åé‡è¯•ã€‚",
                        "sources": [],
                        "confidence": 0.0,
                        "query_time": 0.0,
                        "results_count": 0,
                        "error": "RAG_SYSTEM_ERROR"
                    }
            
            query_time = asyncio.get_event_loop().time() - start_time
            
            response = {
                "answer": answer,
                "sources": sources,
                "confidence": confidence,
                "query_time": query_time,
                "results_count": len(results) if 'results' in locals() else 0
            }
            
            # æ·»åŠ æœç´¢å…ƒæ•°æ®
            if 'search_metadata' in locals():
                response["search_metadata"] = search_metadata
            
            print(f"âœ… [RAG-DEBUG] RAGæŸ¥è¯¢å®Œæˆï¼Œè€—æ—¶: {query_time:.2f}ç§’")
            return response
            
        except Exception as e:
            logger.error(f"RAGæŸ¥è¯¢å¤±è´¥: {e}")
            return {
                "answer": "æŠ±æ­‰ï¼ŒæŸ¥è¯¢è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯ï¼Œè¯·ç¨åé‡è¯•ã€‚",
                "sources": [],
                "confidence": 0.0,
                "query_time": 0.0,
                "error": str(e)
            }

    async def query_stream(self, question: str, top_k: int = 3, original_query: str = None, unified_query_result = None) -> AsyncGenerator[str, None]:
        """
        æ‰§è¡Œæµå¼RAGæŸ¥è¯¢
        
        Args:
            question: ç”¨æˆ·é—®é¢˜
            top_k: æ£€ç´¢ç»“æœæ•°é‡
            original_query: åŸå§‹æŸ¥è¯¢
            unified_query_result: é¢„å¤„ç†çš„ç»Ÿä¸€æŸ¥è¯¢ç»“æœï¼ˆæ¥è‡ªassistant_integrationï¼‰
            
        Yields:
            æµå¼ç­”æ¡ˆå†…å®¹
        """
        if not self.is_initialized:
            await self.initialize()
            
        # å¦‚æœåˆå§‹åŒ–åä»ç„¶æ²¡æœ‰å‘é‡åº“ï¼Œè¿”å›fallbackä¿¡æ¯
        if not self.is_initialized or not self.vector_store:
            print(f"âŒ [RAG-STREAM-DEBUG] RAGç³»ç»Ÿæœªæ­£ç¡®åˆå§‹åŒ–ï¼Œå»ºè®®åˆ‡æ¢åˆ°wikiæ¨¡å¼")
            yield "æŠ±æ­‰ï¼Œæ”»ç•¥æŸ¥è¯¢ç³»ç»Ÿå‡ºç°é—®é¢˜ï¼Œè¯·ç¨åé‡è¯•ã€‚"
            return
            
        start_time = time.time()
        
        try:
            print(f"ğŸŒŠ [RAG-STREAM-DEBUG] å¼€å§‹æµå¼RAGæŸ¥è¯¢: '{question}'")
            if unified_query_result:
                print(f"ğŸ“ [RAG-STREAM-DEBUG] ä½¿ç”¨é¢„å¤„ç†çš„ç»Ÿä¸€æŸ¥è¯¢ç»“æœ:")
                print(f"   - åŸå§‹æŸ¥è¯¢: '{unified_query_result.original_query}'")
                print(f"   - ç¿»è¯‘æŸ¥è¯¢: '{unified_query_result.translated_query}'") 
                print(f"   - é‡å†™æŸ¥è¯¢: '{unified_query_result.rewritten_query}'")
                print(f"   - BM25ä¼˜åŒ–: '{unified_query_result.bm25_optimized_query}'")
                print(f"   - æ„å›¾: {unified_query_result.intent} (ç½®ä¿¡åº¦: {unified_query_result.confidence:.3f})")
            
            if hasattr(self, 'vector_store') and self.vector_store:
                # æ‰§è¡Œæœç´¢ï¼ˆä¸queryæ–¹æ³•ç›¸åŒçš„é€»è¾‘ï¼‰
                if self.enable_hybrid_search and self.hybrid_retriever:
                    print(f"ğŸ” [RAG-STREAM-DEBUG] ä½¿ç”¨æ··åˆæœç´¢")
                    # å¦‚æœæœ‰é¢„å¤„ç†ç»“æœï¼Œä¼ é€’ç»™æ··åˆæœç´¢
                    if unified_query_result:
                        search_response = self._search_hybrid_with_processed_query(unified_query_result, top_k)
                    else:
                        search_response = self._search_hybrid(question, top_k)
                    
                    results = search_response.get("results", [])
                    
                    # åº”ç”¨æ„å›¾æ„ŸçŸ¥é‡æ’åº
                    if self.enable_intent_reranking and self.reranker and results:
                        print(f"ğŸ”„ [RAG-STREAM-DEBUG] åº”ç”¨æ„å›¾æ„ŸçŸ¥é‡æ’åº")
                        results = self.reranker.rerank_results(
                            results, 
                            question,
                            intent_weight=self.reranking_config.get("intent_weight", 0.4),
                            semantic_weight=self.reranking_config.get("semantic_weight", 0.6)
                        )
                        search_response["results"] = results
                        # åœ¨å…ƒæ•°æ®ä¸­è®°å½•é‡æ’åºä¿¡æ¯
                        search_response.setdefault("metadata", {})["reranking_applied"] = True
                    
                    # æ ¼å¼åŒ–ç­”æ¡ˆï¼ˆä½¿ç”¨æµå¼æ‘˜è¦ï¼‰
                    print(f"ğŸ” [SUMMARY-STREAM-DEBUG] æ£€æŸ¥æµå¼æ‘˜è¦æ¡ä»¶:")
                    print(f"   - enable_summarization: {self.enable_summarization}")
                    print(f"   - summarizerå­˜åœ¨: {self.summarizer is not None}")
                    print(f"   - ç»“æœæ•°é‡: {len(results)}")
                    
                    if self.enable_summarization and self.summarizer and len(results) > 0:
                        print(f"ğŸ’¬ [RAG-STREAM-DEBUG] ä½¿ç”¨Geminiæµå¼æ‘˜è¦æ ¼å¼åŒ–ç­”æ¡ˆ")
                        async for chunk in self._format_answer_with_summary_stream(search_response, question, original_query=original_query):
                            yield chunk
                    else:
                        print(f"ğŸ’¬ [RAG-STREAM-DEBUG] ä½¿ç”¨åŸå§‹æ ¼å¼åŒ–ç­”æ¡ˆ")
                        if not self.enable_summarization:
                            print(f"   åŸå› : æ‘˜è¦åŠŸèƒ½æœªå¯ç”¨")
                        elif not self.summarizer:
                            print(f"   åŸå› : æ‘˜è¦å™¨æœªåˆå§‹åŒ–")
                        elif len(results) == 0:
                            print(f"   åŸå› : æ²¡æœ‰æ£€ç´¢ç»“æœ")
                        answer = self._format_answer(search_response, question)
                        yield answer
                        
                else:
                    # å•ä¸€æœç´¢
                    print(f"ğŸ” [RAG-STREAM-DEBUG] ä½¿ç”¨å•ä¸€å‘é‡æœç´¢")
                    if self.config["vector_store_type"] == "faiss":
                        results = self._search_faiss(question, top_k)
                    else:
                        results = self._search_qdrant(question, top_k)
                    
                    # åº”ç”¨æ„å›¾æ„ŸçŸ¥é‡æ’åº
                    if self.enable_intent_reranking and self.reranker and results:
                        print(f"ğŸ”„ [RAG-STREAM-DEBUG] åº”ç”¨æ„å›¾æ„ŸçŸ¥é‡æ’åºï¼ˆå•ä¸€æœç´¢æ¨¡å¼ï¼‰")
                        results = self.reranker.rerank_results(
                            results, 
                            question,
                            intent_weight=self.reranking_config.get("intent_weight", 0.4),
                            semantic_weight=self.reranking_config.get("semantic_weight", 0.6)
                        )
                    
                    # æ„å»ºå…¼å®¹çš„search_responseæ ¼å¼
                    search_response = {
                        "results": results,
                        "query": {"original": question, "rewritten": question, "rewrite_applied": False},
                        "metadata": {
                            "total_results": len(results),
                            "search_type": "vector_only",
                            "fusion_method": "none",
                            "rewrite_info": {
                                "intent": "unknown",
                                "confidence": 0.0,
                                "reasoning": "æœªä½¿ç”¨æŸ¥è¯¢é‡å†™"
                            },
                            "reranking_applied": self.enable_intent_reranking and self.reranker is not None
                        }
                    }
                    
                    # æ ¼å¼åŒ–ç­”æ¡ˆï¼ˆä½¿ç”¨æµå¼æ‘˜è¦ï¼‰
                    print(f"ğŸ” [SUMMARY-STREAM-DEBUG] æ£€æŸ¥æµå¼æ‘˜è¦æ¡ä»¶ (å•ä¸€æœç´¢):")
                    print(f"   - enable_summarization: {self.enable_summarization}")
                    print(f"   - summarizerå­˜åœ¨: {self.summarizer is not None}")
                    print(f"   - ç»“æœæ•°é‡: {len(results)}")
                    
                    if self.enable_summarization and self.summarizer and len(results) > 0:
                        print(f"ğŸ’¬ [RAG-STREAM-DEBUG] ä½¿ç”¨Geminiæµå¼æ‘˜è¦æ ¼å¼åŒ–ç­”æ¡ˆ")
                        async for chunk in self._format_answer_with_summary_stream(search_response, question, original_query=original_query):
                            yield chunk
                    else:
                        print(f"ğŸ’¬ [RAG-STREAM-DEBUG] ä½¿ç”¨åŸå§‹æ ¼å¼åŒ–ç­”æ¡ˆ")
                        if not self.enable_summarization:
                            print(f"   åŸå› : æ‘˜è¦åŠŸèƒ½æœªå¯ç”¨")
                        elif not self.summarizer:
                            print(f"   åŸå› : æ‘˜è¦å™¨æœªåˆå§‹åŒ–")
                        elif len(results) == 0:
                            print(f"   åŸå› : æ²¡æœ‰æ£€ç´¢ç»“æœ")
                        answer = self._format_answer(search_response, question)
                        yield answer
            else:
                # å‘é‡åº“æŸ¥è¯¢å¤±è´¥
                print(f"âŒ [RAG-STREAM-DEBUG] å‘é‡åº“æŸ¥è¯¢å¤±è´¥")
                yield "æŠ±æ­‰ï¼Œæ”»ç•¥æŸ¥è¯¢ç³»ç»Ÿå‡ºç°é—®é¢˜ï¼Œè¯·ç¨åé‡è¯•ã€‚"
                
        except Exception as e:
            print(f"âŒ [RAG-STREAM-DEBUG] æµå¼æŸ¥è¯¢å¼‚å¸¸: {e}")
            logger.error(f"Streaming query error: {str(e)}")
            yield f"æŠ±æ­‰ï¼ŒæŸ¥è¯¢è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}"



# å…¨å±€å®ä¾‹
_enhanced_rag_query = None

def get_enhanced_rag_query(vector_store_path: Optional[str] = None,
                          llm_config: Optional[LLMConfig] = None,
                          enable_summarization: bool = True) -> EnhancedRagQuery:
    """è·å–å¢å¼ºRAGæŸ¥è¯¢å™¨çš„å•ä¾‹å®ä¾‹"""
    global _enhanced_rag_query
    if _enhanced_rag_query is None:
        _enhanced_rag_query = EnhancedRagQuery(
            vector_store_path=vector_store_path,
            llm_config=llm_config,
            enable_summarization=enable_summarization
        )
    return _enhanced_rag_query

async def query_enhanced_rag(question: str, 
                           game_name: Optional[str] = None,
                           top_k: int = 3,
                           enable_hybrid_search: bool = True,
                           hybrid_config: Optional[Dict] = None,
                           llm_config: Optional[LLMConfig] = None,
                           enable_summarization: bool = True,
                           summarization_config: Optional[Dict] = None,
                           enable_intent_reranking: bool = True,
                           reranking_config: Optional[Dict] = None) -> Dict[str, Any]:
    """
    æ‰§è¡Œå¢å¼ºRAGæŸ¥è¯¢çš„ä¾¿æ·å‡½æ•°
    
    Args:
        question: ç”¨æˆ·é—®é¢˜
        game_name: æ¸¸æˆåç§°ï¼ˆå¯é€‰ï¼‰
        top_k: æ£€ç´¢ç»“æœæ•°é‡
        enable_hybrid_search: æ˜¯å¦å¯ç”¨æ··åˆæœç´¢
        hybrid_config: æ··åˆæœç´¢é…ç½®
        llm_config: LLMé…ç½®
        enable_summarization: æ˜¯å¦å¯ç”¨Geminiæ‘˜è¦
        summarization_config: æ‘˜è¦é…ç½®
        enable_intent_reranking: æ˜¯å¦å¯ç”¨æ„å›¾æ„ŸçŸ¥é‡æ’åº
        reranking_config: é‡æ’åºé…ç½®
        
    Returns:
        æŸ¥è¯¢ç»“æœå­—å…¸
    """
    print(f"ğŸ¯ [RAG-DEBUG] è°ƒç”¨query_enhanced_rag - é—®é¢˜: '{question}', æ¸¸æˆ: {game_name}")
    print(f"ğŸ”§ [RAG-DEBUG] é…ç½® - æ··åˆæœç´¢: {enable_hybrid_search}, æ‘˜è¦: {enable_summarization}, é‡æ’åº: {enable_intent_reranking}")
    # ä»é…ç½®æ–‡ä»¶åŠ è½½è®¾ç½®
    import os
    from pathlib import Path
    config_path = Path(__file__).parent.parent / "assets" / "settings.json"
    if config_path.exists():
        import json
        with open(config_path, 'r', encoding='utf-8') as f:
            settings = json.load(f)
            
            # åŠ è½½æ··åˆæœç´¢è®¾ç½®
            if hybrid_config is None:
                hybrid_search_settings = settings.get("hybrid_search", {})
                enable_hybrid_search = hybrid_search_settings.get("enabled", True)
                hybrid_config = {
                    "fusion_method": hybrid_search_settings.get("fusion_method", "rrf"),
                    "vector_weight": hybrid_search_settings.get("vector_weight", 0.3),
                    "bm25_weight": hybrid_search_settings.get("bm25_weight", 0.7),
                    "rrf_k": hybrid_search_settings.get("rrf_k", 60)
                }
            
            # åŠ è½½æ‘˜è¦è®¾ç½®
            if summarization_config is None:
                summarization_settings = settings.get("summarization", {})
                enable_summarization = summarization_settings.get("enabled", True)  # é»˜è®¤å¯ç”¨æ‘˜è¦
                summarization_config = {
                    "api_key": summarization_settings.get("api_key") or os.environ.get("GEMINI_API_KEY"),
                    "model_name": summarization_settings.get("model_name", "gemini-2.5-flash-lite-preview-06-17"),
                    # ç§»é™¤å·²åºŸå¼ƒçš„max_summary_lengthå‚æ•°
                    "temperature": summarization_settings.get("temperature", 0.3),
                    "include_sources": summarization_settings.get("include_sources", True),
                    "language": summarization_settings.get("language", "auto")
                }
            
            # åŠ è½½é‡æ’åºè®¾ç½®
            if enable_intent_reranking is None:
                reranking_settings = settings.get("intent_reranking", {})
                enable_intent_reranking = reranking_settings.get("enabled", True)
            
            if reranking_config is None:
                reranking_settings = settings.get("intent_reranking", {})
                reranking_config = {
                    "intent_weight": reranking_settings.get("intent_weight", 0.4),
                    "semantic_weight": reranking_settings.get("semantic_weight", 0.6)
                }
    
    print(f"ğŸ”§ [RAG-DEBUG] åˆ›å»ºEnhancedRagQueryå®ä¾‹")
    rag_query = EnhancedRagQuery(
        enable_hybrid_search=enable_hybrid_search,
        hybrid_config=hybrid_config,
        llm_config=llm_config,
        enable_summarization=enable_summarization,
        summarization_config=summarization_config,
        enable_intent_reranking=enable_intent_reranking,
        reranking_config=reranking_config
    )
    
    print(f"ğŸ”§ [RAG-DEBUG] åˆå§‹åŒ–RAGå¼•æ“")
    await rag_query.initialize(game_name)
    
    print(f"ğŸ” [RAG-DEBUG] æ‰§è¡ŒRAGæŸ¥è¯¢ï¼ˆæµå¼ï¼‰")
    answer_parts = []
    async for chunk in rag_query.query_stream(question, top_k):
        answer_parts.append(chunk)
    
    # æ„å»ºä¸åŸ query æ–¹æ³•å…¼å®¹çš„ç»“æœæ ¼å¼
    result = {
        "answer": "".join(answer_parts),
        "sources": [],
        "confidence": 0.0,
        "query_time": 0.0,
        "results_count": 0
    }
    print(f"âœ… [RAG-DEBUG] query_enhanced_ragå®Œæˆ")
    return result


class SimpleRagQuery(EnhancedRagQuery):
    """ç®€åŒ–çš„RAGæŸ¥è¯¢å™¨ï¼Œä¿æŒå‘åå…¼å®¹"""
    pass

def get_rag_query() -> SimpleRagQuery:
    """è·å–ç®€åŒ–RAGæŸ¥è¯¢å™¨çš„å•ä¾‹å®ä¾‹ï¼ˆå‘åå…¼å®¹ï¼‰"""
    return SimpleRagQuery()

async def query_rag(question: str, game_name: Optional[str] = None) -> Dict[str, Any]:
    """æ‰§è¡Œç®€åŒ–RAGæŸ¥è¯¢çš„ä¾¿æ·å‡½æ•°ï¼ˆå‘åå…¼å®¹ï¼‰"""
    return await query_enhanced_rag(question, game_name) 